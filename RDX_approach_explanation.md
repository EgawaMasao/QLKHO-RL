# Gi·∫£i th√≠ch chi ti·∫øt: Model-Agnostic Counterfactual Decomposition

## 1. RDX Truy·ªÅn th·ªëng vs Approach Hi·ªán t·∫°i

### RDX Truy·ªÅn th·ªëng (Theory)

**ƒê·ªãnh nghƒ©a**: Decompose Q-value difference th√†nh reward components

```
Q(s, a_best) - Q(s, a_second) = Œ£ ŒîQ^k
                                 k
```

**Pipeline cho DQN**:
```
State s ‚Üí Q-Network ‚Üí Q(s,a) for all actions
                   ‚Üì
         Q(s, a_best) = 0.85
         Q(s, a_second) = 0.72
                   ‚Üì
         Q_gap = 0.13 ‚Üê T·ª´ LEARNED network
                   ‚Üì
    Decompose 0.13 th√†nh: stockout + overstock + waste + quantile
```

**V·∫•n ƒë·ªÅ v·ªõi A2C**: A2C kh√¥ng h·ªçc Q(s,a), ch·ªâ h·ªçc V(s) v√† œÄ(a|s)
- C·∫ßn estimate: Q(s,a) ‚âà r(s,a) + Œ≥V(s')
- Ph·ª©c t·∫°p v√† c√≥ noise

---

### Model-Agnostic Approach (Code hi·ªán t·∫°i)

**ƒê·ªãnh nghƒ©a**: So s√°nh reward components gi·ªØa 2 counterfactual trajectories

```
ŒîReward = Reward(s, a_best) - Reward(s, a_second)
        = Œ£ [Reward_k(s, a_best) - Reward_k(s, a_second)]
         k
        = Œ£ ŒîQ^k
         k
```

**Pipeline CHUNG cho c·∫£ DQN v√† A2C**:
```
State s ‚Üí Agent ‚Üí Select actions: a_best, a_second
                        ‚Üì
            Environment Simulation (1-step)
                        ‚Üì
        Next state:  s'_best, s'_second
                        ‚Üì
        Reward calc: r_best, r_second (t·ª´ reward function)
                        ‚Üì
    Decompose: ŒîQ^stockout, ŒîQ^overstock, ŒîQ^waste, ŒîQ^quantile
```

**ƒê·∫∑c ƒëi·ªÉm**:
- ‚úÖ Kh√¥ng c·∫ßn Q-values t·ª´ neural network
- ‚úÖ Work cho m·ªçi lo·∫°i agent (DQN, A2C, PPO, ...)
- ‚úÖ D·ª±a tr√™n GROUND-TRUTH reward function
- ‚úÖ So s√°nh c√¥ng b·∫±ng cross-architecture

---

## 2. T·∫°i sao approach n√†y H·ª¢P L√ù cho Inventory Management?

### 2.1. Environment ƒë·∫∑c th√π

**Inventory Management c√≥ ƒë·∫∑c ƒëi·ªÉm**:
1. **Deterministic dynamics**: x_{t+1} = max(0, x_t + u_t - sales_t)
   - Kh√¥ng c√≥ random transitions
   - Next state ho√†n to√†n x√°c ƒë·ªãnh t·ª´ (state, action)

2. **Explicit reward function**: 
   ```python
   r = (1 - stockout_penalty) - overstock_penalty - waste_penalty - quantile_penalty
   ```
   - Components r√µ r√†ng, additively separable
   - C√≥ th·ªÉ t√≠nh ch√≠nh x√°c t·ª´ (s, a, s')

3. **Short-term decisions**: 
   - Inventory reorder m·ªói ng√†y
   - Discount factor Œ≥=0.99 ‚Üí long-term lookahead kh√¥ng quan tr·ªçng l·∫Øm
   - 1-step lookahead ƒë√£ capture ƒë∆∞·ª£c majority of effect

### 2.2. So s√°nh v·ªõi c√°c domain kh√°c

| Domain | Dynamics | Reward | Approach ph√π h·ª£p |
|--------|----------|--------|------------------|
| **Atari Games** | Stochastic | Sparse, opaque | RDX truy·ªÅn th·ªëng (ph·∫£i d√πng learned Q) |
| **Robotics** | Continuous, noisy | Complex, multi-modal | RDX truy·ªÅn th·ªëng |
| **Inventory Mgmt** | **Deterministic** | **Explicit, additive** | **Model-agnostic OK** ‚úì |
| **Board Games** | Deterministic | Win/lose | RDX truy·ªÅn th·ªëng |

**K·∫øt lu·∫≠n**: V·ªõi Inventory, model-agnostic approach **ph√π h·ª£p h∆°n** v√¨:
- Ground-truth reward d·ªÖ t√≠nh
- Dynamics ƒë∆°n gi·∫£n
- Kh√¥ng b·ªã noise t·ª´ neural network approximation

---

## 3. Advantage c·ªßa Model-Agnostic Approach

### 3.1. Cross-Agent Comparison

**V·∫•n ƒë·ªÅ v·ªõi RDX truy·ªÅn th·ªëng khi so s√°nh DQN vs A2C**:

```
DQN:  Q_gap = Q_network(s, a_best) - Q_network(s, a_second)
              ‚Üì
      Scale: 0.01 - 2.0 (t√πy training)
      Accuracy: ~85% (DQN approximation error)

A2C:  Q_gap ‚âà [r(s,a) + Œ≥V(s')] - V(s)  (estimated)
              ‚Üì
      Scale: 0.001 - 0.5 (kh√°c DQN)
      Accuracy: ~75% (nhi·ªÅu estimation steps)
```

‚Üí **So s√°nh kh√¥ng c√¥ng b·∫±ng**: DQN Q-values ‚â† A2C Q-estimates v·ªÅ magnitude

**V·ªõi Model-Agnostic**:
```
DQN & A2C:  ŒîReward = r(s, a_best) - r(s, a_second)
                    ‚Üì
            Scale: SAME (t·ª´ c√πng reward function)
            Accuracy: 100% (ground-truth)
```

‚Üí **So s√°nh c√¥ng b·∫±ng**: C√πng scale, c√πng measurement method

### 3.2. Interpretability

**Model-agnostic approach d·ªÖ gi·∫£i th√≠ch h∆°n**:

- **RDX truy·ªÅn th·ªëng**: "Agent nghƒ© r·∫±ng objective k quan tr·ªçng b·∫±ng ŒîQ^k"
  - Ph·ª• thu·ªôc v√†o accuracy c·ªßa learned Q
  - C√≥ th·ªÉ sai n·∫øu Q-network h·ªçc k√©m

- **Model-agnostic**: "N·∫øu agent ch·ªçn action kh√°c, objective k thay ƒë·ªïi b·∫±ng ŒîQ^k"
  - Counterfactual reasoning (d·ªÖ hi·ªÉu cho practitioners)
  - Kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi Q approximation error

### 3.3. Robustness

| Aspect | RDX truy·ªÅn th·ªëng | Model-Agnostic |
|--------|------------------|----------------|
| **Training quality** | Sensitive (bad Q ‚Üí bad explanation) | Robust (reward always correct) |
| **Hyperparameters** | Sensitive (learning rate, architecture) | Agnostic |
| **Agent type** | Need Q-values | Works for any agent |
| **Noise** | Q-approximation error | No neural network noise |

---

## 4. C√≥ ph·∫£i ƒë·ªïi t√™n kh√¥ng?

### 4.1. Option A: Gi·ªØ t√™n "RDX"

**Argument**: 
- RDX = Reward Decomposition eXplanation
- Ta v·∫´n decompose reward difference
- Just m·ªôt variant: "Model-Agnostic RDX"

**Risk**:
- Reviewer quen v·ªõi RDX truy·ªÅn th·ªëng (decompose Q)
- C√≥ th·ªÉ b·ªã challenge v·ªÅ correctness

### 4.2. Option B: ƒê·ªïi t√™n m·ªõi

**Suggestions**:
1. **ODA** (Objective Decomposition Analysis)
2. **CRD** (Counterfactual Reward Decomposition)
3. **MAED** (Model-Agnostic Explanation via Decomposition)
4. **ROCA** (Reward-based Objective Counterfactual Analysis)

**Recommendation**: **Gi·ªØ "RDX" NH∆ØNG clarify**

**C√°ch vi·∫øt trong paper**:

```markdown
### 3.2 Model-Agnostic Reward Decomposition (RDX)

Traditional RDX [Juozapaitis et al., 2019] decomposes learned Q-values 
into reward components. However, this approach faces challenges when 
comparing agents with different architectures (e.g., DQN vs A2C):

1. Q-value scales differ across architectures
2. A2C doesn't directly learn Q(s,a)
3. Decomposition depends on neural network approximation quality

For inventory management, we propose a **model-agnostic variant** that 
leverages domain properties:

**Key insight**: Since our environment has deterministic dynamics and 
an explicit, additive reward function, we can compute reward components 
directly via counterfactual simulation, bypassing learned Q-values entirely.

This enables fair cross-architecture comparison while maintaining the 
interpretability benefits of objective-level decomposition.
```

---

## 5. Validation: C√≥ ƒë√∫ng kh√¥ng?

### Test case ƒë·ªÉ verify approach

```python
# Scenario: Product p=0, state s, actions u_best=0.3, u_second=0.1
x = 0.2       # current inventory
sales = 0.15  # demand
capacity = 1.0
waste_rate = 0.025

# Simulate best action
x_after_best = min(x + 0.3, 1.0) = 0.5
x_next_best = max(x_after_best - sales, 0) = 0.35
overstock_best = max(x_after_best - 1.0, 0) = 0
stockout_best = (x_next_best < 0.05) ? 1 : 0 = 0
waste_best = 0.025 * 0.35 = 0.00875

# Simulate second action  
x_after_second = min(x + 0.1, 1.0) = 0.3
x_next_second = max(x_after_second - sales, 0) = 0.15
overstock_second = 0
stockout_second = 0
waste_second = 0.025 * 0.15 = 0.00375

# Reward difference
Œîoverstock = 0 - 0 = 0
Œîstockout = 0.99 * (0 - 0) = 0
Œîwaste = 0.99 * (0.00375 - 0.00875) = -0.00495

# Interpretation: Best action ‚Üí √≠t waste h∆°n 0.00495
```

**Verification**:
- ‚úÖ Math ƒë√∫ng theo dynamics
- ‚úÖ Reward components additive
- ‚úÖ Kh√¥ng c·∫ßn Q-values
- ‚úÖ Same cho DQN v√† A2C

---

## 6. K·∫øt lu·∫≠n: C√≥ n√™n d√πng H∆∞·ªõng 2?

### ‚úÖ N√äN d√πng n·∫øu:
- [x] Environment deterministic
- [x] Reward function explicit v√† additive
- [x] C·∫ßn so s√°nh nhi·ªÅu agent architectures
- [x] Focus v√†o "objective importance in decision"
- [x] Mu·ªën explanation robust, kh√¥ng ph·ª• thu·ªôc training quality

### ‚ùå KH√îNG n√™n n·∫øu:
- [ ] Environment stochastic (c·∫ßn model expectations)
- [ ] Reward opaque/complex (kh√¥ng t√≠nh ƒë∆∞·ª£c t·ª´ (s,a,s'))
- [ ] Ch·ªâ test 1 agent (kh√¥ng c·∫ßn cross-agent fairness)
- [ ] Focus v√†o "what agent learned" (th√¨ ph·∫£i d√πng Q-values)

### üéØ Cho b√†i to√°n Inventory c·ªßa b·∫°n:

**RECOMMENDATION: D√πng H∆∞·ªõng 2 (Model-Agnostic) + Clarify trong paper**

**Actions c·∫ßn l√†m**:
1. ‚úÖ Gi·ªØ code hi·ªán t·∫°i (ƒë√£ ƒë√∫ng logic)
2. ‚úÖ Fix bug A2C q_gap (ƒë√£ fix ·ªü tr√™n)
3. ‚úÖ Add clarification trong notebook markdown
4. ‚úÖ Justify trong paper (d√πng template ·ªü section 4.2)
5. ‚ùå KH√îNG c·∫ßn implement Q-based RDX (overkill)

---

## 7. Template cho Paper Section

```latex
\subsection{Objective-Level Explanation via Reward Decomposition}

To understand which inventory objectives drive agent decisions, we 
decompose the value difference between selected and alternative actions 
into interpretable reward components.

\textbf{Model-Agnostic Formulation.} 
Unlike traditional RDX which decomposes learned Q-values 
\cite{juozapaitis2019explainable}, our approach leverages domain knowledge:
the inventory environment has deterministic transitions and an additive 
reward structure. This allows direct computation of objective contributions 
via counterfactual simulation:

\begin{equation}
\Delta Q^k = \gamma [r^k(s, s'_{\text{best}}) - r^k(s, s'_{\text{alt}})]
\end{equation}

where $s'$ follows deterministic dynamics $s' = f(s, a)$.

\textbf{Advantages:} 
(1) Architecture-agnostic: enables fair comparison between value-based 
(DQN) and policy-gradient (A2C) methods. 
(2) Robust: unaffected by Q-function approximation quality. 
(3) Interpretable: directly measures ground-truth objective trade-offs.

\textbf{Applicability:} This formulation is suitable for domains with 
known dynamics and structured rewards (e.g., logistics, resource allocation), 
while traditional RDX remains necessary for opaque environments 
(e.g., video games, robotics).
```

---

## 8. FAQ

**Q1: Li·ªáu approach n√†y c√≥ b·ªã reviewer reject kh√¥ng?**

A: Kh√¥ng, n·∫øu justify ƒë√∫ng c√°ch:
- Supply chain/logistics domain th∆∞·ªùng d√πng model-based analysis
- Counterfactual reasoning l√† standard trong decision support
- Ch·ªâ c·∫ßn clarify ƒë√¢y l√† variant ph√π h·ª£p v·ªõi domain characteristics

**Q2: C√≥ c·∫ßn th√™m experiment v·ªõi Q-based RDX ƒë·ªÉ compare kh√¥ng?**

A: Nice-to-have nh∆∞ng kh√¥ng b·∫Øt bu·ªôc:
- N·∫øu c√≥ th·ªùi gian: add ablation "Q-based vs Model-Agnostic RDX"
- N·∫øu kh√¥ng: clarify trong "Limitations" r·∫±ng future work c√≥ th·ªÉ compare

**Q3: MSX v·∫´n d√πng ƒë∆∞·ª£c kh√¥ng?**

A: C√≥, MSX logic kh√¥ng ƒë·ªïi:
- MSX t√¨m minimal set objectives gi·∫£i th√≠ch q_gap
- Kh√¥ng quan tr·ªçng q_gap t·ª´ ƒë√¢u (Q-network hay simulation)

**Q4: CAS metric c√≥ b·ªã ·∫£nh h∆∞·ªüng kh√¥ng?**

A: Kh√¥ng, CAS align SHAP (features) v·ªõi RDX (objectives)
- SHAP v·∫´n d√πng neural network gradients (kh√¥ng ƒë·ªïi)
- RDX ch·ªâ ƒë·ªïi c√°ch t√≠nh objective importance (v·∫´n valid)

---

**T√≥m l·∫°i**: Code hi·ªán t·∫°i ƒë√∫ng v·ªÅ m·∫∑t to√°n h·ªçc v√† ph√π h·ª£p v·ªõi domain. 
Ch·ªâ c·∫ßn clarify methodology trong paper l√† ƒë·ªß, kh√¥ng c·∫ßn refactor code!
