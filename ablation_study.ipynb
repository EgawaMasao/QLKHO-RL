{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Ablation Study: XAI Methods for RL-based Inventory Management\n",
    "---\n",
    "\n",
    "**Agents**: DQN (Double DQN, Per-Product Q-Network) vs A2C_mod (Actor-Critic)  \n",
    "**XAI Methods**: RDX (Reward Decomposition), MSX (Minimal Sufficient Explanation), SHAP  \n",
    "**Environment**: 220 products, 14 discrete actions, reward = `1 - z - overstock - q - quan`\n",
    "\n",
    "### Experiment Grid\n",
    "| Dimension | Values |\n",
    "|---|---|\n",
    "| Agent | DQN, A2C_mod |\n",
    "| Scenario | EASY, MEDIUM, HARD |\n",
    "| XAI Config | RDX_only, SHAP_only, Combined |\n",
    "| Œª (MSX threshold) | 0.5, 1.0, 1.5, 2.0 |\n",
    "\n",
    "### Metrics\n",
    "- **OCS** (Objective Coverage Score): fraction of objectives with |ŒîQ^k| > Œ∏_Q\n",
    "- **FCS** (Feature Coverage Score): fraction of features with |SHAP| > Œ∏_œÜ\n",
    "- **CAS** (Cross-domain Alignment Score): Jaccard similarity between top SHAP features and top RDX objectives\n",
    "- **Stability**: % MSX set change when Œª varies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Kh·ªüi t·∫°o Agent (Restore Checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59614c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings, time\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa   # DQN checkpoint uses tfa.layers.GroupNormalization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import product as iterproduct\n",
    "\n",
    "np.set_printoptions(edgeitems=10, linewidth=10000, precision=6, suppress=True)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa4455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "NUM_PRODUCTS    = 220\n",
    "NUM_FEATURES_PP = 3          # per product: [x, sales, q]\n",
    "NUM_FEATURES    = NUM_PRODUCTS * NUM_FEATURES_PP  # 660\n",
    "NUM_ACTIONS     = 14\n",
    "ACTION_SPACE    = np.array([0, 0.005, 0.01, 0.0125, 0.015, 0.0175,\n",
    "                            0.02, 0.03, 0.04, 0.08, 0.12, 0.2, 0.5, 1],\n",
    "                           dtype=np.float32)\n",
    "WASTE_RATE      = 0.025\n",
    "ZERO_INVENTORY  = 1e-5\n",
    "GAMMA           = 0.99\n",
    "\n",
    "# Architecture sizes\n",
    "DQN_HIDDEN  = 128\n",
    "A2C_HIDDEN  = 32\n",
    "DROPOUT     = 0.1\n",
    "\n",
    "# Paths\n",
    "DATA_DIR        = 'data220'\n",
    "TEST_FILE       = os.path.join(DATA_DIR, 'test.tfrecords')\n",
    "CAP_FILE        = os.path.join(DATA_DIR, 'capacity.tfrecords')\n",
    "STOCK_FILE      = os.path.join(DATA_DIR, 'stock.tfrecords')\n",
    "DQN_CKPT_DIR    = 'checkpoints_dqn_comparison3'\n",
    "A2C_CKPT_DIR    = 'checkpoints'\n",
    "\n",
    "# Reward component identifiers\n",
    "OBJECTIVES = ['stockout', 'overstock', 'waste', 'quantile']\n",
    "FEATURES   = ['inventory', 'sales', 'waste_feat']  # 3 input features\n",
    "\n",
    "print(\"Configuration ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa144df3",
   "metadata": {},
   "source": [
    "### 1.1 Model Architecture Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ef7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# A2C_mod Model Classes ‚Äî from training1.py\n",
    "# ============================================================\n",
    "# Key: Critic uses tf.keras.layers.GroupNormalization(groups=1)\n",
    "#      Actor has 4 Dense layers ‚Üí softmax\n",
    "#      Both use hidden_size=32, dropout=0.1\n",
    "\n",
    "class Dense(tf.Module):\n",
    "    def __init__(self, input_dim, output_size, activation=None, stddev=1.0):\n",
    "        super(Dense, self).__init__()\n",
    "        self.w = tf.Variable(\n",
    "            tf.random.truncated_normal([input_dim, output_size], stddev=stddev), name='w')\n",
    "        self.b = tf.Variable(tf.zeros([output_size]), name='b')\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        if self.activation:\n",
    "            y = self.activation(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Actor(tf.Module):\n",
    "    \"\"\"Policy network: [P, 3] ‚Üí [P, 14] softmax probabilities.\"\"\"\n",
    "    def __init__(self, num_features, num_actions, hidden_size,\n",
    "                 activation=tf.nn.relu, dropout_prob=0.1):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer1 = Dense(num_features, hidden_size, activation=None)\n",
    "        self.layer2 = Dense(hidden_size, hidden_size, activation=None)\n",
    "        self.layer3 = Dense(hidden_size, hidden_size, activation=None)\n",
    "        self.layer4 = Dense(hidden_size, num_actions, activation=None)\n",
    "        self.activation = activation\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def __call__(self, state):\n",
    "        x = self.activation(self.layer1(state))\n",
    "        x = tf.nn.dropout(x, self.dropout_prob)\n",
    "        x = self.activation(self.layer2(x))\n",
    "        x = tf.nn.dropout(x, self.dropout_prob)\n",
    "        x = self.activation(self.layer3(x))\n",
    "        x = tf.nn.dropout(x, self.dropout_prob)\n",
    "        x = self.layer4(x)\n",
    "        return tf.nn.softmax(x)\n",
    "\n",
    "\n",
    "class Critic(tf.Module):\n",
    "    \"\"\"Value network: [P, 3] ‚Üí [P] scalar values. Uses GroupNorm.\"\"\"\n",
    "    def __init__(self, num_features, hidden_size,\n",
    "                 activation=tf.nn.relu, dropout_prob=0.1):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer1 = Dense(num_features, hidden_size, activation=None)\n",
    "        self.layer2 = Dense(hidden_size, 1, activation=None)\n",
    "        self.activation = activation\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.group_norm = tf.keras.layers.GroupNormalization(groups=1)\n",
    "\n",
    "    def __call__(self, state):\n",
    "        x = self.layer1(state)\n",
    "        x = self.group_norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = tf.nn.dropout(x, self.dropout_prob)\n",
    "        x = self.layer2(x)\n",
    "        return tf.squeeze(x, axis=-1, name='factor_squeeze')\n",
    "\n",
    "print(\"A2C_mod classes (Dense, Actor, Critic) defined ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4fa436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DQN Model Class ‚Äî from dqn_a2c_comparison.ipynb\n",
    "# Per-Product Q-Network: [B, 660] ‚Üí reshape ‚Üí [B*220, 3] ‚Üí MLP ‚Üí [B, 220, 14]\n",
    "# ============================================================\n",
    "\n",
    "class MultiProductQNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Per-Product Q-Network. Each product processed independently.\n",
    "    Input:  [B, 660]  (flattened: [x_0..x_P, sales_0..sales_P, q_0..q_P])\n",
    "    Output: [B, 220, 14]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_products, num_actions,\n",
    "                 hidden_size, dropout_prob=0.1, use_group_norm=True, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.num_products      = num_products\n",
    "        self.num_actions       = num_actions\n",
    "        self.features_per_prod = num_features // num_products  # 3\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(hidden_size, activation=None, name=\"dense1\")\n",
    "        self.dense2 = tf.keras.layers.Dense(hidden_size, activation=None, name=\"dense2\")\n",
    "        self.dense3 = tf.keras.layers.Dense(hidden_size, activation=None, name=\"dense3\")\n",
    "        self.out    = tf.keras.layers.Dense(num_actions,  activation=None, name=\"output\")\n",
    "\n",
    "        self._use_gn = use_group_norm\n",
    "        if use_group_norm:\n",
    "            self.gn1 = tfa.layers.GroupNormalization(groups=1, name=\"gn1\")\n",
    "            self.gn2 = tfa.layers.GroupNormalization(groups=1, name=\"gn2\")\n",
    "            self.gn3 = tfa.layers.GroupNormalization(groups=1, name=\"gn3\")\n",
    "\n",
    "        self.drop1 = tf.keras.layers.Dropout(dropout_prob)\n",
    "        self.drop2 = tf.keras.layers.Dropout(dropout_prob)\n",
    "        self.drop3 = tf.keras.layers.Dropout(dropout_prob)\n",
    "\n",
    "    def call(self, state, training=False):\n",
    "        B = tf.shape(state)[0]\n",
    "        P, F = self.num_products, self.features_per_prod\n",
    "        # [B, 660] ‚Üí [B, 3, 220] ‚Üí [B, 220, 3]\n",
    "        s3d = tf.transpose(tf.reshape(state, [B, F, P]), [0, 2, 1])\n",
    "        x = tf.reshape(s3d, [B * P, F])\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        if self._use_gn: x = self.gn1(x, training=training)\n",
    "        x = tf.nn.relu(x); x = self.drop1(x, training=training)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "        if self._use_gn: x = self.gn2(x, training=training)\n",
    "        x = tf.nn.relu(x); x = self.drop2(x, training=training)\n",
    "\n",
    "        x = self.dense3(x)\n",
    "        if self._use_gn: x = self.gn3(x, training=training)\n",
    "        x = tf.nn.relu(x); x = self.drop3(x, training=training)\n",
    "\n",
    "        return tf.reshape(self.out(x), [B, P, self.num_actions])\n",
    "\n",
    "print(\"DQN class (MultiProductQNetwork) defined ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9a4ac",
   "metadata": {},
   "source": [
    "### 1.2 Load Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64bdf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# load_trained_agents(): Restore both agents from disk\n",
    "# ============================================================\n",
    "\n",
    "def load_trained_agents():\n",
    "    \"\"\"Load DQN and A2C_mod agents from checkpoints.\"\"\"\n",
    "\n",
    "    # ‚îÄ‚îÄ A2C_mod ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    actor  = Actor(NUM_FEATURES_PP, NUM_ACTIONS, A2C_HIDDEN,\n",
    "                   activation=tf.nn.relu, dropout_prob=DROPOUT)\n",
    "    critic = Critic(NUM_FEATURES_PP, A2C_HIDDEN,\n",
    "                    activation=tf.nn.relu, dropout_prob=DROPOUT)\n",
    "    # Build with dummy pass\n",
    "    _d = tf.zeros([1, NUM_FEATURES_PP])\n",
    "    _ = actor(_d); _ = critic(_d)\n",
    "\n",
    "    a2c_ckpt = tf.train.Checkpoint(\n",
    "        critic_optimizer=tf.optimizers.Adam(0.0005),\n",
    "        actor_optimizer=tf.optimizers.Adam(0.0001),\n",
    "        critic=critic, actor=actor, step=tf.Variable(0))\n",
    "    a2c_ckpt.restore(tf.train.latest_checkpoint(A2C_CKPT_DIR)).expect_partial()\n",
    "    print(f\"‚úÖ A2C_mod restored: {tf.train.latest_checkpoint(A2C_CKPT_DIR)}\")\n",
    "\n",
    "    # ‚îÄ‚îÄ DQN ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    q_net = MultiProductQNetwork(\n",
    "        NUM_FEATURES, NUM_PRODUCTS, NUM_ACTIONS,\n",
    "        DQN_HIDDEN, DROPOUT, use_group_norm=True, name=\"q_network\")\n",
    "    t_net = MultiProductQNetwork(\n",
    "        NUM_FEATURES, NUM_PRODUCTS, NUM_ACTIONS,\n",
    "        DQN_HIDDEN, DROPOUT, use_group_norm=True, name=\"target_network\")\n",
    "    _d = tf.zeros([1, NUM_FEATURES], dtype=tf.float32)\n",
    "    _ = q_net(_d, training=False); _ = t_net(_d, training=False)\n",
    "\n",
    "    dqn_ckpt = tf.train.Checkpoint(\n",
    "        optimizer=tf.optimizers.Adam(0.001),\n",
    "        q_network=q_net, target_network=t_net,\n",
    "        step=tf.Variable(0, dtype=tf.int64))\n",
    "    dqn_ckpt.restore(tf.train.latest_checkpoint(DQN_CKPT_DIR)).expect_partial()\n",
    "    print(f\"‚úÖ DQN restored: {tf.train.latest_checkpoint(DQN_CKPT_DIR)}\")\n",
    "\n",
    "    return {'actor': actor, 'critic': critic, 'q_network': q_net}\n",
    "\n",
    "agents = load_trained_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed5262",
   "metadata": {},
   "source": [
    "### 1.3 Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4458efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load TFRecord test data\n",
    "# ============================================================\n",
    "def _parse(serialized, key, n):\n",
    "    return tf.io.parse_single_example(\n",
    "        serialized, {key: tf.io.FixedLenFeature([n], tf.float32)})[key]\n",
    "\n",
    "capacity = next(iter(\n",
    "    tf.data.TFRecordDataset(CAP_FILE).map(lambda s: _parse(s, 'capacity', NUM_PRODUCTS))\n",
    ")).numpy()\n",
    "\n",
    "x_init = next(iter(\n",
    "    tf.data.TFRecordDataset(STOCK_FILE).map(lambda s: _parse(s, 'stock', NUM_PRODUCTS))\n",
    ")).numpy()\n",
    "\n",
    "all_sales = []\n",
    "for rec in tf.data.TFRecordDataset(TEST_FILE).map(lambda s: _parse(s, 'sales', NUM_PRODUCTS)):\n",
    "    all_sales.append(rec.numpy())\n",
    "all_sales = np.array(all_sales, dtype=np.float32) / capacity[None, :]\n",
    "T_MAX = len(all_sales)\n",
    "print(f\"‚úÖ Test data: {T_MAX} timesteps √ó {NUM_PRODUCTS} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1bc9b3",
   "metadata": {},
   "source": [
    "## Step 2: XAI Module ‚Äî RDX, MSX, SHAP\n",
    "\n",
    "### 2.1 RDX (Reward Decomposition eXplanation)\n",
    "\n",
    "Ph√¢n r√£ Q-value/Advantage th√†nh 4 th√†nh ph·∫ßn d·ª±a tr√™n c·∫•u tr√∫c reward:\n",
    "$$r = \\underbrace{1}_{base} - \\underbrace{z}_{stockout} - \\underbrace{overstock}_{overstock} - \\underbrace{q}_{waste} - \\underbrace{quan}_{quantile}$$\n",
    "\n",
    "**ŒîQ^k**: Ch√™nh l·ªách Q-value component k gi·ªØa action t·ªët nh·∫•t v√† action thay th·∫ø."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eee5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RDX: Reward Decomposition\n",
    "# ============================================================\n",
    "\n",
    "def compute_reward_components(x_vec):\n",
    "    \"\"\"\n",
    "    T√≠nh 4 reward sub-components cho m·ªôt tr·∫°ng th√°i inventory x.\n",
    "\n",
    "    Args:\n",
    "        x_vec: np.ndarray [P] ‚Äî inventory levels\n",
    "\n",
    "    Returns:\n",
    "        dict[str, np.ndarray[P]]: 4 components (positive = penalty magnitude)\n",
    "    \"\"\"\n",
    "    z    = (x_vec < ZERO_INVENTORY).astype(np.float32)\n",
    "    q    = WASTE_RATE * x_vec\n",
    "    quan = float(np.quantile(x_vec, 0.95) - np.quantile(x_vec, 0.05))\n",
    "    return {\n",
    "        'stockout':  z,                                            # [P]\n",
    "        'overstock': np.zeros(NUM_PRODUCTS, dtype=np.float32),     # placeholder, filled per action\n",
    "        'waste':     q,                                            # [P]\n",
    "        'quantile':  np.full(NUM_PRODUCTS, quan, np.float32),      # [P]\n",
    "    }\n",
    "\n",
    "\n",
    "def rdx_dqn(q_network, state_flat, x_vec):\n",
    "    \"\"\"\n",
    "    RDX for DQN: Decompose Q-value difference into 4 objective contributions.\n",
    "\n",
    "    For each product i:\n",
    "      a* = argmax_a Q(s, a)_i\n",
    "      a' = second-best action for product i\n",
    "      ŒîQ_total = Q(s, a*) - Q(s, a')\n",
    "      ŒîQ^k ‚âà (penalty_k(a*) - penalty_k(a')) weighted by Q-value proportion\n",
    "\n",
    "    Returns:\n",
    "        delta_q: dict[str, np.ndarray[P]] ‚Äî ŒîQ^k per objective per product\n",
    "        best_actions: np.ndarray[P] ‚Äî indices of best actions\n",
    "    \"\"\"\n",
    "    q_vals = q_network(state_flat[None, :], training=False)[0].numpy()  # [P, A]\n",
    "    best_a  = np.argmax(q_vals, axis=1)    # [P]\n",
    "\n",
    "    # Second-best action\n",
    "    q_masked = q_vals.copy()\n",
    "    q_masked[np.arange(NUM_PRODUCTS), best_a] = -np.inf\n",
    "    second_a = np.argmax(q_masked, axis=1)  # [P]\n",
    "\n",
    "    u_best   = ACTION_SPACE[best_a]\n",
    "    u_second = ACTION_SPACE[second_a]\n",
    "\n",
    "    # Compute penalties for best action\n",
    "    os_best   = np.maximum(0, x_vec + u_best - 1)\n",
    "    os_second = np.maximum(0, x_vec + u_second - 1)\n",
    "\n",
    "    comps = compute_reward_components(x_vec)\n",
    "\n",
    "    # ŒîQ^k = difference in penalty between actions (positive = best action reduces penalty)\n",
    "    delta_q = {\n",
    "        'stockout':  comps['stockout'] - comps['stockout'],  # same x ‚Üí same z\n",
    "        'overstock': -(os_best - os_second),                 # negative sign: less overstock = better\n",
    "        'waste':     comps['waste'] - comps['waste'],        # same x ‚Üí same waste\n",
    "        'quantile':  comps['quantile'] - comps['quantile'],  # same x ‚Üí same quantile\n",
    "    }\n",
    "\n",
    "    # For stockout/waste/quantile the action doesn't change these immediately,\n",
    "    # but through next-state value. Approximate via Q-value residual.\n",
    "    q_best   = q_vals[np.arange(NUM_PRODUCTS), best_a]\n",
    "    q_second = q_vals[np.arange(NUM_PRODUCTS), second_a]\n",
    "    q_diff   = q_best - q_second  # [P]\n",
    "    explained = delta_q['overstock']\n",
    "    residual  = q_diff - explained\n",
    "\n",
    "    # Distribute residual proportionally among state-dependent objectives\n",
    "    penalty_magnitudes = np.stack([\n",
    "        comps['stockout'], np.zeros(NUM_PRODUCTS), comps['waste'], comps['quantile']\n",
    "    ])  # [4, P]\n",
    "    total_pen = penalty_magnitudes.sum(axis=0, keepdims=True) + 1e-8\n",
    "    weights = penalty_magnitudes / total_pen  # [4, P]\n",
    "\n",
    "    for i, obj in enumerate(OBJECTIVES):\n",
    "        if obj != 'overstock':\n",
    "            delta_q[obj] = weights[i] * residual\n",
    "\n",
    "    return delta_q, best_a\n",
    "\n",
    "\n",
    "def rdx_a2c(actor, critic, state_pp, x_vec):\n",
    "    \"\"\"\n",
    "    RDX for A2C: Decompose Advantage into 4 objective contributions.\n",
    "\n",
    "    Advantage A(s,a) = Q(s,a) - V(s) ‚âà r + Œ≥V(s') - V(s)\n",
    "    Decompose r into 4 components, so A^k(s,a) ‚âà r^k (immediate sub-reward)\n",
    "\n",
    "    Returns:\n",
    "        delta_q: dict[str, np.ndarray[P]]\n",
    "        best_actions: np.ndarray[P]\n",
    "    \"\"\"\n",
    "    probs    = actor(state_pp).numpy()     # [P, A]\n",
    "    best_a   = np.argmax(probs, axis=1)    # [P]\n",
    "    u_best   = ACTION_SPACE[best_a]\n",
    "\n",
    "    comps  = compute_reward_components(x_vec)\n",
    "    os_val = np.maximum(0, x_vec + u_best - 1)\n",
    "\n",
    "    # Sub-reward components (sign: positive = good, negative = penalty)\n",
    "    delta_q = {\n",
    "        'stockout':  -comps['stockout'],\n",
    "        'overstock': -os_val,\n",
    "        'waste':     -comps['waste'],\n",
    "        'quantile':  -comps['quantile'],\n",
    "    }\n",
    "    return delta_q, best_a\n",
    "\n",
    "print(\"RDX module defined ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911b56b6",
   "metadata": {},
   "source": [
    "### 2.2 MSX (Minimal Sufficient Explanation)\n",
    "\n",
    "Thu·∫≠t to√°n t√¨m t·∫≠p h·ª£p m·ª•c ti√™u nh·ªè nh·∫•t sao cho:\n",
    "$$\\sum_{k \\in MSX} |\\Delta Q^k| \\geq \\lambda \\times Q_{threshold}$$\n",
    "\n",
    "trong ƒë√≥ $Q_{threshold} = \\sum_k |\\Delta Q^k|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e3ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MSX: Minimal Sufficient Explanation\n",
    "# ============================================================\n",
    "\n",
    "def compute_msx(delta_q, lam=1.0):\n",
    "    \"\"\"\n",
    "    Find the Minimal Sufficient eXplanation set.\n",
    "\n",
    "    Args:\n",
    "        delta_q: dict[str, np.ndarray[P]] ‚Äî ŒîQ per objective per product\n",
    "        lam: float ‚Äî threshold multiplier\n",
    "\n",
    "    Returns:\n",
    "        msx_sets: list[set] ‚Äî MSX set per product\n",
    "        msx_sizes: np.ndarray[P] ‚Äî size of MSX per product\n",
    "    \"\"\"\n",
    "    # Stack: [4, P]\n",
    "    dq_matrix = np.stack([np.abs(delta_q[obj]) for obj in OBJECTIVES])  # [4, P]\n",
    "    total_dq  = dq_matrix.sum(axis=0)  # [P]\n",
    "    threshold = lam * total_dq         # [P]\n",
    "\n",
    "    msx_sets  = []\n",
    "    msx_sizes = np.zeros(NUM_PRODUCTS, dtype=int)\n",
    "\n",
    "    for p in range(NUM_PRODUCTS):\n",
    "        obj_importance = [(dq_matrix[k, p], OBJECTIVES[k]) for k in range(4)]\n",
    "        obj_importance.sort(key=lambda t: -t[0])  # descending by |ŒîQ|\n",
    "\n",
    "        cumsum = 0.0\n",
    "        msx = set()\n",
    "        for val, name in obj_importance:\n",
    "            msx.add(name)\n",
    "            cumsum += val\n",
    "            if cumsum >= threshold[p]:\n",
    "                break\n",
    "        msx_sets.append(msx)\n",
    "        msx_sizes[p] = len(msx)\n",
    "\n",
    "    return msx_sets, msx_sizes\n",
    "\n",
    "\n",
    "def msx_stability(delta_q, lambda_values):\n",
    "    \"\"\"\n",
    "    Measure MSX stability across different Œª values.\n",
    "    Returns: float ‚Äî fraction of products whose MSX changed between consecutive Œª.\n",
    "    \"\"\"\n",
    "    prev_sets = None\n",
    "    changes   = []\n",
    "\n",
    "    for lam in lambda_values:\n",
    "        curr_sets, _ = compute_msx(delta_q, lam)\n",
    "        if prev_sets is not None:\n",
    "            n_changed = sum(1 for a, b in zip(prev_sets, curr_sets) if a != b)\n",
    "            changes.append(n_changed / NUM_PRODUCTS)\n",
    "        prev_sets = curr_sets\n",
    "\n",
    "    return np.mean(changes) if changes else 0.0  # avg change rate\n",
    "\n",
    "print(\"MSX module defined ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1521fd9",
   "metadata": {},
   "source": [
    "### 2.3 SHAP Module\n",
    "\n",
    "Wrapper s·ª≠ d·ª•ng `shap.GradientExplainer`:\n",
    "- **DQN**: SHAP tr√™n Q-values ‚Äî Input `[B, 660]`, output Q cho action ƒë∆∞·ª£c ch·ªçn\n",
    "- **A2C**: SHAP tr√™n Action Probabilities ‚Äî Input `[B*P, 3]`, output softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b4549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SHAP Module\n",
    "# ============================================================\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "    print(f\"SHAP version: {shap.__version__} ‚úì\")\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è SHAP not installed ‚Äî SHAP_only and Combined configs will use fallback\")\n",
    "\n",
    "\n",
    "def shap_dqn(q_network, background_states, eval_states):\n",
    "    \"\"\"\n",
    "    SHAP for DQN: feature importance on Q-values.\n",
    "\n",
    "    Args:\n",
    "        q_network: MultiProductQNetwork\n",
    "        background_states: np.ndarray [N_bg, 660] ‚Äî background dataset\n",
    "        eval_states: np.ndarray [N_eval, 660] ‚Äî states to explain\n",
    "\n",
    "    Returns:\n",
    "        shap_vals: np.ndarray [N_eval, 660] ‚Äî SHAP values\n",
    "        shap_per_feature: np.ndarray [N_eval, 3] ‚Äî aggregated per feature type\n",
    "            (mean abs SHAP across products for each of: inventory, sales, waste)\n",
    "    \"\"\"\n",
    "    if not SHAP_AVAILABLE:\n",
    "        # Fallback: gradient-based importance\n",
    "        return _gradient_importance_dqn(q_network, eval_states)\n",
    "\n",
    "    # Wrap model for SHAP: return mean Q across products for best action\n",
    "    @tf.function\n",
    "    def model_fn(x):\n",
    "        q_all = q_network(x, training=False)          # [B, P, A]\n",
    "        q_max = tf.reduce_max(q_all, axis=-1)          # [B, P]\n",
    "        return tf.reduce_mean(q_max, axis=-1, keepdims=True)  # [B, 1]\n",
    "\n",
    "    bg = tf.constant(background_states[:50], dtype=tf.float32)\n",
    "    explainer = shap.GradientExplainer(model_fn, bg)\n",
    "    sv = explainer.shap_values(tf.constant(eval_states, dtype=tf.float32))\n",
    "\n",
    "    if isinstance(sv, list):\n",
    "        sv = sv[0]\n",
    "    sv = np.array(sv).reshape(len(eval_states), -1)  # [N, 660]\n",
    "\n",
    "    # Aggregate per feature type:\n",
    "    # Layout: [x_0..x_219, sales_0..sales_219, q_0..q_219]\n",
    "    shap_pf = np.stack([\n",
    "        np.mean(np.abs(sv[:, :220]), axis=1),       # inventory\n",
    "        np.mean(np.abs(sv[:, 220:440]), axis=1),     # sales\n",
    "        np.mean(np.abs(sv[:, 440:]), axis=1),         # waste\n",
    "    ], axis=1)  # [N, 3]\n",
    "\n",
    "    return sv, shap_pf\n",
    "\n",
    "\n",
    "def shap_a2c(actor, background_states_pp, eval_states_pp):\n",
    "    \"\"\"\n",
    "    SHAP for A2C Actor: feature importance on action probabilities.\n",
    "\n",
    "    Args:\n",
    "        actor: Actor model\n",
    "        background_states_pp: np.ndarray [N_bg, 3] ‚Äî per-product states\n",
    "        eval_states_pp: np.ndarray [N_eval, 3]\n",
    "\n",
    "    Returns:\n",
    "        shap_vals: np.ndarray [N_eval, 3]\n",
    "        shap_per_feature: np.ndarray [N_eval, 3]\n",
    "    \"\"\"\n",
    "    if not SHAP_AVAILABLE:\n",
    "        return _gradient_importance_a2c(actor, eval_states_pp)\n",
    "\n",
    "    # Wrap: Actor is tf.Module, wrap in a tf.function for SHAP\n",
    "    @tf.function\n",
    "    def model_fn(x):\n",
    "        probs = actor(x)  # [B, 14]\n",
    "        return tf.reduce_max(probs, axis=-1, keepdims=True)  # [B, 1]\n",
    "\n",
    "    bg = tf.constant(background_states_pp[:100], dtype=tf.float32)\n",
    "    explainer = shap.GradientExplainer(model_fn, bg)\n",
    "    sv = explainer.shap_values(tf.constant(eval_states_pp, dtype=tf.float32))\n",
    "\n",
    "    if isinstance(sv, list):\n",
    "        sv = sv[0]\n",
    "    sv = np.array(sv)  # [N, 3]\n",
    "    return sv, np.abs(sv)\n",
    "\n",
    "\n",
    "def _gradient_importance_dqn(q_network, eval_states):\n",
    "    \"\"\"Fallback: gradient-based feature importance for DQN.\"\"\"\n",
    "    x = tf.constant(eval_states, dtype=tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        q_all = q_network(x, training=False)\n",
    "        q_max = tf.reduce_max(q_all, axis=-1)\n",
    "        out   = tf.reduce_mean(q_max, axis=-1)\n",
    "    grads = tape.gradient(out, x).numpy()  # [N, 660]\n",
    "    shap_pf = np.stack([\n",
    "        np.mean(np.abs(grads[:, :220]), axis=1),\n",
    "        np.mean(np.abs(grads[:, 220:440]), axis=1),\n",
    "        np.mean(np.abs(grads[:, 440:]), axis=1),\n",
    "    ], axis=1)\n",
    "    return grads, shap_pf\n",
    "\n",
    "\n",
    "def _gradient_importance_a2c(actor, eval_states_pp):\n",
    "    \"\"\"Fallback: gradient-based feature importance for A2C.\"\"\"\n",
    "    x = tf.Variable(eval_states_pp, dtype=tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs = actor(x)\n",
    "        out   = tf.reduce_mean(tf.reduce_max(probs, axis=-1))\n",
    "    grads = tape.gradient(out, x).numpy()\n",
    "    return grads, np.abs(grads)\n",
    "\n",
    "print(\"SHAP module defined ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af8ffdd",
   "metadata": {},
   "source": [
    "## Step 3: XAI Evaluation Metrics\n",
    "\n",
    "| Metric | Formula | Description |\n",
    "|--------|---------|-------------|\n",
    "| **OCS** | `Œ£(|ŒîQ^k| > Œ∏_Q) / 4` | Objective Coverage Score |\n",
    "| **FCS** | `Œ£(|SHAP_f| > Œ∏_œÜ) / 3` | Feature Coverage Score |\n",
    "| **CAS** | `Jaccard(top_features, mapped_objectives)` | Cross-domain Alignment |\n",
    "| **Stability** | `avg % MSX change across Œª` | MSX robustness |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# XAI EVALUATION METRICS\n",
    "# ============================================================\n",
    "\n",
    "def ocs(delta_q, theta_q=0.01):\n",
    "    \"\"\"\n",
    "    Objective Coverage Score: fraction of objectives with |ŒîQ^k| > Œ∏_Q.\n",
    "    Returns: float ‚Äî mean across products.\n",
    "    \"\"\"\n",
    "    scores = np.zeros(NUM_PRODUCTS)\n",
    "    for p in range(NUM_PRODUCTS):\n",
    "        n_active = sum(1 for obj in OBJECTIVES if abs(delta_q[obj][p]) > theta_q)\n",
    "        scores[p] = n_active / len(OBJECTIVES)\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "def fcs(shap_per_feature, theta_phi=0.001):\n",
    "    \"\"\"\n",
    "    Feature Coverage Score: fraction of features with mean|SHAP| > Œ∏_œÜ.\n",
    "    shap_per_feature: [N_states, 3]\n",
    "    Returns: float ‚Äî mean across states.\n",
    "    \"\"\"\n",
    "    mean_shap = np.mean(np.abs(shap_per_feature), axis=0)  # [3]\n",
    "    return float(np.mean(mean_shap > theta_phi))\n",
    "\n",
    "\n",
    "def cas(delta_q, shap_per_feature, theta_q=0.01, theta_phi=0.001):\n",
    "    \"\"\"\n",
    "    Cross-domain Alignment Score using Jaccard Similarity.\n",
    "\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  FEATURE ‚Üí OBJECTIVE MAPPING (for CAS computation)                ‚îÇ\n",
    "    ‚îÇ                                                                     ‚îÇ\n",
    "    ‚îÇ  This mapping encodes the CAUSAL RELATIONSHIP between input         ‚îÇ\n",
    "    ‚îÇ  features and reward sub-components:                                ‚îÇ\n",
    "    ‚îÇ                                                                     ‚îÇ\n",
    "    ‚îÇ  feature 'inventory' (x) ‚Üí objectives 'stockout', 'overstock'      ‚îÇ\n",
    "    ‚îÇ    ‚ñ† x < zero_inv ‚Üí z=1 (stockout penalty)                         ‚îÇ\n",
    "    ‚îÇ    ‚ñ† x + u > 1    ‚Üí overstock penalty                              ‚îÇ\n",
    "    ‚îÇ    ‚ñ† Inventory is the PRIMARY driver of both these penalties.       ‚îÇ\n",
    "    ‚îÇ                                                                     ‚îÇ\n",
    "    ‚îÇ  feature 'sales'         ‚Üí objective 'stockout'                     ‚îÇ\n",
    "    ‚îÇ    ‚ñ† High sales relative to x ‚Üí stockout. Sales forecasting        ‚îÇ\n",
    "    ‚îÇ      accuracy directly impacts stockout risk.                       ‚îÇ\n",
    "    ‚îÇ                                                                     ‚îÇ\n",
    "    ‚îÇ  feature 'waste_feat' (q)‚Üí objective 'waste'                        ‚îÇ\n",
    "    ‚îÇ    ‚ñ† q = waste_rate √ó x: waste feature is the DIRECT input to      ‚îÇ\n",
    "    ‚îÇ      the waste penalty term.                                        ‚îÇ\n",
    "    ‚îÇ                                                                     ‚îÇ\n",
    "    ‚îÇ  No feature maps directly to 'quantile' (it's a global statistic  ‚îÇ\n",
    "    ‚îÇ  across all products, not a per-product feature).                   ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    \"\"\"\n",
    "    FEATURE_TO_OBJECTIVE = {\n",
    "        'inventory':  {'stockout', 'overstock'},\n",
    "        'sales':      {'stockout'},\n",
    "        'waste_feat': {'waste'},\n",
    "    }\n",
    "\n",
    "    # Top features: those with |SHAP| > Œ∏_œÜ\n",
    "    mean_shap = np.mean(np.abs(shap_per_feature), axis=0)  # [3]\n",
    "    top_features = set()\n",
    "    for i, feat in enumerate(FEATURES):\n",
    "        if mean_shap[i] > theta_phi:\n",
    "            top_features.add(feat)\n",
    "\n",
    "    # Map top features ‚Üí expected objectives\n",
    "    expected_objectives = set()\n",
    "    for feat in top_features:\n",
    "        expected_objectives.update(FEATURE_TO_OBJECTIVE.get(feat, set()))\n",
    "\n",
    "    # Top objectives: those with |ŒîQ^k| > Œ∏_Q (mean across products)\n",
    "    detected_objectives = set()\n",
    "    for obj in OBJECTIVES:\n",
    "        if np.mean(np.abs(delta_q[obj])) > theta_q:\n",
    "            detected_objectives.add(obj)\n",
    "\n",
    "    # Jaccard similarity\n",
    "    intersection = expected_objectives & detected_objectives\n",
    "    union        = expected_objectives | detected_objectives\n",
    "    return float(len(intersection) / max(len(union), 1))\n",
    "\n",
    "\n",
    "print(\"Metrics (OCS, FCS, CAS, Stability) defined ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdaaa98",
   "metadata": {},
   "source": [
    "## Step 4: Main Ablation Loop\n",
    "\n",
    "Duy·ªát qua l∆∞·ªõi th√≠ nghi·ªám:\n",
    "- **Agents**: DQN, A2C_mod\n",
    "- **Scenarios**: EASY (low variance sales), MEDIUM (default), HARD (high variance, high waste)\n",
    "- **Configs**: RDX_only, SHAP_only, Combined\n",
    "- **Œª values**: 0.5, 1.0, 1.5, 2.0\n",
    "\n",
    "M·ªói c·∫•u h√¨nh: l·∫•y 496 states t·ª´ test data, ch·∫°y XAI t∆∞∆°ng ·ª©ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4fde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SCENARIO DEFINITIONS\n",
    "# Modify environment parameters to create difficulty levels\n",
    "# ============================================================\n",
    "\n",
    "SCENARIOS = {\n",
    "    'EASY':   {'sales_scale': 0.5, 'waste_rate': 0.010},\n",
    "    'MEDIUM': {'sales_scale': 1.0, 'waste_rate': 0.025},\n",
    "    'HARD':   {'sales_scale': 1.5, 'waste_rate': 0.050},\n",
    "}\n",
    "\n",
    "XAI_CONFIGS   = ['RDX_only', 'SHAP_only', 'Combined']\n",
    "LAMBDA_VALUES = [0.5, 1.0, 1.5, 2.0]\n",
    "AGENT_NAMES   = ['DQN', 'A2C_mod']\n",
    "N_STATES       = 496  # number of test states to evaluate\n",
    "\n",
    "print(\"Experiment grid:\")\n",
    "total_runs = len(AGENT_NAMES) * len(SCENARIOS) * len(XAI_CONFIGS) * len(LAMBDA_VALUES)\n",
    "print(f\"  {len(AGENT_NAMES)} agents √ó {len(SCENARIOS)} scenarios √ó \"\n",
    "      f\"{len(XAI_CONFIGS)} configs √ó {len(LAMBDA_VALUES)} Œª = {total_runs} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c6da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COLLECT STATES FOR EACH SCENARIO\n",
    "# ============================================================\n",
    "\n",
    "def collect_states(scenario_params, n_states=N_STATES):\n",
    "    \"\"\"\n",
    "    Run environment forward to collect n_states from test data.\n",
    "    Applies scenario-specific sales scaling and waste rate.\n",
    "\n",
    "    Returns:\n",
    "        states_flat: np.ndarray [n_states, 660] ‚Äî for DQN\n",
    "        states_pp:   np.ndarray [n_states, P, 3] ‚Äî for A2C (per-product)\n",
    "        x_vecs:      np.ndarray [n_states, P]    ‚Äî raw inventory vectors\n",
    "    \"\"\"\n",
    "    scale = scenario_params['sales_scale']\n",
    "    wr    = scenario_params['waste_rate']\n",
    "\n",
    "    x = x_init.copy()\n",
    "    states_flat = []\n",
    "    states_pp   = []\n",
    "    x_vecs      = []\n",
    "\n",
    "    for t in range(min(n_states, T_MAX)):\n",
    "        sales = all_sales[t] * scale\n",
    "        q     = wr * x\n",
    "\n",
    "        # Per-product state [P, 3]\n",
    "        s_pp = np.stack([x, sales, q], axis=1).astype(np.float32)\n",
    "        # Flat state [660] = [x_0..x_P, sales_0..sales_P, q_0..q_P]\n",
    "        s_flat = np.concatenate([x, sales, q]).astype(np.float32)\n",
    "\n",
    "        states_pp.append(s_pp)\n",
    "        states_flat.append(s_flat)\n",
    "        x_vecs.append(x.copy())\n",
    "\n",
    "        # Step environment forward using A2C actor (default policy)\n",
    "        probs = agents['actor'](s_pp).numpy()\n",
    "        a_idx = np.argmax(probs, axis=1)\n",
    "        u     = ACTION_SPACE[a_idx]\n",
    "        x_u   = np.minimum(1, x + u)\n",
    "        x     = np.maximum(0, x_u - sales)\n",
    "\n",
    "    return (np.array(states_flat, dtype=np.float32),\n",
    "            np.array(states_pp, dtype=np.float32),\n",
    "            np.array(x_vecs, dtype=np.float32))\n",
    "\n",
    "# Pre-collect states for each scenario\n",
    "scenario_states = {}\n",
    "for scn_name, scn_params in SCENARIOS.items():\n",
    "    scenario_states[scn_name] = collect_states(scn_params)\n",
    "    print(f\"  Scenario {scn_name}: {scenario_states[scn_name][0].shape[0]} states collected\")\n",
    "\n",
    "print(\"\\n‚úÖ All scenario states collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633747d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN ABLATION LOOP\n",
    "# ============================================================\n",
    "\n",
    "results = []\n",
    "t_start = time.time()\n",
    "\n",
    "for agent_name in AGENT_NAMES:\n",
    "    for scn_name in SCENARIOS:\n",
    "        states_flat, states_pp, x_vecs = scenario_states[scn_name]\n",
    "        n = states_flat.shape[0]\n",
    "\n",
    "        # ‚îÄ‚îÄ Pre-compute RDX for this (agent, scenario) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        all_delta_q = {obj: np.zeros(NUM_PRODUCTS) for obj in OBJECTIVES}\n",
    "        all_best_a  = np.zeros(NUM_PRODUCTS, dtype=int)\n",
    "\n",
    "        # Aggregate RDX across sampled states (use every 10th for speed)\n",
    "        rdx_indices = np.arange(0, n, max(1, n // 50))\n",
    "        for idx in rdx_indices:\n",
    "            if agent_name == 'DQN':\n",
    "                dq, ba = rdx_dqn(agents['q_network'],\n",
    "                                  states_flat[idx], x_vecs[idx])\n",
    "            else:\n",
    "                dq, ba = rdx_a2c(agents['actor'], agents['critic'],\n",
    "                                  states_pp[idx], x_vecs[idx])\n",
    "            for obj in OBJECTIVES:\n",
    "                all_delta_q[obj] += dq[obj]\n",
    "            all_best_a = ba  # keep last for reference\n",
    "\n",
    "        # Average\n",
    "        for obj in OBJECTIVES:\n",
    "            all_delta_q[obj] /= len(rdx_indices)\n",
    "\n",
    "        # ‚îÄ‚îÄ Pre-compute SHAP (agent, scenario) ‚Äî only if needed ‚îÄ‚îÄ\n",
    "        shap_pf = None\n",
    "        bg_idx = np.random.choice(n, min(50, n), replace=False)\n",
    "\n",
    "        for xai_cfg in XAI_CONFIGS:\n",
    "            run_rdx  = xai_cfg in ('RDX_only', 'Combined')\n",
    "            run_shap = xai_cfg in ('SHAP_only', 'Combined')\n",
    "\n",
    "            # Compute SHAP if needed and not yet computed\n",
    "            if run_shap and shap_pf is None:\n",
    "                eval_idx = np.random.choice(n, min(100, n), replace=False)\n",
    "                if agent_name == 'DQN':\n",
    "                    _, shap_pf = shap_dqn(agents['q_network'],\n",
    "                                           states_flat[bg_idx], states_flat[eval_idx])\n",
    "                else:\n",
    "                    # Flatten per-product states for SHAP\n",
    "                    bg_pp   = states_pp[bg_idx].reshape(-1, 3)\n",
    "                    eval_pp = states_pp[eval_idx].reshape(-1, 3)\n",
    "                    _, shap_pf = shap_a2c(agents['actor'], bg_pp, eval_pp)\n",
    "                    # Average back to [N_eval, 3]\n",
    "                    shap_pf = shap_pf.reshape(len(eval_idx), NUM_PRODUCTS, 3).mean(axis=1)\n",
    "\n",
    "            for lam in LAMBDA_VALUES:\n",
    "                row = {\n",
    "                    'agent':    agent_name,\n",
    "                    'scenario': scn_name,\n",
    "                    'xai_config': xai_cfg,\n",
    "                    'lambda':   lam,\n",
    "                }\n",
    "\n",
    "                # OCS\n",
    "                if run_rdx:\n",
    "                    row['OCS'] = ocs(all_delta_q)\n",
    "                else:\n",
    "                    row['OCS'] = np.nan\n",
    "\n",
    "                # FCS\n",
    "                if run_shap and shap_pf is not None:\n",
    "                    row['FCS'] = fcs(shap_pf)\n",
    "                else:\n",
    "                    row['FCS'] = np.nan\n",
    "\n",
    "                # CAS (only for Combined)\n",
    "                if xai_cfg == 'Combined' and shap_pf is not None:\n",
    "                    row['CAS'] = cas(all_delta_q, shap_pf)\n",
    "                else:\n",
    "                    row['CAS'] = np.nan\n",
    "\n",
    "                # Stability\n",
    "                if run_rdx:\n",
    "                    row['Stability'] = 1.0 - msx_stability(all_delta_q, LAMBDA_VALUES)\n",
    "                else:\n",
    "                    row['Stability'] = np.nan\n",
    "\n",
    "                # MSX size\n",
    "                if run_rdx:\n",
    "                    _, msx_sizes = compute_msx(all_delta_q, lam)\n",
    "                    row['MSX_size_mean'] = float(msx_sizes.mean())\n",
    "                else:\n",
    "                    row['MSX_size_mean'] = np.nan\n",
    "\n",
    "                results.append(row)\n",
    "\n",
    "        # Reset shap cache for next scenario\n",
    "        shap_pf = None\n",
    "\n",
    "    print(f\"  Agent {agent_name} done ({time.time()-t_start:.1f}s)\")\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('ablation_results.csv', index=False)\n",
    "print(f\"\\n‚úÖ Ablation complete: {len(df)} rows saved to ablation_results.csv\")\n",
    "print(f\"   Total time: {time.time()-t_start:.1f}s\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b38b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESULTS OVERVIEW\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"ABLATION RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show pivot tables\n",
    "for metric in ['OCS', 'FCS', 'CAS', 'Stability', 'MSX_size_mean']:\n",
    "    valid = df.dropna(subset=[metric])\n",
    "    if len(valid) == 0:\n",
    "        continue\n",
    "    print(f\"\\n{'‚îÄ'*50}\")\n",
    "    print(f\"  {metric}\")\n",
    "    print(f\"{'‚îÄ'*50}\")\n",
    "    pivot = valid.pivot_table(values=metric,\n",
    "                              index=['agent', 'scenario'],\n",
    "                              columns='xai_config',\n",
    "                              aggfunc='mean')\n",
    "    print(pivot.round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d888b",
   "metadata": {},
   "source": [
    "## Step 5: Visualization & Statistical Testing\n",
    "\n",
    "### 5.1 Line Chart: Stability vs Œª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae99f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT 1: Stability vs Œª ‚Äî DQN vs A2C\n",
    "# ============================================================\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for agent_name in AGENT_NAMES:\n",
    "    subset = df[(df['agent'] == agent_name) &\n",
    "                (df['xai_config'].isin(['RDX_only', 'Combined'])) &\n",
    "                (df['Stability'].notna())]\n",
    "    if subset.empty:\n",
    "        continue\n",
    "    grouped = subset.groupby('lambda')['Stability'].mean()\n",
    "    ax.plot(grouped.index, grouped.values,\n",
    "            marker='o', linewidth=2.5, markersize=8,\n",
    "            label=agent_name)\n",
    "\n",
    "ax.set_xlabel('Œª (MSX Threshold Multiplier)', fontsize=13)\n",
    "ax.set_ylabel('Stability (1 - avg % MSX change)', fontsize=13)\n",
    "ax.set_title('MSX Stability vs Œª: DQN vs A2C_mod', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ablation_stability_vs_lambda.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: ablation_stability_vs_lambda.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a61b9",
   "metadata": {},
   "source": [
    "### 5.2 Heatmap: CAS (Agent √ó Scenario) ‚Äî Combined Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4997550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT 2: CAS Heatmap ‚Äî Agent √ó Scenario (Combined config)\n",
    "# ============================================================\n",
    "cas_data = df[(df['xai_config'] == 'Combined') & (df['CAS'].notna())]\n",
    "\n",
    "if not cas_data.empty:\n",
    "    pivot_cas = cas_data.pivot_table(values='CAS',\n",
    "                                      index='agent',\n",
    "                                      columns='scenario',\n",
    "                                      aggfunc='mean')\n",
    "    # Reorder columns\n",
    "    col_order = [c for c in ['EASY', 'MEDIUM', 'HARD'] if c in pivot_cas.columns]\n",
    "    pivot_cas = pivot_cas[col_order]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    sns.heatmap(pivot_cas, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "                vmin=0, vmax=1, linewidths=1.5, ax=ax,\n",
    "                annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    ax.set_title('Cross-domain Alignment Score (CAS)\\nCombined XAI Config',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Agent', fontsize=12)\n",
    "    ax.set_xlabel('Scenario', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ablation_cas_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Saved: ablation_cas_heatmap.png\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No CAS data available for Combined config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12737c",
   "metadata": {},
   "source": [
    "### 5.3 Stacked Area Chart: Dominance Ratio of Reward Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366617bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT 3: Stacked Area ‚Äî Reward Component Dominance over States\n",
    "# Uses MEDIUM scenario, both agents\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "colors = ['#E53935', '#FF9800', '#7B1FA2', '#1565C0']\n",
    "\n",
    "for ax_idx, agent_name in enumerate(AGENT_NAMES):\n",
    "    ax = axes[ax_idx]\n",
    "    s_flat, s_pp, x_vs = scenario_states['MEDIUM']\n",
    "    n = min(100, s_flat.shape[0])  # limit for clarity\n",
    "\n",
    "    comp_matrix = np.zeros((n, 4))  # [n_states, 4 objectives]\n",
    "\n",
    "    for t in range(n):\n",
    "        if agent_name == 'DQN':\n",
    "            dq, _ = rdx_dqn(agents['q_network'], s_flat[t], x_vs[t])\n",
    "        else:\n",
    "            dq, _ = rdx_a2c(agents['actor'], agents['critic'], s_pp[t], x_vs[t])\n",
    "\n",
    "        for k, obj in enumerate(OBJECTIVES):\n",
    "            comp_matrix[t, k] = np.mean(np.abs(dq[obj]))\n",
    "\n",
    "    # Normalize to proportions\n",
    "    row_sums = comp_matrix.sum(axis=1, keepdims=True) + 1e-8\n",
    "    comp_pct = comp_matrix / row_sums  # [n, 4]\n",
    "\n",
    "    x_axis = np.arange(n)\n",
    "    ax.stackplot(x_axis, comp_pct.T, labels=OBJECTIVES if ax_idx == 0 else None,\n",
    "                 colors=colors, alpha=0.8)\n",
    "    ax.set_title(f'{agent_name}', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('State Index')\n",
    "    if ax_idx == 0:\n",
    "        ax.set_ylabel('Dominance Ratio')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "axes[0].legend(loc='upper left', fontsize=9, ncol=2)\n",
    "plt.suptitle('Reward Component Dominance Ratio Across States (MEDIUM)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ablation_dominance_ratio.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: ablation_dominance_ratio.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d6236",
   "metadata": {},
   "source": [
    "### 5.4 Additional: OCS Comparison and MSX Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a162917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT 4: OCS Comparison across configs\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# OCS by agent and config\n",
    "ocs_data = df[df['OCS'].notna()]\n",
    "if not ocs_data.empty:\n",
    "    pivot_ocs = ocs_data.pivot_table(values='OCS',\n",
    "                                      index=['agent'],\n",
    "                                      columns=['scenario'],\n",
    "                                      aggfunc='mean')\n",
    "    col_order = [c for c in ['EASY', 'MEDIUM', 'HARD'] if c in pivot_ocs.columns]\n",
    "    pivot_ocs[col_order].plot(kind='bar', ax=axes[0], rot=0)\n",
    "    axes[0].set_title('Objective Coverage Score (OCS)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('OCS')\n",
    "    axes[0].legend(title='Scenario')\n",
    "    axes[0].set_ylim(0, 1.1)\n",
    "\n",
    "# MSX size by lambda\n",
    "msx_data = df[df['MSX_size_mean'].notna()]\n",
    "if not msx_data.empty:\n",
    "    for agent_name in AGENT_NAMES:\n",
    "        subset = msx_data[msx_data['agent'] == agent_name]\n",
    "        grouped = subset.groupby('lambda')['MSX_size_mean'].mean()\n",
    "        axes[1].plot(grouped.index, grouped.values,\n",
    "                     marker='s', linewidth=2, label=agent_name)\n",
    "    axes[1].set_xlabel('Œª')\n",
    "    axes[1].set_ylabel('Mean MSX Size')\n",
    "    axes[1].set_title('MSX Set Size vs Œª', fontsize=12, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ablation_ocs_msx.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: ablation_ocs_msx.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5775073",
   "metadata": {},
   "source": [
    "## Step 6: Sensitivity Analysis\n",
    "\n",
    "### M·ª•c ti√™u\n",
    "1. **Sinh d·ªØ li·ªáu test** cho 3 k·ªãch b·∫£n EASY / MEDIUM / HARD b·∫±ng c√°ch scale\n",
    "   `x` (inventory), `sales`, v√† `waste_rate` theo t·ª∑ l·ªá quy ƒë·ªãnh.\n",
    "2. **Gi·∫£ l·∫≠p thay ƒë·ªïi tr·ªçng s·ªë RDX**: Duy·ªát qua t·ª´ng component ($w_s, w_h, w_w, w_o$),\n",
    "   nh√¢n RDX c·ªßa n√≥ v·ªõi $\\lambda \\in \\{0.5, 1.0, 1.5, 2.0\\}$ trong khi gi·ªØ nguy√™n c√°c RDX kh√°c.\n",
    "3. **Tr·ª±c quan h√≥a**: Line chart Mean RDX theo Œª + Stacked Bar chart MSX inclusion frequency.\n",
    "\n",
    "### Scenario Data Scaling\n",
    "| Parameter | EASY | MEDIUM | HARD |\n",
    "|-----------|------|--------|------|\n",
    "| `x` (inventory) scale | 30% | 60% | 90% |\n",
    "| `sales` scale | 20% | 50% | 80% |\n",
    "| `waste_rate` | 1% | 5% | 15% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac70d0b",
   "metadata": {},
   "source": [
    "### 6.1 Sinh d·ªØ li·ªáu test cho c√°c k·ªãch b·∫£n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304785bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SENSITIVITY SCENARIO DATA GENERATION\n",
    "# ============================================================\n",
    "# Scale factors theo y√™u c·∫ßu:\n",
    "#   EASY:   x=30%, sales=20%, waste_rate=1%\n",
    "#   MEDIUM: x=60%, sales=50%, waste_rate=5%\n",
    "#   HARD:   x=90%, sales=80%, waste_rate=15%\n",
    "\n",
    "SENSITIVITY_SCENARIOS = {\n",
    "    'EASY':   {'x_scale': 0.30, 'sales_scale': 0.20, 'waste_rate': 0.01},\n",
    "    'MEDIUM': {'x_scale': 0.60, 'sales_scale': 0.50, 'waste_rate': 0.05},\n",
    "    'HARD':   {'x_scale': 0.90, 'sales_scale': 0.80, 'waste_rate': 0.15},\n",
    "}\n",
    "\n",
    "def generate_scenario_data(scenario_params, n_states=N_STATES):\n",
    "    \"\"\"\n",
    "    Sinh t·∫≠p test data cho t·ª´ng k·ªãch b·∫£n Sensitivity Analysis.\n",
    "\n",
    "    Kh√°c v·ªõi collect_states() ·ªü Step 4:\n",
    "      - x_init ƒë∆∞·ª£c SCALE theo x_scale (m√¥ ph·ªèng m·ª©c t·ªìn kho ban ƒë·∫ßu kh√°c nhau)\n",
    "      - sales ƒë∆∞·ª£c SCALE theo sales_scale (m√¥ ph·ªèng nhu c·∫ßu kh√°c nhau)\n",
    "      - waste_rate thay ƒë·ªïi theo k·ªãch b·∫£n (kh√¥ng d√πng WASTE_RATE to√†n c·ª•c)\n",
    "\n",
    "    Args:\n",
    "        scenario_params: dict v·ªõi 'x_scale', 'sales_scale', 'waste_rate'\n",
    "        n_states: s·ªë l∆∞·ª£ng states c·∫ßn sinh\n",
    "\n",
    "    Returns:\n",
    "        states_flat: np.ndarray [n_states, 660] ‚Äî cho DQN\n",
    "        states_pp:   np.ndarray [n_states, P, 3] ‚Äî cho A2C (per-product)\n",
    "        x_vecs:      np.ndarray [n_states, P]    ‚Äî inventory vectors th√¥\n",
    "    \"\"\"\n",
    "    x_sc  = scenario_params['x_scale']\n",
    "    s_sc  = scenario_params['sales_scale']\n",
    "    wr    = scenario_params['waste_rate']\n",
    "\n",
    "    # Scale initial inventory\n",
    "    x = (x_init * x_sc).astype(np.float32)\n",
    "\n",
    "    states_flat, states_pp, x_vecs = [], [], []\n",
    "\n",
    "    for t in range(min(n_states, T_MAX)):\n",
    "        # Scale sales\n",
    "        sales = (all_sales[t] * s_sc).astype(np.float32)\n",
    "        # Waste with scenario-specific rate\n",
    "        q = (wr * x).astype(np.float32)\n",
    "\n",
    "        # Build state representations\n",
    "        s_pp   = np.stack([x, sales, q], axis=1).astype(np.float32)      # [P, 3]\n",
    "        s_flat = np.concatenate([x, sales, q]).astype(np.float32)  # [660]\n",
    "\n",
    "        states_pp.append(s_pp)\n",
    "        states_flat.append(s_flat)\n",
    "        x_vecs.append(x.copy())\n",
    "\n",
    "        # Step environment forward (A2C policy)\n",
    "        probs = agents['actor'](s_pp).numpy()\n",
    "        a_idx = np.argmax(probs, axis=1)\n",
    "        u     = ACTION_SPACE[a_idx]\n",
    "        x_u   = np.minimum(1, x + u)\n",
    "        x     = np.maximum(0, x_u - sales).astype(np.float32)\n",
    "\n",
    "    return (np.array(states_flat, np.float32),\n",
    "            np.array(states_pp, np.float32),\n",
    "            np.array(x_vecs, np.float32))\n",
    "\n",
    "# Generate data for each sensitivity scenario\n",
    "sens_data = {}\n",
    "for scn_name, scn_params in SENSITIVITY_SCENARIOS.items():\n",
    "    sens_data[scn_name] = generate_scenario_data(scn_params)\n",
    "    n = sens_data[scn_name][0].shape[0]\n",
    "    print(f\"  {scn_name}: {n} states | \"\n",
    "          f\"x_scale={scn_params['x_scale']:.0%}, \"\n",
    "          f\"sales_scale={scn_params['sales_scale']:.0%}, \"\n",
    "          f\"waste_rate={scn_params['waste_rate']:.0%}\")\n",
    "\n",
    "print(\"\\n‚úÖ Sensitivity scenario data generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42029250",
   "metadata": {},
   "source": [
    "### 6.2 Gi·∫£ l·∫≠p thay ƒë·ªïi tr·ªçng s·ªë RDX\n",
    "\n",
    "√ù t∆∞·ªüng: V·ªõi m·ªói component $w_k \\in \\{w_s, w_h, w_w, w_o\\}$:\n",
    "- T√≠nh RDX g·ªëc: $\\Delta Q^k$ cho t·∫•t c·∫£ 4 objectives\n",
    "- Nh√¢n $\\Delta Q^k$ c·ªßa component ƒëang x√©t v·ªõi $\\lambda$, gi·ªØ nguy√™n c√°c component kh√°c\n",
    "- Quan s√°t: Mean RDX thay ƒë·ªïi th·∫ø n√†o? MSX set thay ƒë·ªïi th·∫ø n√†o?\n",
    "\n",
    "Mapping k√Ω hi·ªáu:\n",
    "- $w_s$ ‚Üí `stockout` (z)\n",
    "- $w_h$ ‚Üí `overstock`\n",
    "- $w_w$ ‚Üí `waste` (q)\n",
    "- $w_o$ ‚Üí `quantile` (quan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a36b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PERTURB RDX WEIGHTS & COLLECT SENSITIVITY METRICS\n",
    "# ============================================================\n",
    "\n",
    "# Component weights notation: w_s=stockout, w_h=overstock, w_w=waste, w_o=quantile\n",
    "WEIGHT_LABELS = {\n",
    "    'stockout':  '$w_s$ (Stockout)',\n",
    "    'overstock': '$w_h$ (Overstock)',\n",
    "    'waste':     '$w_w$ (Waste)',\n",
    "    'quantile':  '$w_o$ (Quantile)',\n",
    "}\n",
    "\n",
    "def perturb_rdx_weights(delta_q_original, target_component, lam):\n",
    "    \"\"\"\n",
    "    Gi·∫£ l·∫≠p thay ƒë·ªïi tr·ªçng s·ªë c·ªßa M·ªòT component RDX.\n",
    "\n",
    "    Nh√¢n ŒîQ^k c·ªßa target_component v·ªõi Œª, gi·ªØ nguy√™n c√°c component kh√°c.\n",
    "    ‚Üí M√¥ ph·ªèng vi·ªác \"tƒÉng/gi·∫£m t·∫ßm quan tr·ªçng\" c·ªßa m·ªôt m·ª•c ti√™u.\n",
    "\n",
    "    Args:\n",
    "        delta_q_original: dict[str, np.ndarray[P]] ‚Äî RDX g·ªëc\n",
    "        target_component: str ‚Äî component c·∫ßn thay ƒë·ªïi ('stockout', 'overstock', ...)\n",
    "        lam: float ‚Äî h·ªá s·ªë scale\n",
    "\n",
    "    Returns:\n",
    "        delta_q_perturbed: dict[str, np.ndarray[P]] ‚Äî RDX ƒë√£ ƒëi·ªÅu ch·ªânh\n",
    "    \"\"\"\n",
    "    perturbed = {}\n",
    "    for obj in OBJECTIVES:\n",
    "        if obj == target_component:\n",
    "            perturbed[obj] = delta_q_original[obj] * lam   # Scale component n√†y\n",
    "        else:\n",
    "            perturbed[obj] = delta_q_original[obj].copy()   # Gi·ªØ nguy√™n\n",
    "    return perturbed\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Run sensitivity analysis ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "SENS_LAMBDAS = [0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "sensitivity_results = []\n",
    "\n",
    "for agent_name in AGENT_NAMES:\n",
    "    for scn_name in SENSITIVITY_SCENARIOS:\n",
    "        s_flat, s_pp, x_vs = sens_data[scn_name]\n",
    "        n = s_flat.shape[0]\n",
    "\n",
    "        # Compute baseline RDX (average over sampled states)\n",
    "        rdx_base = {obj: np.zeros(NUM_PRODUCTS) for obj in OBJECTIVES}\n",
    "        sample_idx = np.arange(0, n, max(1, n // 50))\n",
    "\n",
    "        for idx in sample_idx:\n",
    "            if agent_name == 'DQN':\n",
    "                dq, _ = rdx_dqn(agents['q_network'], s_flat[idx], x_vs[idx])\n",
    "            else:\n",
    "                dq, _ = rdx_a2c(agents['actor'], agents['critic'],\n",
    "                                 s_pp[idx], x_vs[idx])\n",
    "            for obj in OBJECTIVES:\n",
    "                rdx_base[obj] += dq[obj]\n",
    "        for obj in OBJECTIVES:\n",
    "            rdx_base[obj] /= len(sample_idx)\n",
    "\n",
    "        # Perturb each component independently\n",
    "        for target_comp in OBJECTIVES:\n",
    "            for lam in SENS_LAMBDAS:\n",
    "                dq_perturbed = perturb_rdx_weights(rdx_base, target_comp, lam)\n",
    "\n",
    "                # Mean absolute RDX for perturbed component\n",
    "                mean_rdx = float(np.mean(np.abs(dq_perturbed[target_comp])))\n",
    "\n",
    "                # MSX inclusion: how often does target_comp appear in MSX?\n",
    "                msx_sets, _ = compute_msx(dq_perturbed, lam=1.0)  # MSX threshold fixed at 1.0\n",
    "                inclusion_rate = sum(1 for ms in msx_sets if target_comp in ms) / NUM_PRODUCTS\n",
    "\n",
    "                sensitivity_results.append({\n",
    "                    'agent':     agent_name,\n",
    "                    'scenario':  scn_name,\n",
    "                    'component': target_comp,\n",
    "                    'w_label':   WEIGHT_LABELS[target_comp],\n",
    "                    'lambda':    lam,\n",
    "                    'mean_rdx':       mean_rdx,\n",
    "                    'msx_inclusion':  inclusion_rate,\n",
    "                })\n",
    "\n",
    "    print(f\"  Sensitivity done for {agent_name}\")\n",
    "\n",
    "df_sens = pd.DataFrame(sensitivity_results)\n",
    "df_sens.to_csv('sensitivity_results.csv', index=False)\n",
    "print(f\"\\n‚úÖ Sensitivity analysis: {len(df_sens)} rows saved to sensitivity_results.csv\")\n",
    "df_sens.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f4b2c",
   "metadata": {},
   "source": [
    "### 6.3 Visualization: Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd42fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT SA-1: Line Chart ‚Äî Mean |ŒîQ^k| vs Œª (per component)\n",
    "# ============================================================\n",
    "# Tr·ª•c X: Œª (h·ªá s·ªë scale c·ªßa component ƒëang x√©t)\n",
    "# Tr·ª•c Y: Mean |ŒîQ^k| (gi√° tr·ªã RDX trung b√¨nh sau khi nh√¢n Œª)\n",
    "# M·ªói subplot = 1 agent, 4 ƒë∆∞·ªùng cho 4 components\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=False)\n",
    "\n",
    "comp_colors = {\n",
    "    'stockout':  '#E53935',\n",
    "    'overstock': '#FF9800',\n",
    "    'waste':     '#7B1FA2',\n",
    "    'quantile':  '#1565C0',\n",
    "}\n",
    "\n",
    "for ax_idx, agent_name in enumerate(AGENT_NAMES):\n",
    "    ax = axes[ax_idx]\n",
    "\n",
    "    for comp in OBJECTIVES:\n",
    "        subset = df_sens[(df_sens['agent'] == agent_name) &\n",
    "                         (df_sens['component'] == comp)]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        # Average across scenarios for cleaner visualization\n",
    "        grouped = subset.groupby('lambda')['mean_rdx'].mean()\n",
    "\n",
    "        ax.plot(grouped.index, grouped.values,\n",
    "                marker='o', linewidth=2.5, markersize=8,\n",
    "                color=comp_colors[comp],\n",
    "                label=WEIGHT_LABELS[comp])\n",
    "\n",
    "    ax.set_xlabel('Œª (Scale Factor)', fontsize=12)\n",
    "    ax.set_ylabel('Mean |ŒîQ^k|', fontsize=12)\n",
    "    ax.set_title(f'{agent_name}: Mean RDX vs Œª', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=10, loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xticks(SENS_LAMBDAS)\n",
    "\n",
    "plt.suptitle('Sensitivity Analysis: How RDX Magnitude Responds to Weight Scaling',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('sensitivity_mean_rdx_vs_lambda.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: sensitivity_mean_rdx_vs_lambda.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f93e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT SA-2: 100% Stacked Bar ‚Äî MSX Inclusion Frequency\n",
    "# ============================================================\n",
    "# Khi Œª c·ªßa component k thay ƒë·ªïi, t·∫ßn su·∫•t k l·ªçt v√†o MSX thay ƒë·ªïi th·∫ø n√†o?\n",
    "# M·ªói thanh bar = 1 gi√° tr·ªã Œª, chia th√†nh 4 ph·∫ßn (4 components)\n",
    "# Chi·ªÅu cao m·ªói ph·∫ßn = inclusion rate c·ªßa component ƒë√≥ trong MSX\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "bar_colors = ['#E53935', '#FF9800', '#7B1FA2', '#1565C0']\n",
    "\n",
    "for ax_idx, agent_name in enumerate(AGENT_NAMES):\n",
    "    ax = axes[ax_idx]\n",
    "\n",
    "    # For each lambda, get inclusion rates when THAT component is the perturbed one\n",
    "    # We want to show: when we scale w_k by Œª, how often does k appear in MSX?\n",
    "    inclusion_matrix = np.zeros((len(OBJECTIVES), len(SENS_LAMBDAS)))\n",
    "\n",
    "    for k, comp in enumerate(OBJECTIVES):\n",
    "        for j, lam in enumerate(SENS_LAMBDAS):\n",
    "            subset = df_sens[(df_sens['agent'] == agent_name) &\n",
    "                             (df_sens['component'] == comp) &\n",
    "                             (df_sens['lambda'] == lam)]\n",
    "            if not subset.empty:\n",
    "                inclusion_matrix[k, j] = subset['msx_inclusion'].mean()\n",
    "\n",
    "    # Normalize columns to 100% for stacked bar\n",
    "    col_sums = inclusion_matrix.sum(axis=0, keepdims=True)\n",
    "    col_sums = np.where(col_sums == 0, 1, col_sums)  # avoid div by 0\n",
    "    inc_pct = inclusion_matrix / col_sums  # [4, len(Œª)]\n",
    "\n",
    "    x_pos = np.arange(len(SENS_LAMBDAS))\n",
    "    bar_width = 0.6\n",
    "    bottom = np.zeros(len(SENS_LAMBDAS))\n",
    "\n",
    "    for k, comp in enumerate(OBJECTIVES):\n",
    "        ax.bar(x_pos, inc_pct[k], bar_width, bottom=bottom,\n",
    "               color=bar_colors[k], label=WEIGHT_LABELS[comp],\n",
    "               alpha=0.85, edgecolor='white', linewidth=0.5)\n",
    "        # Annotate percentage\n",
    "        for j in range(len(SENS_LAMBDAS)):\n",
    "            if inc_pct[k, j] > 0.05:  # only label if visible\n",
    "                ax.text(x_pos[j], bottom[j] + inc_pct[k, j] / 2,\n",
    "                        f'{inc_pct[k, j]:.0%}',\n",
    "                        ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "        bottom += inc_pct[k]\n",
    "\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels([f'Œª={v}' for v in SENS_LAMBDAS])\n",
    "    ax.set_xlabel('Œª (Scale Factor of Perturbed Component)', fontsize=11)\n",
    "    ax.set_ylabel('MSX Inclusion Proportion', fontsize=11)\n",
    "    ax.set_title(f'{agent_name}', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.legend(fontsize=9, loc='upper right', ncol=2)\n",
    "\n",
    "plt.suptitle('MSX Inclusion Frequency: How Component Importance Changes with Weight Scaling',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('sensitivity_msx_inclusion_stacked.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: sensitivity_msx_inclusion_stacked.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SENSITIVITY SUMMARY TABLE\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SENSITIVITY ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pivot: Mean RDX by (agent, component) vs lambda\n",
    "pivot_rdx = df_sens.pivot_table(\n",
    "    values='mean_rdx',\n",
    "    index=['agent', 'component'],\n",
    "    columns='lambda',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "print(\"\\nüìä Mean |ŒîQ^k| across Œª values:\")\n",
    "print(pivot_rdx.round(4).to_string())\n",
    "\n",
    "# Pivot: MSX inclusion by (agent, component) vs lambda\n",
    "pivot_inc = df_sens.pivot_table(\n",
    "    values='msx_inclusion',\n",
    "    index=['agent', 'component'],\n",
    "    columns='lambda',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "print(\"\\nüìä MSX Inclusion Rate across Œª values:\")\n",
    "print(pivot_inc.round(3).to_string())\n",
    "\n",
    "# Key insight: which component is most sensitive?\n",
    "for agent_name in AGENT_NAMES:\n",
    "    agent_data = df_sens[df_sens['agent'] == agent_name]\n",
    "    rdx_range = agent_data.groupby('component')['mean_rdx'].agg(['min', 'max'])\n",
    "    rdx_range['sensitivity'] = rdx_range['max'] - rdx_range['min']\n",
    "    most_sensitive = rdx_range['sensitivity'].idxmax()\n",
    "    print(f\"\\nüîç {agent_name}: Most sensitive component = {most_sensitive} \"\n",
    "          f\"(range = {rdx_range.loc[most_sensitive, 'sensitivity']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e07d651",
   "metadata": {},
   "source": [
    "## Summary & Interpretation\n",
    "\n",
    "### How to Read Results\n",
    "\n",
    "**OCS (Objective Coverage Score)**:\n",
    "- High OCS ‚Üí agent's decisions are driven by multiple reward objectives\n",
    "- Low OCS ‚Üí agent focuses narrowly on 1-2 objectives\n",
    "\n",
    "**FCS (Feature Coverage Score)**:\n",
    "- High FCS ‚Üí model uses all input features (inventory, sales, waste) in decisions\n",
    "- Low FCS ‚Üí model relies on subset of features\n",
    "\n",
    "**CAS (Cross-domain Alignment)**:\n",
    "- High CAS ‚Üí SHAP feature importances AGREE with RDX objective importances\n",
    "- Low CAS ‚Üí disconnect between what features the model uses and what objectives it optimizes\n",
    "\n",
    "**Stability**:\n",
    "- High Stability ‚Üí MSX explanation is robust across threshold changes\n",
    "- Low Stability ‚Üí explanations are fragile, sensitive to hyperparameters\n",
    "\n",
    "### Sensitivity Analysis Interpretation\n",
    "- **Line chart (SA-1)**: N·∫øu ƒë∆∞·ªùng d·ªëc ‚Üí component ƒë√≥ c√≥ ·∫£nh h∆∞·ªüng m·∫°nh l√™n gi·∫£i th√≠ch\n",
    "- **Stacked bar (SA-2)**: N·∫øu t·ª∑ l·ªá ph·∫ßn trƒÉm thay ƒë·ªïi nhi·ªÅu ‚Üí MSX nh·∫°y c·∫£m v·ªõi tr·ªçng s·ªë\n",
    "\n",
    "**For Reviewers**: The CAS metric is the key contribution ‚Äî it bridges\n",
    "the gap between feature-level (SHAP) and objective-level (RDX) explanations,\n",
    "validating that the model's internal feature usage aligns with the reward\n",
    "structure of the environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
