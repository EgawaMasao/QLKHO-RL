# ğŸš¨ Cháº©n Ä‘oÃ¡n: Agent há»c SAI - Chi phÃ­ cao hÆ¡n 1800 láº§n!

## ğŸ“Š Triá»‡u chá»©ng

```
âœ… Q-table Ä‘Ã£ train:     81/82 giÃ¡ trá»‹ khÃ¡c 0
âŒ Q-values Cá»°C Ã‚M:      -83,565 â†’ 0
âŒ Performance tá»‡ háº¡i:   55,081 â‚¬/ngÃ y (vs 30 â‚¬/ngÃ y)
âŒ Tá»‡ hÆ¡n baseline:      179,907%
```

---

## ğŸ” NguyÃªn nhÃ¢n chÃ­nh: **REWARD SCALE QUÃ Lá»šN**

### Váº¥n Ä‘á»

Trong code hiá»‡n táº¡i:

```python
# Cell 7: Training
total_cost = holding_cost + backorder_cost + ordering_cost
reward = -total_cost  # âŒ Reward quÃ¡ lá»›n (Ã¢m)

# VÃ­ dá»¥:
# holding_cost = 0.0274 * 5 = 0.137 â‚¬
# backorder_cost = 20 * 5 = 100 â‚¬  
# ordering_cost = 50 â‚¬
# total_cost = 150.137 â‚¬
# reward = -150.137  # âŒ Q-values sáº½ tÃ­ch lÅ©y thÃ nh sá»‘ ráº¥t Ã¢m
```

### Táº¡i sao Q-values Ã¢m cá»±c lá»›n?

```
Episode 1, day 1:   Q[s,a] = 0 + 0.5 * (-150 + 0.7 * 0 - 0) = -75
Episode 1, day 2:   Q[s,a] = -75 + 0.5 * (-150 + 0.7 * (-75) - (-75)) = -113.75
...
Episode 1000:       Q[s,a] â‰ˆ -83,565  # âŒ TÃ­ch lÅ©y qua 1,000,000 steps
```

**Alpha = 0.5 quÃ¡ cao** â†’ cáº­p nháº­t quÃ¡ nhanh â†’ Q-values phÃ¡t tÃ¡n

---

## âœ… Giáº£i phÃ¡p 1: NORMALIZE REWARD (Khuyáº¿n nghá»‹ â­)

### Sá»­a Cell 7 - Training Loop

```python
# Thay vÃ¬:
reward = -total_cost

# DÃ¹ng:
reward = -total_cost / 100.0  # âœ… Normalize vá» [-1.5, 0]
```

**LÃ½ do:**
- Cost thÆ°á»ng 0-150 â‚¬ â†’ Reward -1.5 â†’ 0
- Q-values sáº½ á»•n Ä‘á»‹nh hÆ¡n
- Alpha = 0.5 váº«n hoáº¡t Ä‘á»™ng tá»‘t

---

## âœ… Giáº£i phÃ¡p 2: GIáº¢M ALPHA (Dá»… nháº¥t)

### Sá»­a Cell 3 - Parameters

```python
# Thay vÃ¬:
alpha = 0.5  # âŒ QuÃ¡ cao

# DÃ¹ng:
alpha = 0.1  # âœ… á»”n Ä‘á»‹nh hÆ¡n
```

**LÃ½ do:**
- Alpha nhá» â†’ cáº­p nháº­t cháº­m hÆ¡n â†’ á»•n Ä‘á»‹nh hÆ¡n
- PhÃ¹ há»£p vá»›i reward lá»›n
- Trade-off: há»c cháº­m hÆ¡n (cáº§n nhiá»u episodes)

---

## âœ… Giáº£i phÃ¡p 3: GIáº¢M GAMMA

### Sá»­a Cell 3 - Parameters

```python
# Thay vÃ¬:
gamma = 0.7  # TÃ­nh toÃ¡n quÃ¡ xa tÆ°Æ¡ng lai

# DÃ¹ng:
gamma = 0.5  # âœ… Táº­p trung vÃ o reward gáº§n
```

**LÃ½ do:**
- Inventory management = short-term problem
- Gamma nhá» â†’ Ã­t bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi tÆ°Æ¡ng lai xa
- Giáº£m variance cá»§a Q-values

---

## ğŸ¯ Giáº£i phÃ¡p KHUYáº¾N NGHá»Š (Káº¿t há»£p)

### Step 1: Sá»­a Cell 3

```python
# Khá»Ÿi táº¡o tham sá»‘ cho mÃ´ hÃ¬nh
alpha = 0.2  # âœ… Giáº£m tá»« 0.5 â†’ 0.2
gamma = 0.5  # âœ… Giáº£m tá»« 0.7 â†’ 0.5
epsilon_start = 0.3
epsilon_end = 0.01
epsilon_decay = 0.995
```

### Step 2: Sá»­a Cell 7 - Normalize Reward

TÃ¬m dÃ²ng:
```python
reward = -total_cost
```

Sá»­a thÃ nh:
```python
# Normalize reward vá» scale [-2, 0]
reward = -total_cost / 100.0  # âœ… Scale down 100 láº§n
```

### Step 3: Train láº¡i

```
1. USE_PRETRAINED = False
2. Cháº¡y Cell 1-9
3. Kiá»ƒm tra Q-values: nÃªn tá»« -50 â†’ 0
4. ÄÃ¡nh giÃ¡: nÃªn tá»‘t hÆ¡n baseline
```

---

## ğŸ“Š Expected Results sau khi sá»­a

```
Q-table min:           -50 â†’ -10  âœ… Há»£p lÃ½
Q-table max:           0  âœ…
Learned cost:          25-28 â‚¬/ngÃ y  âœ…
Traditional (r,q):     30 â‚¬/ngÃ y
Improvement:           +5-15%  âœ…
```

---

## ğŸ”¬ Giáº£i thÃ­ch chi tiáº¿t

### Táº¡i sao alpha = 0.5 quÃ¡ cao?

**Q-learning update formula:**
```
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]
```

Náº¿u `Î± = 0.5`:
- Má»—i update thay Ä‘á»•i Q-value **50%**
- Vá»›i reward lá»›n (-150), má»—i step thay Ä‘á»•i Â±75
- Sau 1000 episodes Ã— 1000 days = 1,000,000 updates â†’ phÃ¡t tÃ¡n

Náº¿u `Î± = 0.1`:
- Má»—i update chá»‰ thay Ä‘á»•i **10%**
- á»”n Ä‘á»‹nh hÆ¡n nhiá»u
- Cáº§n nhiá»u episodes hÆ¡n Ä‘á»ƒ há»™i tá»¥

---

### Táº¡i sao cáº§n normalize reward?

**KhÃ´ng normalize:**
```
Day 1: cost = 150 â‚¬ â†’ reward = -150
Day 2: cost = 80 â‚¬ â†’ reward = -80
Q-values: [-150, -80, -120, ...]  # âŒ QuÃ¡ lá»›n
```

**CÃ³ normalize (Ã·100):**
```
Day 1: cost = 150 â‚¬ â†’ reward = -1.5
Day 2: cost = 80 â‚¬ â†’ reward = -0.8
Q-values: [-1.5, -0.8, -1.2, ...]  # âœ… Há»£p lÃ½
```

---

## ğŸ“ Quy táº¯c chung cho Reward Design

### 1. Scale reward vá» [-1, 1]

```python
reward_min = -200  # Worst case cost
reward_max = 0     # Best case cost
reward = (reward - reward_min) / (reward_max - reward_min) * 2 - 1
```

### 2. Hoáº·c Ä‘Æ¡n giáº£n: Chia cho constant

```python
reward = -cost / 100.0  # ÄÆ¡n giáº£n nháº¥t
```

### 3. Äiá»u chá»‰nh alpha phÃ¹ há»£p vá»›i reward scale

| Reward scale | Alpha khuyáº¿n nghá»‹ |
|--------------|-------------------|
| [-1, 1] | 0.3 - 0.5 |
| [-10, 10] | 0.1 - 0.3 |
| [-100, 100] | 0.05 - 0.1 |
| [-1000+] | âŒ Scale láº¡i! |

---

## ğŸ”§ Debugging Tips

### Check 1: Q-values range

```python
print(f"Q min: {Q.min():.2f}")  # NÃªn > -100
print(f"Q max: {Q.max():.2f}")  # NÃªn â‰ˆ 0
print(f"Q mean: {Q.mean():.2f}") # NÃªn -50 â†’ 0
```

### Check 2: Reward range per episode

```python
print(f"Avg reward per day: {np.mean([...rewards...])}")
# NÃªn -2 â†’ 0 (náº¿u normalize)
```

### Check 3: Q-values convergence

```python
# Váº½ Q-values evolution
plt.plot([Q.mean() for Q in Q_history])
# NÃªn há»™i tá»¥ (flatten) sau 500-800 episodes
```

---

## ğŸ“š Tham kháº£o

1. **Sutton & Barto (2018)** - Chapter 6.5: Q-learning
2. **Reward Shaping** - Ng et al. (1999)
3. **DeepMind** - DQN paper vá» reward clipping

---

## âœ… Action Items

- [ ] 1. Sá»­a `alpha = 0.2` trong Cell 3
- [ ] 2. Sá»­a `gamma = 0.5` trong Cell 3
- [ ] 3. ThÃªm `reward = -total_cost / 100.0` trong Cell 7
- [ ] 4. Train láº¡i tá»« Cell 7
- [ ] 5. Kiá»ƒm tra Q-values trong Cell 16
- [ ] 6. ÄÃ¡nh giÃ¡ láº¡i performance
- [ ] 7. Expected: Improvement 5-15%

---

## ğŸ¯ Káº¿t luáº­n

**Root cause:** Reward scale quÃ¡ lá»›n + Alpha quÃ¡ cao

**Solution:** 
1. Normalize reward (Ã·100)
2. Giáº£m alpha (0.5 â†’ 0.2)
3. Giáº£m gamma (0.7 â†’ 0.5)

**Expected outcome:** Agent há»c Ä‘Ãºng, improve 5-15% vs baseline

---

**TÃ¡c giáº£:** GitHub Copilot
**NgÃ y:** 2025-10-15
