{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37419706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "‚úÖ IMPORTS SUCCESSFUL\n",
      "======================================================================\n",
      "   TensorFlow version: 2.14.0\n",
      "   NumPy version: 1.24.3\n",
      "   Random seed: 42\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED_VAL = 42\n",
    "random.seed(SEED_VAL)\n",
    "np.random.seed(SEED_VAL)\n",
    "tf.random.set_seed(SEED_VAL)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ IMPORTS SUCCESSFUL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   TensorFlow version: {tf.__version__}\")\n",
    "print(f\"   NumPy version: {np.__version__}\")\n",
    "print(f\"   Random seed: {SEED_VAL}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c38bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "‚úÖ REPLAY BUFFER CREATED\n",
      "======================================================================\n",
      "   Capacity: 10,000 experiences\n",
      "   Function: Store and sample transitions for training\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Experience Replay Buffer for DQN\"\"\"\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add experience to buffer\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample random batch from buffer\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int32),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ REPLAY BUFFER CREATED\")\n",
    "print(\"=\"*70)\n",
    "print(\"   Capacity: 10,000 experiences\")\n",
    "print(\"   Function: Store and sample transitions for training\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34353c41",
   "metadata": {},
   "source": [
    "## 3. REPLAY BUFFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1094b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "‚úÖ DQNAgentRDX MODEL DEFINED\n",
      "======================================================================\n",
      "   Architecture: [3‚Üí32‚Üí32‚Üí32‚Üí14]\n",
      "   Input: State (3 features)\n",
      "   Hidden layers: 32‚Üí32‚Üí32\n",
      "   Output: Q-values (14 actions)\n",
      "   RDX features: 32-dimensional t·ª´ dense3 layer\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "class DQNAgentRDX(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    DQN Agent v·ªõi RDX feature extraction\n",
    "    Architecture: [3‚Üí32‚Üí32‚Üí32‚Üí14]\n",
    "    Compatible v·ªõi A2CAgentRDX ƒë·ªÉ c√≥ th·ªÉ so s√°nh RDX features\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=32, num_actions=14, num_features=3):\n",
    "        super(DQNAgentRDX, self).__init__()\n",
    "        # Shared layers for feature extraction\n",
    "        self.dense1 = tf.keras.layers.Dense(hidden_size, activation='relu', name='dense1')\n",
    "        self.dense2 = tf.keras.layers.Dense(hidden_size, activation='relu', name='dense2')\n",
    "        self.dense3 = tf.keras.layers.Dense(hidden_size, activation='relu', name='dense3')  # RDX features\n",
    "        \n",
    "        # Q-values output\n",
    "        self.q_values = tf.keras.layers.Dense(num_actions, name='q_values')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        rdx_features = self.dense3(x)  # 32-dim RDX representation\n",
    "        q_vals = self.q_values(rdx_features)\n",
    "        return q_vals, rdx_features\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ DQNAgentRDX MODEL DEFINED\")\n",
    "print(\"=\"*70)\n",
    "print(\"   Architecture: [3‚Üí32‚Üí32‚Üí32‚Üí14]\")\n",
    "print(\"   Input: State (3 features)\")\n",
    "print(\"   Hidden layers: 32‚Üí32‚Üí32\")\n",
    "print(\"   Output: Q-values (14 actions)\")\n",
    "print(\"   RDX features: 32-dimensional t·ª´ dense3 layer\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20954fa",
   "metadata": {},
   "source": [
    "## 2. DQN MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c03313bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "‚úÖ IMPORTS SUCCESSFUL\n",
      "======================================================================\n",
      "   TensorFlow version: 2.14.0\n",
      "   NumPy version: 1.24.3\n",
      "   Random seed: 42\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED_VAL = 42\n",
    "random.seed(SEED_VAL)\n",
    "np.random.seed(SEED_VAL)\n",
    "tf.random.set_seed(SEED_VAL)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ IMPORTS SUCCESSFUL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   TensorFlow version: {tf.__version__}\")\n",
    "print(f\"   NumPy version: {np.__version__}\")\n",
    "print(f\"   Random seed: {SEED_VAL}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81cc6a1",
   "metadata": {},
   "source": [
    "## 1. IMPORTS & SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171967d6",
   "metadata": {},
   "source": [
    "# ü§ñ TRAINING DQN FOR INVENTORY MANAGEMENT\n",
    "\n",
    "## Objective:\n",
    "Train DQN agent v·ªõi environment gi·ªëng A2C/A2C_mod t·ª´ training.py\n",
    "\n",
    "## Configuration:\n",
    "- **Episodes**: 600\n",
    "- **Steps per episode**: 900\n",
    "- **Total steps**: 540,000\n",
    "- **Architecture**: [3‚Üí32‚Üí32‚Üí32‚Üí14] (same as A2C)\n",
    "- **State**: [inventory, sales_forecast, waste_rate]\n",
    "- **Actions**: 14 discrete levels\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a65a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DQNTrainer created\n",
      "   Features: Target Network, Experience Replay, Epsilon-Greedy\n",
      "   Ready to train!\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# 3. DQN TRAINING AGENT\n",
    "# =================================================================\n",
    "\n",
    "class DQNTrainer:\n",
    "    \"\"\"DQN Training with Target Network and Experience Replay\"\"\"\n",
    "    def __init__(self, env, hidden_size=32, lr=0.001, gamma=0.99, \n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Q-network v√† Target network\n",
    "        self.q_network = DQNAgentRDX(hidden_size=hidden_size, num_actions=env.n_actions)\n",
    "        self.target_network = DQNAgentRDX(hidden_size=hidden_size, num_actions=env.n_actions)\n",
    "        \n",
    "        # Initialize networks\n",
    "        dummy_state = tf.constant([[0.5, 0.2, 0.01]], dtype=tf.float32)\n",
    "        self.q_network(dummy_state)\n",
    "        self.target_network(dummy_state)\n",
    "        \n",
    "        # Copy weights\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(capacity=10000)\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from Q-network to Target network\"\"\"\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(0, self.env.n_actions)\n",
    "        else:\n",
    "            state_tensor = tf.constant([state], dtype=tf.float32)\n",
    "            q_values, _ = self.q_network(state_tensor)\n",
    "            return tf.argmax(q_values[0]).numpy()\n",
    "    \n",
    "    def train_step(self, batch_size=64):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states_t = tf.constant(states, dtype=tf.float32)\n",
    "        actions_t = tf.constant(actions, dtype=tf.int32)\n",
    "        rewards_t = tf.constant(rewards, dtype=tf.float32)\n",
    "        next_states_t = tf.constant(next_states, dtype=tf.float32)\n",
    "        dones_t = tf.constant(dones, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Current Q-values\n",
    "            q_values, _ = self.q_network(states_t)\n",
    "            action_masks = tf.one_hot(actions_t, self.env.n_actions)\n",
    "            q_values_selected = tf.reduce_sum(q_values * action_masks, axis=1)\n",
    "            \n",
    "            # Target Q-values (Double DQN)\n",
    "            next_q_values, _ = self.target_network(next_states_t)\n",
    "            next_q_max = tf.reduce_max(next_q_values, axis=1)\n",
    "            \n",
    "            # TD target\n",
    "            targets = rewards_t + self.gamma * next_q_max * (1 - dones_t)\n",
    "            \n",
    "            # Loss\n",
    "            loss = tf.reduce_mean(tf.square(targets - q_values_selected))\n",
    "        \n",
    "        # Backpropagation\n",
    "        gradients = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.q_network.trainable_variables))\n",
    "        \n",
    "        return loss.numpy()\n",
    "    \n",
    "    def train(self, num_episodes=600, batch_size=64, update_target_freq=10, \n",
    "              verbose=True, save_freq=20, save_path=None):\n",
    "        \"\"\"Train DQN agent\"\"\"\n",
    "        episode_rewards = []\n",
    "        losses = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_loss = []\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                # Select action\n",
    "                action = self.select_action(state, training=True)\n",
    "                \n",
    "                # Execute action\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                \n",
    "                # Store experience\n",
    "                self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                \n",
    "                # Train\n",
    "                if len(self.replay_buffer) >= batch_size:\n",
    "                    loss = self.train_step(batch_size)\n",
    "                    episode_loss.append(loss)\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "            \n",
    "            # Update epsilon\n",
    "            self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "            \n",
    "            # Update target network\n",
    "            if (episode + 1) % update_target_freq == 0:\n",
    "                self.update_target_network()\n",
    "            \n",
    "            # Store metrics\n",
    "            episode_rewards.append(episode_reward)\n",
    "            avg_loss = np.mean(episode_loss) if episode_loss else 0\n",
    "            losses.append(avg_loss)\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if save_path and (episode + 1) % save_freq == 0:\n",
    "                checkpoint_dir = save_path\n",
    "                os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                \n",
    "                checkpoint = tf.train.Checkpoint(\n",
    "                    q_network=self.q_network,\n",
    "                    optimizer=self.optimizer\n",
    "                )\n",
    "                checkpoint.save(os.path.join(checkpoint_dir, f'ckpt'))\n",
    "            \n",
    "            # Verbose\n",
    "            if verbose and (episode + 1) % 10 == 0:\n",
    "                avg_reward = np.mean(episode_rewards[-10:])\n",
    "                print(f\"Episode {episode+1}/{num_episodes} | \"\n",
    "                      f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                      f\"Epsilon: {self.epsilon:.3f} | \"\n",
    "                      f\"Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        return episode_rewards, losses\n",
    "\n",
    "print(\"‚úÖ DQNTrainer created\")\n",
    "print(f\"   Features: Target Network, Experience Replay, Epsilon-Greedy\")\n",
    "print(f\"   Ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7cadc6",
   "metadata": {},
   "source": [
    "## 4. DQN TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2522e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ A2CStyleInventoryEnv created\n",
      "   Based on training.py structure\n",
      "   Num products: 100\n",
      "   Timesteps per episode: 900\n",
      "   Action space: 14 levels (same as A2C)\n",
      "   State: [inventory, sales_forecast, waste_rate]\n",
      "   Reward: revenue - costs (stockout, holding, waste, order)\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# ENVIRONMENT GI·ªêNG TRAINING.PY (A2C/A2C_MOD)\n",
    "# =================================================================\n",
    "import numpy as np\n",
    "class A2CStyleInventoryEnv:\n",
    "    \"\"\"\n",
    "    Environment t∆∞∆°ng t·ª± training.py c·ªßa A2C/A2C_mod\n",
    "    - State: [inventory_level, sales_forecast, waste_rate]\n",
    "    - Dynamics: gi·ªëng training.py\n",
    "    - Rewards: gi·ªëng training.py\n",
    "    \"\"\"\n",
    "    def __init__(self, num_products=220, num_timesteps=900, waste_rate=0.025):\n",
    "        self.num_products = num_products\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.waste_rate = waste_rate\n",
    "        \n",
    "        # Action space: 14 discrete levels (gi·ªëng training.py)\n",
    "        self.action_space = np.array([0, 0.005, 0.01, 0.0125, 0.015, 0.0175, \n",
    "                                      0.02, 0.03, 0.04, 0.08, 0.12, 0.2, 0.5, 1.0])\n",
    "        self.n_actions = len(self.action_space)\n",
    "        \n",
    "        # Generate synthetic sales data (normalized [0, 1])\n",
    "        # Gi·ªëng nh∆∞ trong training.py, sales ƒë∆∞·ª£c normalize b·ªüi capacity\n",
    "        self._generate_sales_data()\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def _generate_sales_data(self):\n",
    "        \"\"\"Generate synthetic sales patterns\"\"\"\n",
    "        # T·∫°o sales patterns v·ªõi seasonality v√† trend\n",
    "        t = np.arange(self.num_timesteps)\n",
    "        \n",
    "        # Base demand with seasonality (weekly pattern)\n",
    "        base = 0.3 + 0.15 * np.sin(2 * np.pi * t / 7)  # Weekly cycle\n",
    "        \n",
    "        # Add monthly trend\n",
    "        trend = 0.1 * np.sin(2 * np.pi * t / 30)  # Monthly cycle\n",
    "        \n",
    "        # Random noise\n",
    "        noise = np.random.uniform(-0.05, 0.05, self.num_timesteps)\n",
    "        \n",
    "        # Combine\n",
    "        self.sales_pattern = np.clip(base + trend + noise, 0.1, 0.8)\n",
    "        \n",
    "        # Initialize for all products (v·ªõi variation)\n",
    "        self.sales_data = np.zeros((self.num_timesteps, self.num_products))\n",
    "        for i in range(self.num_products):\n",
    "            product_factor = np.random.uniform(0.8, 1.2)\n",
    "            self.sales_data[:, i] = self.sales_pattern * product_factor\n",
    "        \n",
    "        self.sales_data = np.clip(self.sales_data, 0.0, 1.0)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment - gi·ªëng training.py\"\"\"\n",
    "        # Random initial inventory: 0 <= x <= 1 (eq 2 in training.py)\n",
    "        self.x = np.random.uniform(0, 1, self.num_products).astype(np.float32)\n",
    "        \n",
    "        # Waste estimate\n",
    "        self.q = self.waste_rate * self.x\n",
    "        \n",
    "        self.t = 0\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        # Get current state\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        State construction gi·ªëng training.py:\n",
    "        s = [x, sales_forecast, q]\n",
    "        \"\"\"\n",
    "        # Current inventory\n",
    "        x_norm = self.x  # Already normalized [0, 1]\n",
    "        \n",
    "        # Sales forecast (current timestep)\n",
    "        sales_forecast = self.sales_data[self.t % self.num_timesteps]\n",
    "        \n",
    "        # Waste estimate\n",
    "        q = self.q\n",
    "        \n",
    "        # Stack: (num_products, 3) -> average to (3,) for single state\n",
    "        # Trong training.py, state l√† per-product, nh∆∞ng ƒë·ªÉ ƒë∆°n gi·∫£n h√≥a v·ªõi DQN,\n",
    "        # ta average across products\n",
    "        state = np.array([\n",
    "            np.mean(x_norm),\n",
    "            np.mean(sales_forecast),\n",
    "            np.mean(q)\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action_idx):\n",
    "        \"\"\"\n",
    "        Execute action - dynamics gi·ªëng training.py\n",
    "        \"\"\"\n",
    "        # Convert action index to actual order level\n",
    "        u = self.action_space[action_idx]\n",
    "        \n",
    "        # Apply action to all products (simplified - same action for all)\n",
    "        u_array = np.full(self.num_products, u, dtype=np.float32)\n",
    "        \n",
    "        # Get current sales\n",
    "        sales = self.sales_data[self.t % self.num_timesteps]\n",
    "        \n",
    "        # Dynamics (gi·ªëng training.py):\n",
    "        # 1. Add order to inventory\n",
    "        x_u = np.minimum(1.0, self.x + u_array)\n",
    "        \n",
    "        # 2. Calculate overstock\n",
    "        overstock = np.maximum(0, (self.x + u_array) - 1.0)\n",
    "        \n",
    "        # 3. Meet demand (sales)\n",
    "        x_prime = np.maximum(0, x_u - sales)\n",
    "        \n",
    "        # 4. Calculate stockout\n",
    "        stockout = np.maximum(0, sales - x_u)\n",
    "        \n",
    "        # 5. Update waste for next step\n",
    "        self.q = self.waste_rate * x_prime\n",
    "        \n",
    "        # Reward calculation (inspired by training.py):\n",
    "        # In training.py, reward includes:\n",
    "        # - Negative for stockout (lost sales)\n",
    "        # - Negative for overstock (wasted inventory)\n",
    "        # - Negative for holding cost\n",
    "        \n",
    "        # Stockout penalty (lost revenue)\n",
    "        stockout_cost = -10.0 * np.sum(stockout)\n",
    "        \n",
    "        # Overstock penalty\n",
    "        overstock_cost = -5.0 * np.sum(overstock)\n",
    "        \n",
    "        # Holding cost\n",
    "        holding_cost = -0.5 * np.sum(x_prime)\n",
    "        \n",
    "        # Order cost\n",
    "        order_cost = -2.0 if u > 0 else 0\n",
    "        \n",
    "        # Waste cost\n",
    "        waste_cost = -5.0 * np.sum(self.q)\n",
    "        \n",
    "        # Revenue from sales\n",
    "        actual_sales = sales - stockout\n",
    "        revenue = 15.0 * np.sum(actual_sales)\n",
    "        \n",
    "        # Total reward\n",
    "        reward = revenue + stockout_cost + overstock_cost + holding_cost + order_cost + waste_cost\n",
    "        \n",
    "        # Update state\n",
    "        self.x = x_prime\n",
    "        self.t += 1\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        # Check done\n",
    "        done = (self.t >= self.num_timesteps)\n",
    "        \n",
    "        # Info\n",
    "        info = {\n",
    "            'inventory': np.mean(self.x),\n",
    "            'sales': np.mean(sales),\n",
    "            'stockout': np.sum(stockout),\n",
    "            'overstock': np.sum(overstock),\n",
    "            'waste': np.sum(self.q),\n",
    "            'reward': reward\n",
    "        }\n",
    "        \n",
    "        return self._get_state(), reward, done, info\n",
    "\n",
    "print(\"‚úÖ A2CStyleInventoryEnv created\")\n",
    "print(f\"   Based on training.py structure\")\n",
    "print(f\"   Num products: 100\")\n",
    "print(f\"   Timesteps per episode: 900\")\n",
    "print(f\"   Action space: 14 levels (same as A2C)\")\n",
    "print(f\"   State: [inventory, sales_forecast, waste_rate]\")\n",
    "print(f\"   Reward: revenue - costs (stockout, holding, waste, order)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6800ad01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ TRAINING DQN V·ªöI A2C-STYLE ENVIRONMENT\n",
      "======================================================================\n",
      "\n",
      "üìã Training Configuration:\n",
      "   Environment: A2CStyleInventoryEnv (based on training.py)\n",
      "   Episodes: 600\n",
      "   Steps per episode: 900\n",
      "   Total steps: 540,000\n",
      "   Num products: 100\n",
      "   State: [avg_inventory, avg_sales_forecast, avg_waste]\n",
      "   Actions: 14 discrete levels (same as A2C)\n",
      "   Hidden size: 32\n",
      "   Learning rate: 0.001\n",
      "   Gamma: 0.99\n",
      "   Batch size: 64\n",
      "   Epsilon decay: 0.998 (slower for more exploration)\n",
      "\n",
      "‚ö†Ô∏è  L∆∞u √Ω: Training 600 episodes c√≥ th·ªÉ m·∫•t 10-15 ph√∫t\n",
      "======================================================================\n",
      "‚è≥ Starting training...\n",
      "Episode 10/600 | Avg Reward: 30201.70 | Epsilon: 0.980 | Loss: 635.4204\n",
      "Episode 340/600 | Avg Reward: 290770.81 | Epsilon: 0.506 | Loss: 231028.1250\n",
      "Episode 350/600 | Avg Reward: 286830.74 | Epsilon: 0.496 | Loss: 237516.0156\n",
      "Episode 360/600 | Avg Reward: 291939.20 | Epsilon: 0.486 | Loss: 247701.9062\n",
      "Episode 370/600 | Avg Reward: 291239.03 | Epsilon: 0.477 | Loss: 267156.3438\n",
      "Episode 380/600 | Avg Reward: 286167.89 | Epsilon: 0.467 | Loss: 321059.9688\n",
      "Episode 390/600 | Avg Reward: 297500.15 | Epsilon: 0.458 | Loss: 268339.8750\n",
      "Episode 400/600 | Avg Reward: 293874.86 | Epsilon: 0.449 | Loss: 308718.3750\n",
      "Episode 410/600 | Avg Reward: 292468.14 | Epsilon: 0.440 | Loss: 246713.3125\n",
      "Episode 420/600 | Avg Reward: 292385.65 | Epsilon: 0.431 | Loss: 288942.3438\n",
      "Episode 430/600 | Avg Reward: 298100.20 | Epsilon: 0.423 | Loss: 295203.5312\n",
      "Episode 440/600 | Avg Reward: 300624.99 | Epsilon: 0.414 | Loss: 320659.8125\n",
      "Episode 450/600 | Avg Reward: 299308.16 | Epsilon: 0.406 | Loss: 339790.7188\n",
      "Episode 460/600 | Avg Reward: 294803.19 | Epsilon: 0.398 | Loss: 336930.2188\n",
      "Episode 470/600 | Avg Reward: 307463.37 | Epsilon: 0.390 | Loss: 334991.2812\n",
      "Episode 480/600 | Avg Reward: 291049.52 | Epsilon: 0.383 | Loss: 333733.9375\n",
      "Episode 490/600 | Avg Reward: 304387.70 | Epsilon: 0.375 | Loss: 332722.8438\n",
      "Episode 500/600 | Avg Reward: 299643.62 | Epsilon: 0.368 | Loss: 323534.9375\n",
      "Episode 510/600 | Avg Reward: 298643.69 | Epsilon: 0.360 | Loss: 369616.8438\n",
      "Episode 520/600 | Avg Reward: 305181.33 | Epsilon: 0.353 | Loss: 362158.2812\n",
      "Episode 530/600 | Avg Reward: 309140.26 | Epsilon: 0.346 | Loss: 411134.2188\n",
      "Episode 540/600 | Avg Reward: 308478.07 | Epsilon: 0.339 | Loss: 389860.7188\n",
      "Episode 550/600 | Avg Reward: 295526.44 | Epsilon: 0.333 | Loss: 412957.9062\n",
      "Episode 560/600 | Avg Reward: 309252.45 | Epsilon: 0.326 | Loss: 453392.5625\n",
      "Episode 570/600 | Avg Reward: 305541.21 | Epsilon: 0.319 | Loss: 397604.6875\n",
      "Episode 580/600 | Avg Reward: 306369.57 | Epsilon: 0.313 | Loss: 397952.8438\n",
      "Episode 590/600 | Avg Reward: 307160.32 | Epsilon: 0.307 | Loss: 470697.6250\n",
      "Episode 600/600 | Avg Reward: 306577.41 | Epsilon: 0.301 | Loss: 516229.0000\n",
      "\n",
      "======================================================================\n",
      "‚úÖ TRAINING HO√ÄN T·∫§T!\n",
      "======================================================================\n",
      "\n",
      "üìä Final Statistics:\n",
      "   Total episodes: 600\n",
      "   Average reward (last 50): 306980.19\n",
      "   Max reward: 316542.62\n",
      "   Min reward: 7350.44\n",
      "   Final epsilon: 0.3008\n",
      "   Checkpoint saved to: c:\\Study\\NCKH\\QLKHO-RL\\checkpointDQN_A2Cstyle\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# TRAIN DQN V·ªöI A2C-STYLE ENVIRONMENT - 600 EPISODES √ó 900 STEPS\n",
    "# =================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ TRAINING DQN V·ªöI A2C-STYLE ENVIRONMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create A2C-style environment\n",
    "env_a2c_style = A2CStyleInventoryEnv(\n",
    "    num_products=220,\n",
    "    num_timesteps=900,  # 900 steps per episode\n",
    "    waste_rate=0.025\n",
    ")\n",
    "\n",
    "# Create DQN trainer\n",
    "trainer_v2 = DQNTrainer(\n",
    "    env=env_a2c_style,\n",
    "    hidden_size=32,\n",
    "    lr=0.001,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.998  # Slower decay for 600 episodes\n",
    ")\n",
    "\n",
    "print(\"\\nüìã Training Configuration:\")\n",
    "print(f\"   Environment: A2CStyleInventoryEnv (based on training.py)\")\n",
    "print(f\"   Episodes: 600\")\n",
    "print(f\"   Steps per episode: 900\")\n",
    "print(f\"   Total steps: 540,000\")\n",
    "print(f\"   Num products: 100\")\n",
    "print(f\"   State: [avg_inventory, avg_sales_forecast, avg_waste]\")\n",
    "print(f\"   Actions: 14 discrete levels (same as A2C)\")\n",
    "print(f\"   Hidden size: 32\")\n",
    "print(f\"   Learning rate: 0.001\")\n",
    "print(f\"   Gamma: 0.99\")\n",
    "print(f\"   Batch size: 64\")\n",
    "print(f\"   Epsilon decay: 0.998 (slower for more exploration)\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  L∆∞u √Ω: Training 600 episodes c√≥ th·ªÉ m·∫•t 10-15 ph√∫t\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚è≥ Starting training...\")\n",
    "\n",
    "# Train\n",
    "checkpoint_path_v2 = r'c:\\Study\\NCKH\\QLKHO-RL\\checkpointDQN_A2Cstyle'\n",
    "\n",
    "rewards_v2, losses_v2 = trainer_v2.train(\n",
    "    num_episodes=600,\n",
    "    batch_size=64,\n",
    "    update_target_freq=10,\n",
    "    verbose=True,\n",
    "    save_freq=50,  # Save every 50 episodes\n",
    "    save_path=checkpoint_path_v2\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TRAINING HO√ÄN T·∫§T!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Final Statistics:\")\n",
    "print(f\"   Total episodes: {len(rewards_v2)}\")\n",
    "print(f\"   Average reward (last 50): {np.mean(rewards_v2[-50:]):.2f}\")\n",
    "print(f\"   Max reward: {np.max(rewards_v2):.2f}\")\n",
    "print(f\"   Min reward: {np.min(rewards_v2):.2f}\")\n",
    "print(f\"   Final epsilon: {trainer_v2.epsilon:.4f}\")\n",
    "print(f\"   Checkpoint saved to: {checkpoint_path_v2}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeaffa9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ TRAINING COMPLETE!\n",
    "\n",
    "### Summary:\n",
    "- ‚úÖ DQN trained v·ªõi 600 episodes √ó 900 steps\n",
    "- ‚úÖ Environment gi·ªëng A2C/A2C_mod t·ª´ training.py\n",
    "- ‚úÖ Architecture t∆∞∆°ng th√≠ch: [3‚Üí32‚Üí32‚Üí32‚Üí14]\n",
    "- ‚úÖ Checkpoint saved for comparison\n",
    "\n",
    "### Next Steps:\n",
    "1. Load checkpoint n√†y v√†o [RDX-MSX.ipynb](RDX-MSX.ipynb)\n",
    "2. So s√°nh RDX features v·ªõi A2C v√† A2C_mod\n",
    "3. Ph√¢n t√≠ch decision-making differences\n",
    "\n",
    "### Key Files:\n",
    "- **Checkpoint**: `checkpointDQN_A2Cstyle/`\n",
    "- **Visualization**: `dqn_training_results.png`\n",
    "- **Notebook**: [Train_DQN.ipynb](Train_DQN.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d43789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üíæ SAVING FINAL MODEL\n",
      "======================================================================\n",
      "   ‚úÖ Final model saved to:\n",
      "      c:\\Study\\NCKH\\QLKHO-RL\\checkpointDQN_A2Cstyle\n",
      "\n",
      "   üìù Use this checkpoint for:\n",
      "      - RDX analysis\n",
      "      - Comparison with A2C/A2C_mod\n",
      "      - Testing and evaluation\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# SAVE FINAL MODEL\n",
    "# =================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üíæ SAVING FINAL MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_checkpoint_path = r'c:\\Study\\NCKH\\QLKHO-RL\\checkpointDQN_A2Cstyle'\n",
    "os.makedirs(final_checkpoint_path, exist_ok=True)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    q_network=trainer_v2.q_network,\n",
    "    optimizer=trainer_v2.optimizer\n",
    ")\n",
    "checkpoint.save(os.path.join(final_checkpoint_path, 'ckpt-final'))\n",
    "\n",
    "print(f\"   ‚úÖ Final model saved to:\")\n",
    "print(f\"      {final_checkpoint_path}\")\n",
    "print(f\"\\n   üìù Use this checkpoint for:\")\n",
    "print(f\"      - RDX analysis\")\n",
    "print(f\"      - Comparison with A2C/A2C_mod\")\n",
    "print(f\"      - Testing and evaluation\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c0b5b4",
   "metadata": {},
   "source": [
    "## 7. SAVE FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0007b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# TEST AGENT PERFORMANCE\n",
    "# =================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ TESTING TRAINED DQN AGENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_episodes = 10\n",
    "test_rewards = []\n",
    "\n",
    "for ep in range(test_episodes):\n",
    "    state = env_a2c_style.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = trainer_v2.select_action(state, training=False)  # Greedy\n",
    "        next_state, reward, done, info = env_a2c_style.step(action)\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    test_rewards.append(episode_reward)\n",
    "    print(f\"   Test Episode {ep+1}: Reward = {episode_reward:.2f}\")\n",
    "\n",
    "print(f\"\\nüìä Test Results:\")\n",
    "print(f\"   Average reward: {np.mean(test_rewards):.2f}\")\n",
    "print(f\"   Std deviation: {np.std(test_rewards):.2f}\")\n",
    "print(f\"   Min reward: {np.min(test_rewards):.2f}\")\n",
    "print(f\"   Max reward: {np.max(test_rewards):.2f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59b3e4",
   "metadata": {},
   "source": [
    "## 6. TEST TRAINED AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642f7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# VISUALIZATION - TRAINING CURVES\n",
    "# =================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä VISUALIZATION: TRAINING CURVES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Episode Rewards\n",
    "ax1 = axes[0]\n",
    "ax1.plot(rewards_v2, alpha=0.3, color='#2E86AB', linewidth=0.5, label='Raw rewards')\n",
    "\n",
    "# Moving average\n",
    "window = 20\n",
    "moving_avg = np.convolve(rewards_v2, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(window-1, len(rewards_v2)), moving_avg, color='#2E86AB', \n",
    "         linewidth=2, label=f'Moving Avg ({window})')\n",
    "\n",
    "ax1.set_xlabel('Episode', fontweight='bold', fontsize=12)\n",
    "ax1.set_ylabel('Total Reward', fontweight='bold', fontsize=12)\n",
    "ax1.set_title('DQN Training: Episode Rewards', fontweight='bold', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Training Loss\n",
    "ax2 = axes[1]\n",
    "ax2.plot(losses_v2, alpha=0.3, color='#E74C3C', linewidth=0.5, label='Raw loss')\n",
    "\n",
    "# Moving average\n",
    "moving_avg_loss = np.convolve(losses_v2, np.ones(window)/window, mode='valid')\n",
    "ax2.plot(range(window-1, len(losses_v2)), moving_avg_loss, color='#E74C3C', \n",
    "         linewidth=2, label=f'Moving Avg ({window})')\n",
    "\n",
    "ax2.set_xlabel('Episode', fontweight='bold', fontsize=12)\n",
    "ax2.set_ylabel('Loss', fontweight='bold', fontsize=12)\n",
    "ax2.set_title('DQN Training: Loss', fontweight='bold', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dqn_training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Training Curve Analysis:\")\n",
    "print(f\"   Initial reward (ep 1-50): {np.mean(rewards_v2[:50]):.2f}\")\n",
    "print(f\"   Middle reward (ep 275-325): {np.mean(rewards_v2[275:325]):.2f}\")\n",
    "print(f\"   Final reward (ep 550-600): {np.mean(rewards_v2[-50:]):.2f}\")\n",
    "improvement = ((np.mean(rewards_v2[-50:]) - np.mean(rewards_v2[:50])) / abs(np.mean(rewards_v2[:50])) * 100)\n",
    "print(f\"   Improvement: {improvement:.1f}%\")\n",
    "print(f\"\\n   üìä Plot saved: dqn_training_results.png\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4122b8",
   "metadata": {},
   "source": [
    "## 5. VISUALIZATION & ANALYSIS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
