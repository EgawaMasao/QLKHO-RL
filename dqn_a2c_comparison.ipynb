{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa000001",
   "metadata": {},
   "source": [
    "# DQN for Inventory Management — A2C Drop-in Comparison\n",
    "\n",
    "**Architecture matches A2C Mod** (`training_220.ipynb`) exactly:\n",
    "- Same state space: `[inventory, sales, waste]` per product → flat `[660]`\n",
    "- Same action space: 14 discrete values\n",
    "- Same reward function: `r = 1 - z - overstock - q - quan`\n",
    "- Same data parsers and TFRecord files\n",
    "- Same 600 episodes × 900 timesteps\n",
    "\n",
    "**DQN Additions over A2C:**\n",
    "- Experience Replay Buffer (100 000 capacity)\n",
    "- Target Network (updated every 10 episodes)\n",
    "- Double-DQN update rule\n",
    "- Epsilon-Greedy exploration\n",
    "- GroupNormalization(groups=1) after each hidden layer (matches Critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa000002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.14 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.14.0)\n",
      "Requirement already satisfied: tensorflow-addons==0.22.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: wandb in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.14.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.14) (2.14.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-addons==0.22.0) (2.13.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\lviet\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-addons==0.22.0) (25.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (3.15.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lviet\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lviet\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (4.15.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (1.14.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (1.76.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow==2.14) (2.14.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (2.43.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (3.10)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (2.32.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (3.1.3)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (2.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (0.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lviet\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: click>=8.0.1 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (3.1.46)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\lviet\\appdata\\roaming\\python\\python310\\site-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (2.12.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (6.0.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (2.49.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3->wandb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.14.0->tensorflow==2.14) (0.45.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lviet\\appdata\\roaming\\python\\python310\\site-packages (from click>=8.0.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (3.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lviet\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow==2.14) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0. DEPENDENCIES\n",
    "# ============================================================\n",
    "import sys\n",
    "# Uncomment if needed:\n",
    "!{sys.executable} -m pip install tensorflow==2.14 tensorflow-addons==0.22.0 numpy pandas matplotlib wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa000003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lviet\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow : 2.14.0\n",
      "GPU devices: []\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORTS\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "np.set_printoptions(edgeitems=25, linewidth=10000, precision=8, suppress=True)\n",
    "\n",
    "print(f\"TensorFlow : {tf.__version__}\")\n",
    "print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa000004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Products       : 220\n",
      "  State features : 660  (220 × 3)\n",
      "  Episodes       : 600\n",
      "  Timesteps/ep   : 900\n",
      "  Batch size     : 32\n",
      "  Learning rate  : 0.001\n",
      "  Hidden size    : 128\n",
      "  GroupNorm      : True\n",
      "  Replay buffer  : 100000\n",
      "  Gamma          : 0.99\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. CONFIGURATION  (mirrors training_220.ipynb FLAGS)\n",
    "# ============================================================\n",
    "\n",
    "class Config:\n",
    "    # ── Environment ─────────────────────────────────────────\n",
    "    num_products          = 220\n",
    "    num_features_per_prod = 3        # [inventory x, sales, waste q]\n",
    "    num_features          = 220 * 3  # flat state: 660\n",
    "    num_actions           = 14\n",
    "    num_timesteps         = 900\n",
    "\n",
    "    # ── Shared training knobs ────────────────────────────────\n",
    "    train_episodes = 600\n",
    "    batch_size     = 32       # same as A2C\n",
    "    gamma          = 0.99     # same as A2C\n",
    "    waste          = 0.025    # same as A2C\n",
    "    zero_inventory = 1e-5     # same as A2C\n",
    "\n",
    "    # ── Network architecture ─────────────────────────────────\n",
    "    hidden_size  = 128        # matches DQN hidden_size request\n",
    "    dropout_prob = 0.1        # same as A2C actor/critic\n",
    "    use_group_norm = True     # GroupNormalization(groups=1) like A2C Critic\n",
    "\n",
    "    # ── DQN-specific ─────────────────────────────────────────\n",
    "    learning_rate          = 0.001   # same as A2C actor/critic lr for fairness\n",
    "    replay_buffer_size     = 100_000\n",
    "    min_replay_size        = 1_000\n",
    "    epsilon_start          = 1.0\n",
    "    epsilon_end            = 0.01\n",
    "    epsilon_decay_episodes = 400     # decay over first 400 episodes\n",
    "    target_update_freq     = 10      # episodes between target-network syncs\n",
    "\n",
    "    # ── Action space (same 14 values as A2C) ─────────────────\n",
    "    action_space = [\n",
    "        0, 0.005, 0.01, 0.0125, 0.015, 0.0175,\n",
    "        0.02, 0.03, 0.04, 0.08, 0.12, 0.2, 0.5, 1\n",
    "    ]\n",
    "\n",
    "    # ── File paths (same as A2C 220-product setup) ────────────\n",
    "    train_file    = 'data220/train.tfrecords'\n",
    "    capacity_file = 'data220/capacity.tfrecords'\n",
    "    stock_file    = 'data220/stock.tfrecords'\n",
    "    predict_file  = 'data220/test.tfrecords'\n",
    "    output_dir    = 'checkpoints_dqn_comparison3'\n",
    "    output_file   = './output_dqn_comparison.csv3'\n",
    "\n",
    "    # ── W&B ──────────────────────────────────────────────────\n",
    "    use_wandb    = False          # set True to enable\n",
    "    wandb_project = 'inventory-dqn-vs-a2c'\n",
    "\n",
    "\n",
    "FLAGS = Config()\n",
    "os.makedirs(FLAGS.output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Products       : {FLAGS.num_products}\")\n",
    "print(f\"  State features : {FLAGS.num_features}  ({FLAGS.num_products} × {FLAGS.num_features_per_prod})\")\n",
    "print(f\"  Episodes       : {FLAGS.train_episodes}\")\n",
    "print(f\"  Timesteps/ep   : {FLAGS.num_timesteps}\")\n",
    "print(f\"  Batch size     : {FLAGS.batch_size}\")\n",
    "print(f\"  Learning rate  : {FLAGS.learning_rate}\")\n",
    "print(f\"  Hidden size    : {FLAGS.hidden_size}\")\n",
    "print(f\"  GroupNorm      : {FLAGS.use_group_norm}\")\n",
    "print(f\"  Replay buffer  : {FLAGS.replay_buffer_size}\")\n",
    "print(f\"  Gamma          : {FLAGS.gamma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa000005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data parsers defined (identical to training_220.ipynb)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. DATA PARSERS  (identical to training_220.ipynb)\n",
    "# ============================================================\n",
    "\n",
    "def sales_parser(serialized_example):\n",
    "    \"\"\"Parse a single sales record from TFRecordDataset.\"\"\"\n",
    "    example = tf.io.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\"sales\": tf.io.FixedLenFeature([FLAGS.num_products], tf.float32)}\n",
    "    )\n",
    "    return example\n",
    "\n",
    "\n",
    "def capacity_parser(serialized_example):\n",
    "    \"\"\"Parse a single capacity record from TFRecordDataset.\"\"\"\n",
    "    example = tf.io.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\"capacity\": tf.io.FixedLenFeature([FLAGS.num_products], tf.float32)}\n",
    "    )\n",
    "    return example\n",
    "\n",
    "\n",
    "def stock_parser(serialized_example):\n",
    "    \"\"\"Parse a single stock record from TFRecordDataset.\"\"\"\n",
    "    example = tf.io.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\"stock\": tf.io.FixedLenFeature([FLAGS.num_products], tf.float32)}\n",
    "    )\n",
    "    return example\n",
    "\n",
    "\n",
    "print(\"Data parsers defined (identical to training_220.ipynb)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa000006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment helpers defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. ENVIRONMENT HELPERS  (identical to training_220.ipynb)\n",
    "# ============================================================\n",
    "\n",
    "def waste(x):\n",
    "    \"\"\"Waste fraction q̂ = waste_rate * inventory.\"\"\"\n",
    "    return FLAGS.waste * x\n",
    "\n",
    "\n",
    "# def calc_reward(x_clip, sales_now, overstock):\n",
    "    \"\"\"\n",
    "    Reward per product (NumPy arrays) — CORRECTED LOGIC.\n",
    "\n",
    "    r = 1 - z - overstock - q - quan\n",
    "\n",
    "    where:\n",
    "      z    = stockout indicator (1 if demand exceeds available stock)\n",
    "      q    = waste of inventory AFTER replenishment (x_clip)\n",
    "      quan = quantile spread (95th - 5th percentile of x_clip)\n",
    "\n",
    "    FIXED ISSUES:\n",
    "      ❌ Old: Checked stockout on OLD inventory (before action)\n",
    "      ✅ New: Check stockout based on unfulfilled demand\n",
    "      \n",
    "      ❌ Old: Calculated waste on OLD inventory (before action)\n",
    "      ✅ New: Calculate waste on inventory AFTER replenishment\n",
    "      \n",
    "      ❌ Old: Quantile spread on OLD inventory\n",
    "      ✅ New: Quantile spread on inventory AFTER replenishment\n",
    "    \n",
    "    Timeline:\n",
    "      t:   inventory x\n",
    "      ↓    +action (replenishment)\n",
    "      t+:  x_clip (after replenishment & capacity constraint)\n",
    "      ↓    waste happens here: q = waste(x_clip)\n",
    "      ↓    sales fulfillment    \n",
    "      t+1: x_next (remaining inventory)\n",
    "      ↓    stockout if sales_now > x_clip\n",
    "    \"\"\"\n",
    "    # # CORRECTED: Check stockout based on unfulfilled demand\n",
    "    # stockout_amount = np.maximum(0.0, sales_now - x_clip)        # [P] amount of unmet demand\n",
    "    # z = (stockout_amount > FLAGS.zero_inventory).astype(np.float32)  # [P] binary indicator\n",
    "    \n",
    "    # # CORRECTED: Waste happens on inventory AFTER replenishment\n",
    "    # q = waste(x_clip)                                            # [P] waste on post-action inventory\n",
    "    \n",
    "    # # CORRECTED: Quantile spread on inventory distribution AFTER replenishment\n",
    "    # quan = float(np.quantile(x_clip, 0.95) - np.quantile(x_clip, 0.05))  # scalar\n",
    "    # quan_vec = np.full(FLAGS.num_products, quan, dtype=np.float32)       # [P]\n",
    "    \n",
    "    # r = (1.0 - z - overstock - q - quan_vec).astype(np.float32)\n",
    "    # return r, z, quan\n",
    "def calc_reward(x_old, overstock):\n",
    "    \"\"\"\n",
    "    Reward per product — identical to A2C_mod in training1.py (lines 399-404).\n",
    "\n",
    "    All penalty terms are computed from x_old (inventory BEFORE action),\n",
    "    exactly matching:\n",
    "        z    = tf.cast(x < FLAGS.zero_inventory, tf.float32)\n",
    "        quan = tf.repeat(quantile(x, 0.95) - quantile(x, 0.05), num_products)\n",
    "        r    = 1 - z - overstock - q - quan\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_old     : np.ndarray [P]  — inventory BEFORE action (same as 'x' in A2C_mod)\n",
    "    overstock : np.ndarray [P]  — max(0, x_old + u - 1)\n",
    "    \"\"\"\n",
    "    # z: stockout indicator on OLD inventory (before action)\n",
    "    z = (x_old < FLAGS.zero_inventory).astype(np.float32)            # [P]\n",
    "\n",
    "    # q: waste on OLD inventory (before action)\n",
    "    q = waste(x_old)                                                  # [P]\n",
    "\n",
    "    # quan: quantile spread on OLD inventory, broadcast to all products\n",
    "    quan = float(np.quantile(x_old, 0.95) - np.quantile(x_old, 0.05))  # scalar\n",
    "    quan_vec = np.full(FLAGS.num_products, quan, dtype=np.float32)      # [P]\n",
    "\n",
    "    r = (1.0 - z - overstock - q - quan_vec).astype(np.float32)      # [P]\n",
    "    return r, z, quan\n",
    "\n",
    "print(\"Environment helpers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa000007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Network output shape:  (2, 220, 14)\n",
      "Parameters: 36,110\n",
      "PerProduct MultiProductQNetwork defined ✓\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. Q-NETWORK  (Per-Product, matches A2C cloned-agent design)\n",
    "#\n",
    "# OLD (commented below): Global network [B, 660] -> [B, 220, 14]\n",
    "#   All products share info through hidden layers — unfair vs A2C\n",
    "#\n",
    "# NEW: Per-product network [B*P, 3] -> [B*P, 14] -> [B, P, 14]\n",
    "#   Each product only sees its own (x_i, sales_i, q_i)\n",
    "#   Same weights applied to every product (cloned agent)\n",
    "#   Matches A2C Actor architecture for fair comparison\n",
    "# ============================================================\n",
    "\n",
    "# ┌──────────────────────────────────────────────────────────┐\n",
    "# │  OLD: Global Q-Network (COMMENTED OUT)                  │\n",
    "# └──────────────────────────────────────────────────────────┘\n",
    "# # ============================================================\n",
    "# # 5. Q-NETWORK\n",
    "# #\n",
    "# # Architecture mirrors the A2C Actor/Critic:\n",
    "# #   Dense → GroupNorm(groups=1) → ReLU → Dropout   (×3 hidden layers)\n",
    "# #   Dense → reshape to [B, P, A]\n",
    "# # ============================================================\n",
    "#\n",
    "# class MultiProductQNetwork(tf.keras.Model):\n",
    "#     \"\"\"\n",
    "#     Q-Network for multi-product inventory management.\n",
    "#\n",
    "#     Input  : [B, num_features]         e.g. [B, 660]\n",
    "#     Output : [B, num_products, num_actions]  e.g. [B, 220, 14]\n",
    "#\n",
    "#     Each hidden layer uses:\n",
    "#         Dense → GroupNormalization(groups=1) → ReLU → Dropout\n",
    "#     This matches the GroupNorm usage in the A2C Critic.\n",
    "#     \"\"\"\n",
    "#\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         num_features: int,\n",
    "#         num_products: int,\n",
    "#         num_actions: int,\n",
    "#         hidden_size: int,\n",
    "#         dropout_prob: float = 0.1,\n",
    "#         use_group_norm: bool = True,\n",
    "#         name: str | None = None,\n",
    "#     ):\n",
    "#         super().__init__(name=name)\n",
    "#\n",
    "#         self.num_products = num_products\n",
    "#         self.num_actions  = num_actions\n",
    "#\n",
    "#         # ── Shared trunk (same depth as A2C Actor: 3 hidden layers) ──\n",
    "#         self.dense1 = tf.keras.layers.Dense(hidden_size, activation=None, name=\"dense1\")\n",
    "#         self.dense2 = tf.keras.layers.Dense(hidden_size, activation=None, name=\"dense2\")\n",
    "#         self.dense3 = tf.keras.layers.Dense(hidden_size, activation=None, name=\"dense3\")\n",
    "#\n",
    "#         # ── Output: one Q-value per (product, action) ──────────────\n",
    "#         self.out = tf.keras.layers.Dense(num_products * num_actions, activation=None, name=\"output\")\n",
    "#\n",
    "#         # ── Normalisation & regularisation ─────────────────────────\n",
    "#         self._use_gn = use_group_norm\n",
    "#         if use_group_norm:\n",
    "#             # GroupNormalization(groups=1) == LayerNorm on the channel axis\n",
    "#             # Matches tfa.layers.GroupNormalization(groups=1) from A2C Critic\n",
    "#             self.gn1 = tfa.layers.GroupNormalization(groups=1, name=\"gn1\")\n",
    "#             self.gn2 = tfa.layers.GroupNormalization(groups=1, name=\"gn2\")\n",
    "#             self.gn3 = tfa.layers.GroupNormalization(groups=1, name=\"gn3\")\n",
    "#\n",
    "#         self.drop1 = tf.keras.layers.Dropout(dropout_prob)\n",
    "#         self.drop2 = tf.keras.layers.Dropout(dropout_prob)\n",
    "#         self.drop3 = tf.keras.layers.Dropout(dropout_prob)\n",
    "#\n",
    "#     def call(self, state, training: bool = False):\n",
    "#         # state : [B, num_features]\n",
    "#         x = self.dense1(state)\n",
    "#         if self._use_gn:\n",
    "#             x = self.gn1(x, training=training)\n",
    "#         x = tf.nn.relu(x)\n",
    "#         x = self.drop1(x, training=training)\n",
    "#\n",
    "#         x = self.dense2(x)\n",
    "#         if self._use_gn:\n",
    "#             x = self.gn2(x, training=training)\n",
    "#         x = tf.nn.relu(x)\n",
    "#         x = self.drop2(x, training=training)\n",
    "#\n",
    "#         x = self.dense3(x)\n",
    "#         if self._use_gn:\n",
    "#             x = self.gn3(x, training=training)\n",
    "#         x = tf.nn.relu(x)\n",
    "#         x = self.drop3(x, training=training)\n",
    "#\n",
    "#         q = self.out(x)                          # [B, P*A]\n",
    "#         bsz = tf.shape(state)[0]\n",
    "#         q = tf.reshape(q, [bsz, self.num_products, self.num_actions])  # [B, P, A]\n",
    "#         return q\n",
    "#\n",
    "#\n",
    "# # Quick sanity-check\n",
    "# _dummy = tf.zeros([2, FLAGS.num_features])\n",
    "# _net   = MultiProductQNetwork(\n",
    "#     FLAGS.num_features, FLAGS.num_products, FLAGS.num_actions,\n",
    "#     FLAGS.hidden_size, FLAGS.dropout_prob, FLAGS.use_group_norm, name=\"test\"\n",
    "# )\n",
    "# _out   = _net(_dummy, training=False)\n",
    "# print(f\"Q-Network output shape: {_out.shape}\")\n",
    "# assert _out.shape == (2, FLAGS.num_products, FLAGS.num_actions), \"Shape mismatch!\"\n",
    "# del _dummy, _net, _out\n",
    "# print(\"MultiProductQNetwork defined ✓\")\n",
    "\n",
    "\n",
    "# ┌──────────────────────────────────────────────────────────┐\n",
    "# │  NEW: Per-Product Q-Network (matches A2C cloned-agent)  │\n",
    "# └──────────────────────────────────────────────────────────┘\n",
    "\n",
    "class MultiProductQNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Per-Product Q-Network — each product is processed INDEPENDENTLY.\n",
    "\n",
    "    External interface unchanged:\n",
    "      Input  : [B, num_features]              e.g. [B, 660]\n",
    "      Output : [B, num_products, num_actions]  e.g. [B, 220, 14]\n",
    "\n",
    "    Internally:\n",
    "      1. Reshape  [B, 660] -> [B, 3, 220] -> [B, 220, 3]  (split per product)\n",
    "      2. Flatten  [B, 220, 3] -> [B*220, 3]\n",
    "      3. Forward  [B*220, 3] -> Dense(3->H)->GN->ReLU->Drop x3 -> Dense(H->14)\n",
    "      4. Reshape  [B*220, 14] -> [B, 220, 14]\n",
    "\n",
    "    This matches A2C Actor exactly:\n",
    "      - Same input per product: [x_i, sales_i, q_i]  (3 features)\n",
    "      - Same network depth: 3 hidden layers\n",
    "      - Same hidden size (configurable)\n",
    "      - Same GroupNorm(groups=1) + ReLU + Dropout\n",
    "      - Product i has NO access to product j's features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        num_products,\n",
    "        num_actions,\n",
    "        hidden_size,\n",
    "        dropout_prob=0.1,\n",
    "        use_group_norm=True,\n",
    "        name=None,\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.num_products      = num_products\n",
    "        self.num_actions        = num_actions\n",
    "        self.features_per_prod  = num_features // num_products  # 660 // 220 = 3\n",
    "\n",
    "        # ── Per-product trunk (3 hidden layers, same as A2C Actor) ──\n",
    "        self.dense1 = tf.keras.layers.Dense(hidden_size, activation=None, name=\"dense1\")\n",
    "        self.dense2 = tf.keras.layers.Dense(hidden_size, activation=None, name=\"dense2\")\n",
    "        self.dense3 = tf.keras.layers.Dense(hidden_size, activation=None, name=\"dense3\")\n",
    "\n",
    "        # ── Output: num_actions Q-values per product ───────────\n",
    "        self.out = tf.keras.layers.Dense(num_actions, activation=None, name=\"output\")\n",
    "\n",
    "        # ── Normalisation & regularisation ─────────────────────\n",
    "        self._use_gn = use_group_norm\n",
    "        if use_group_norm:\n",
    "            self.gn1 = tfa.layers.GroupNormalization(groups=1, name=\"gn1\")\n",
    "            self.gn2 = tfa.layers.GroupNormalization(groups=1, name=\"gn2\")\n",
    "            self.gn3 = tfa.layers.GroupNormalization(groups=1, name=\"gn3\")\n",
    "\n",
    "        self.drop1 = tf.keras.layers.Dropout(dropout_prob)\n",
    "        self.drop2 = tf.keras.layers.Dropout(dropout_prob)\n",
    "        self.drop3 = tf.keras.layers.Dropout(dropout_prob)\n",
    "\n",
    "    def call(self, state, training=False):\n",
    "        \"\"\"\n",
    "        state: [B, F]  where F = num_products * features_per_prod  (e.g. 660)\n",
    "\n",
    "        State layout: [x_0..x_P, sales_0..sales_P, q_0..q_P]\n",
    "        Rearrange to [B, P, 3] where dim 2 = [x_i, sales_i, q_i]\n",
    "        \"\"\"\n",
    "        B = tf.shape(state)[0]\n",
    "        P = self.num_products\n",
    "        F = self.features_per_prod  # 3\n",
    "\n",
    "        # [B, 660] -> [B, 3, 220] -> [B, 220, 3]\n",
    "        state_3d = tf.reshape(state, [B, F, P])       # [B, 3, 220]\n",
    "        state_3d = tf.transpose(state_3d, [0, 2, 1])  # [B, 220, 3]\n",
    "\n",
    "        # Flatten products into batch dim: [B*220, 3]\n",
    "        x = tf.reshape(state_3d, [B * P, F])          # [B*P, 3]\n",
    "\n",
    "        # ── Per-product forward (same weights for every product) ──\n",
    "        x = self.dense1(x)                             # [B*P, H]\n",
    "        if self._use_gn:\n",
    "            x = self.gn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.drop1(x, training=training)\n",
    "\n",
    "        x = self.dense2(x)                             # [B*P, H]\n",
    "        if self._use_gn:\n",
    "            x = self.gn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.drop2(x, training=training)\n",
    "\n",
    "        x = self.dense3(x)                             # [B*P, H]\n",
    "        if self._use_gn:\n",
    "            x = self.gn3(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.drop3(x, training=training)\n",
    "\n",
    "        q = self.out(x)                                # [B*P, A]\n",
    "\n",
    "        # Reshape back: [B*P, A] -> [B, P, A]\n",
    "        q = tf.reshape(q, [B, P, self.num_actions])    # [B, 220, 14]\n",
    "        return q\n",
    "\n",
    "\n",
    "# Quick sanity-check\n",
    "_dummy = tf.zeros([2, FLAGS.num_features])\n",
    "_net   = MultiProductQNetwork(\n",
    "    FLAGS.num_features, FLAGS.num_products, FLAGS.num_actions,\n",
    "    FLAGS.hidden_size, FLAGS.dropout_prob, FLAGS.use_group_norm, name=\"test\"\n",
    ")\n",
    "_out   = _net(_dummy, training=False)\n",
    "print(f\"Q-Network output shape:  {_out.shape}\")\n",
    "assert _out.shape == (2, FLAGS.num_products, FLAGS.num_actions)\n",
    "print(f\"Parameters: {sum(v.numpy().size for v in _net.trainable_variables):,}\")\n",
    "del _dummy, _net, _out\n",
    "print(\"PerProduct MultiProductQNetwork defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa000008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplayBuffer defined ✓\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6. EXPERIENCE REPLAY BUFFER\n",
    "# ============================================================\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Circular experience replay buffer.\n",
    "\n",
    "    Stores transitions: (state, action_indices, reward_vector, next_state, done)\n",
    "      state / next_state : np.float32 [num_features]   (660,)\n",
    "      action_indices     : np.int32   [num_products]   (220,) — index into action_space\n",
    "      reward_vector      : np.float32 [num_products]   (220,) — per-product reward\n",
    "      done               : float scalar\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action_indices, reward_vector, next_state, done):\n",
    "        self.buffer.append((state, action_indices, reward_vector, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          states      : [B, F]   float32\n",
    "          actions     : [B, P]   int32\n",
    "          rewards     : [B, P]   float32\n",
    "          next_states : [B, F]   float32\n",
    "          dones       : [B]      float32\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states      = np.array([e[0] for e in batch], dtype=np.float32)\n",
    "        actions     = np.array([e[1] for e in batch], dtype=np.int32)\n",
    "        rewards     = np.array([e[2] for e in batch], dtype=np.float32)\n",
    "        next_states = np.array([e[3] for e in batch], dtype=np.float32)\n",
    "        dones       = np.array([e[4] for e in batch], dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "print(\"ReplayBuffer defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa000009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiProductDQNAgent defined ✓\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7. DQN AGENT\n",
    "# ============================================================\n",
    "\n",
    "class MultiProductDQNAgent:\n",
    "    \"\"\"\n",
    "    Double-DQN agent for multi-product inventory management.\n",
    "\n",
    "    Design mirrors the A2C for fair comparison:\n",
    "      - Same network depth / GroupNorm as A2C Critic\n",
    "      - Same learning rate (0.001) and optimizer (Adam)\n",
    "      - Huber loss (more stable than MSE for large action spaces)\n",
    "      - Double-DQN: online net selects actions, target net evaluates them\n",
    "      - Per-product epsilon-greedy exploration\n",
    "      - Vector reward: one reward per product (matches A2C reward shape)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # ── Online Q-network (trained every step) ────────────\n",
    "        self.q_network = MultiProductQNetwork(\n",
    "            config.num_features, config.num_products, config.num_actions,\n",
    "            config.hidden_size, config.dropout_prob, config.use_group_norm,\n",
    "            name=\"q_network\",\n",
    "        )\n",
    "        # ── Target Q-network (frozen copy, synced every N episodes) ──\n",
    "        self.target_network = MultiProductQNetwork(\n",
    "            config.num_features, config.num_products, config.num_actions,\n",
    "            config.hidden_size, config.dropout_prob, config.use_group_norm,\n",
    "            name=\"target_network\",\n",
    "        )\n",
    "\n",
    "        # ── Optimizer — same as A2C ───────────────────────────\n",
    "        self.optimizer = tf.optimizers.Adam(config.learning_rate)\n",
    "\n",
    "        # ── Replay buffer ─────────────────────────────────────\n",
    "        self.replay_buffer = ReplayBuffer(config.replay_buffer_size)\n",
    "\n",
    "        # ── Epsilon-greedy parameters ─────────────────────────\n",
    "        self.epsilon = config.epsilon_start\n",
    "        self.epsilon_decay = (\n",
    "            (config.epsilon_start - config.epsilon_end)\n",
    "            / config.epsilon_decay_episodes\n",
    "        )\n",
    "\n",
    "        # ── Step counter (used for checkpoint resume) ─────────\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int64)\n",
    "\n",
    "        # ── Build networks so weights exist before set_weights ─\n",
    "        _dummy = tf.zeros([1, config.num_features], dtype=tf.float32)\n",
    "        _ = self.q_network(_dummy, training=False)\n",
    "        _ = self.target_network(_dummy, training=False)\n",
    "        self.sync_target_network()\n",
    "\n",
    "        # ── Huber loss (stable for large Q-value magnitudes) ──\n",
    "        self._huber = tf.keras.losses.Huber(\n",
    "            reduction=tf.keras.losses.Reduction.NONE\n",
    "        )\n",
    "\n",
    "        self.action_space_arr = np.array(config.action_space, dtype=np.float32)\n",
    "\n",
    "    # ── Target network sync ────────────────────────────────────────\n",
    "    def sync_target_network(self):\n",
    "        \"\"\"Hard-copy weights from online → target network.\"\"\"\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "    # ── Action selection ────────────────────────────────────────────\n",
    "    def select_actions(self, state, training: bool = True):\n",
    "        \"\"\"\n",
    "        Vectorised epsilon-greedy action selection.\n",
    "\n",
    "        Input  : state [num_features]     (660,)\n",
    "        Output : action_indices [num_products]  — index into action_space\n",
    "        \"\"\"\n",
    "        state_batch = tf.expand_dims(\n",
    "            tf.convert_to_tensor(state, dtype=tf.float32), axis=0\n",
    "        )                                              # [1, F]\n",
    "        q_vals = self.q_network(state_batch, training=False)[0]  # [P, A]\n",
    "        greedy = tf.argmax(q_vals, axis=1, output_type=tf.int32)  # [P]\n",
    "\n",
    "        if not training:\n",
    "            return greedy.numpy()\n",
    "\n",
    "        # Per-product random exploration\n",
    "        explore = (\n",
    "            tf.random.uniform([self.config.num_products]) < self.epsilon\n",
    "        )\n",
    "        rand_acts = tf.random.uniform(\n",
    "            [self.config.num_products], 0, self.config.num_actions, dtype=tf.int32\n",
    "        )\n",
    "        return tf.where(explore, rand_acts, greedy).numpy()\n",
    "\n",
    "    # ── Double-DQN train step (compiled with tf.function) ──────────\n",
    "    @tf.function\n",
    "    def _train_step_tf(\n",
    "        self, states, actions, rewards, next_states, dones\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Double-DQN update with per-product (vector) rewards.\n",
    "\n",
    "        Tensors:\n",
    "          states      [B, F]\n",
    "          actions     [B, P]   int32 — action indices\n",
    "          rewards     [B, P]   float32\n",
    "          next_states [B, F]\n",
    "          dones       [B]      float32\n",
    "\n",
    "        Loss:\n",
    "          Huber( r + γ · Q_target(s', argmax_a Q_online(s',a)),  Q_online(s,a) )\n",
    "        \"\"\"\n",
    "        gamma       = tf.cast(self.config.gamma, tf.float32)\n",
    "        states      = tf.cast(states,      tf.float32)\n",
    "        next_states = tf.cast(next_states, tf.float32)\n",
    "        actions     = tf.cast(actions,     tf.int32)\n",
    "        rewards     = tf.cast(rewards,     tf.float32)\n",
    "        dones       = tf.cast(dones,       tf.float32)\n",
    "\n",
    "        B = tf.shape(states)[0]\n",
    "        P = self.config.num_products\n",
    "\n",
    "        # Index helpers: [B, P]\n",
    "        b_idx = tf.repeat(tf.range(B)[:, tf.newaxis], P, axis=1)  # [B, P]\n",
    "        p_idx = tf.repeat(tf.range(P)[tf.newaxis, :], B, axis=0)  # [B, P]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Q(s, a) from online network  →  [B, P]\n",
    "            q_all = self.q_network(states, training=True)          # [B, P, A]\n",
    "            g_idx  = tf.stack([b_idx, p_idx, actions], axis=-1)   # [B, P, 3]\n",
    "            q_sa   = tf.gather_nd(q_all, g_idx)                   # [B, P]\n",
    "\n",
    "            # Double-DQN: online net picks best next action\n",
    "            nq_online  = self.q_network(next_states, training=False)       # [B, P, A]\n",
    "            best_next  = tf.argmax(nq_online, axis=2, output_type=tf.int32) # [B, P]\n",
    "\n",
    "            # Target net evaluates that action\n",
    "            nq_target  = self.target_network(next_states, training=False)  # [B, P, A]\n",
    "            g_next_idx = tf.stack([b_idx, p_idx, best_next], axis=-1)      # [B, P, 3]\n",
    "            next_q     = tf.gather_nd(nq_target, g_next_idx)               # [B, P]\n",
    "\n",
    "            td_target = rewards + (1.0 - dones[:, tf.newaxis]) * gamma * next_q  # [B, P]\n",
    "\n",
    "            # Huber loss, mean across batch × products\n",
    "            loss = tf.reduce_mean(self._huber(td_target, q_sa))\n",
    "\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, 10.0)  # gradient clipping\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.q_network.trainable_variables)\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    # ── Public train-step wrapper ──────────────────────────────────\n",
    "    def train_step(self):\n",
    "        \"\"\"Sample a mini-batch and perform one gradient update. Returns loss or None.\"\"\"\n",
    "        if len(self.replay_buffer) < self.config.min_replay_size:\n",
    "            return None\n",
    "        batch = self.replay_buffer.sample(self.config.batch_size)\n",
    "        loss  = self._train_step_tf(*batch)\n",
    "        return float(loss.numpy())\n",
    "\n",
    "    # ── Epsilon decay ──────────────────────────────────────────────\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.config.epsilon_end, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "\n",
    "print(\"MultiProductDQNAgent defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa000010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dqn() defined ✓\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. TRAINING LOOP\n",
    "#\n",
    "# Follows training_220.ipynb A2C loop structure:\n",
    "#   - Load all sales from TFRecordDataset using sales_parser\n",
    "#   - Normalise by capacity (same as A2C)\n",
    "#   - 600 episodes × 900 timesteps\n",
    "#   - Same reward formula: r = 1 - z - overstock - q - quan\n",
    "#   - Checkpoint every 10 episodes\n",
    "# ============================================================\n",
    "\n",
    "def train_dqn():\n",
    "    \"\"\"Main DQN training loop — drop-in replacement for A2C train().\"\"\"\n",
    "\n",
    "    # ── Optional W&B init ──────────────────────────────────────────\n",
    "    if FLAGS.use_wandb:\n",
    "        import wandb\n",
    "        wandb.init(\n",
    "            project=FLAGS.wandb_project,\n",
    "            config=vars(FLAGS),\n",
    "        )\n",
    "\n",
    "    # ── Load all sales data (same as A2C: TFRecordDataset → sales_parser) ──\n",
    "    print(\"Loading data...\")\n",
    "    all_sales_raw = []\n",
    "    for rec in tf.data.TFRecordDataset(FLAGS.train_file).map(sales_parser):\n",
    "        all_sales_raw.append(rec[\"sales\"].numpy())\n",
    "    all_sales_raw = np.array(all_sales_raw, dtype=np.float32)  # [T, P]\n",
    "\n",
    "    # Capacity — single record (same as A2C)\n",
    "    capacity = next(\n",
    "        iter(tf.data.TFRecordDataset(FLAGS.capacity_file).map(capacity_parser))\n",
    "    )[\"capacity\"].numpy()  # [P]\n",
    "\n",
    "    # Normalise sales by capacity (same as A2C)\n",
    "    all_sales = all_sales_raw / capacity[np.newaxis, :]  # [T, P]\n",
    "    print(f\"Sales loaded: {all_sales.shape}  (timesteps × products)\")\n",
    "\n",
    "    # ── Initialise agent ────────────────────────────────────────────\n",
    "    print(\"Initialising DQN agent...\")\n",
    "    agent = MultiProductDQNAgent(FLAGS)\n",
    "\n",
    "    # ── Checkpoint setup ────────────────────────────────────────────\n",
    "    checkpoint = tf.train.Checkpoint(\n",
    "        optimizer     = agent.optimizer,\n",
    "        q_network     = agent.q_network,\n",
    "        target_network= agent.target_network,\n",
    "        step          = agent.global_step,\n",
    "    )\n",
    "    ckpt_manager = tf.train.CheckpointManager(\n",
    "        checkpoint, FLAGS.output_dir, max_to_keep=5\n",
    "    )\n",
    "\n",
    "    start_episode = 0\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        checkpoint.restore(ckpt_manager.latest_checkpoint)\n",
    "        start_episode = int(agent.global_step.numpy())\n",
    "        # Restore epsilon to where it would be after start_episode decays\n",
    "        agent.epsilon = max(\n",
    "            FLAGS.epsilon_end,\n",
    "            FLAGS.epsilon_start - agent.epsilon_decay * start_episode\n",
    "        )\n",
    "        print(f\"✓ Restored checkpoint from episode {start_episode}\")\n",
    "    else:\n",
    "        print(\"Starting fresh training\")\n",
    "\n",
    "    T = all_sales.shape[0]  # total timesteps in dataset\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DQN Training — A2C Fair Comparison\")\n",
    "    print(f\"{'Episodes':>12} : {FLAGS.train_episodes}\")\n",
    "    print(f\"{'Timesteps':>12} : {FLAGS.num_timesteps}\")\n",
    "    print(f\"{'LR':>12} : {FLAGS.learning_rate}\")\n",
    "    print(f\"{'Hidden':>12} : {FLAGS.hidden_size}\")\n",
    "    print(f\"{'GroupNorm':>12} : {FLAGS.use_group_norm}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # ── Episode loop ────────────────────────────────────────────────\n",
    "    for episode in range(start_episode, FLAGS.train_episodes):\n",
    "\n",
    "        # Random initial inventory in [0, 1], same as A2C\n",
    "        x = np.random.uniform(0, 1, size=FLAGS.num_products).astype(np.float32)\n",
    "\n",
    "        # Random start index in the time-series (A2C does a window slide)\n",
    "        max_start = max(0, T - FLAGS.num_timesteps - 1)\n",
    "        start_idx = np.random.randint(0, max_start + 1) if episode > 0 else 0\n",
    "        ep_len    = min(FLAGS.num_timesteps, T - start_idx - 1)\n",
    "\n",
    "        # Episode-level metric accumulators\n",
    "        ep_rewards   = []\n",
    "        ep_losses    = []\n",
    "        ep_stockouts = []\n",
    "        ep_waste     = []\n",
    "        ep_overstock = []\n",
    "        ep_quantile  = []\n",
    "\n",
    "        # ── Timestep loop ──────────────────────────────────────────\n",
    "        for t in range(ep_len):\n",
    "            idx          = start_idx + t\n",
    "            sales_now    = all_sales[idx]       # [P]  current period\n",
    "            sales_next   = all_sales[idx + 1]   # [P]  next-period forecast\n",
    "\n",
    "            # ── Build state (matches A2C: [x, sales, q] flat) ──────\n",
    "            q_now = waste(x)                    # [P]  waste estimate\n",
    "            state = np.concatenate(\n",
    "                [x, sales_now, q_now], axis=0\n",
    "            ).astype(np.float32)                # [660]\n",
    "\n",
    "            # ── Action selection ────────────────────────────────────\n",
    "            action_idx = agent.select_actions(state, training=True)  # [P]  int\n",
    "            actions    = agent.action_space_arr[action_idx]          # [P]  float\n",
    "\n",
    "            # ── Environment step (same dynamics as A2C) ────────────\n",
    "            x_rep  = x + actions                           # add replenishment\n",
    "            over   = np.maximum(0.0, x_rep - 1.0)         # overstock before clip\n",
    "            x_clip = np.minimum(1.0, x_rep)               # clip to capacity\n",
    "            x_next = np.maximum(0.0, x_clip - sales_now)  # fulfill demand\n",
    "\n",
    "            # ── Reward (CORRECTED: evaluate consequences of action) ──\n",
    "            # Pass x_clip (inventory AFTER replenishment) instead of x (OLD inventory)\n",
    "            # This correctly evaluates: waste on new stock, stockout from unmet demand\n",
    "            # r, z, quan = calc_reward(x_clip, sales_now, over)  # r: [P]\n",
    "            r, z, quan = calc_reward(x, over)\n",
    "            done = 1.0 if (t == ep_len - 1) else 0.0  # terminal flag at episode end\n",
    "           \n",
    "            # ── Build next-state ────────────────────────────────────\n",
    "            q_next     = waste(x_next)\n",
    "            next_state = np.concatenate(\n",
    "                [x_next, sales_next, q_next], axis=0\n",
    "            ).astype(np.float32)                           # [660]\n",
    "\n",
    "            # ── Store transition ────────────────────────────────────\n",
    "            agent.replay_buffer.add(state, action_idx, r, next_state, done)\n",
    "\n",
    "            # ── Collect metrics ─────────────────────────────────────\n",
    "            # CORRECTED: Track waste on x_clip (consistent with reward calculation)\n",
    "            # q_clip = waste(x_clip)\n",
    "            ep_rewards.append(float(np.mean(r)))\n",
    "            ep_stockouts.append(float(np.mean(z)))\n",
    "            # ep_waste.append(float(np.mean(q_clip)))  # waste after replenishment\n",
    "            ep_waste.append(float(np.mean(waste(x))))  # waste on old inventory (matches reward)\n",
    "            ep_overstock.append(float(np.mean(over)))\n",
    "            ep_quantile.append(float(quan))\n",
    "\n",
    "            # ── Gradient update ─────────────────────────────────────\n",
    "            loss = agent.train_step()\n",
    "            if loss is not None:\n",
    "                ep_losses.append(loss)\n",
    "\n",
    "            # ── Advance state ───────────────────────────────────────\n",
    "            x = x_next\n",
    "\n",
    "        # ── End of episode: epsilon decay, target sync, logging ────\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        if (episode + 1) % FLAGS.target_update_freq == 0:\n",
    "            agent.sync_target_network()\n",
    "\n",
    "        avg_r  = float(np.mean(ep_rewards))\n",
    "        avg_l  = float(np.mean(ep_losses)) if ep_losses else 0.0\n",
    "        avg_so = float(np.mean(ep_stockouts))\n",
    "        avg_w  = float(np.mean(ep_waste))\n",
    "        avg_o  = float(np.mean(ep_overstock))\n",
    "        avg_q  = float(np.mean(ep_quantile))\n",
    "\n",
    "        print(\n",
    "            f\"Ep {episode+1:4d}/{FLAGS.train_episodes} | \"\n",
    "            f\"R={avg_r:+.4f}  L={avg_l:.4f}  \"\n",
    "            f\"SO={avg_so:.4f}  W={avg_w:.4f}  \"\n",
    "            f\"O={avg_o:.4f}  Q={avg_q:.4f}  \"\n",
    "            f\"ε={agent.epsilon:.4f}  buf={len(agent.replay_buffer)}\"\n",
    "        )\n",
    "\n",
    "        if FLAGS.use_wandb:\n",
    "            import wandb\n",
    "            wandb.log({\n",
    "                \"episode\": episode + 1,\n",
    "                \"reward\":  avg_r, \"loss\": avg_l,\n",
    "                \"stockout\": avg_so, \"waste\": avg_w,\n",
    "                \"overstock\": avg_o, \"quantile\": avg_q,\n",
    "                \"epsilon\": agent.epsilon,\n",
    "                \"buffer_size\": len(agent.replay_buffer),\n",
    "            })\n",
    "\n",
    "        # Checkpoint every 10 episodes (same as A2C)\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            agent.global_step.assign(episode + 1)\n",
    "            ckpt_manager.save()\n",
    "            print(f\"  ✓ Checkpoint saved at episode {episode+1}\")\n",
    "\n",
    "    # ── Final checkpoint ────────────────────────────────────────────\n",
    "    agent.global_step.assign(FLAGS.train_episodes)\n",
    "    ckpt_manager.save()\n",
    "\n",
    "    if FLAGS.use_wandb:\n",
    "        import wandb\n",
    "        wandb.finish()\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Training complete! Checkpoints in: {FLAGS.output_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return agent   # return agent for immediate use / evaluation\n",
    "\n",
    "\n",
    "print(\"train_dqn() defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa000011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_dqn() defined ✓\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. PREDICTION / EVALUATION\n",
    "#\n",
    "# Same output format as A2C predict() in training_220.ipynb:\n",
    "#   stock, action, overstock, sales, stockout, capacity  (one line each per timestep)\n",
    "# ============================================================\n",
    "\n",
    "def predict_dqn(checkpoint_dir=None):\n",
    "    \"\"\"\n",
    "    Run the trained DQN on test data and write results to FLAGS.output_file.\n",
    "    Output format is identical to the A2C predict() for fair metric comparison.\n",
    "    \"\"\"\n",
    "    checkpoint_dir = checkpoint_dir or FLAGS.output_dir\n",
    "\n",
    "    # ── Load test-period sales ──────────────────────────────────────\n",
    "    sales_dataset    = tf.data.TFRecordDataset(FLAGS.predict_file).map(sales_parser)\n",
    "    capacity_dataset = tf.data.TFRecordDataset(FLAGS.capacity_file).map(capacity_parser)\n",
    "    stock_dataset    = tf.data.TFRecordDataset(FLAGS.stock_file).map(stock_parser)\n",
    "\n",
    "    capacity = next(iter(capacity_dataset))[\"capacity\"]  # [P]\n",
    "    x        = next(iter(stock_dataset))[\"stock\"]        # [P]  initial stock\n",
    "\n",
    "    # ── Load agent ──────────────────────────────────────────────────\n",
    "    print(\"Initialising agent for prediction...\")\n",
    "    agent    = MultiProductDQNAgent(FLAGS)\n",
    "    checkpoint = tf.train.Checkpoint(\n",
    "        q_network=agent.q_network,\n",
    "        step=agent.global_step,\n",
    "    )\n",
    "    ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        checkpoint.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
    "        print(f\"✓ Loaded: {ckpt_manager.latest_checkpoint}\")\n",
    "    else:\n",
    "        print(\"✗ No checkpoint found. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # ── Predict & write ─────────────────────────────────────────────\n",
    "    print(f\"Writing predictions to {FLAGS.output_file}...\")\n",
    "    with open(FLAGS.output_file, \"w\") as writer:\n",
    "        for rec in sales_dataset:\n",
    "            sales = tf.divide(rec[\"sales\"], capacity)  # normalise by capacity\n",
    "            q     = waste(x.numpy())\n",
    "\n",
    "            state       = np.concatenate([x.numpy(), sales.numpy(), q], axis=0)\n",
    "            action_idx  = agent.select_actions(state, training=False)\n",
    "            actions     = agent.action_space_arr[action_idx]\n",
    "            u           = tf.constant(actions, dtype=tf.float32)\n",
    "\n",
    "            overstock = tf.maximum(0.0, (x + u) - 1.0)\n",
    "            x_u       = tf.minimum(1.0, x + u)\n",
    "            stockout  = tf.minimum(0.0, x_u - sales)\n",
    "\n",
    "            # Same line format as A2C predict()\n",
    "            writer.write(\"stock:\"     + \",\".join(map(str, x.numpy()))                    + \"\\n\")\n",
    "            writer.write(\"action:\"    + \",\".join(map(str, u.numpy()))                    + \"\\n\")\n",
    "            writer.write(\"overstock:\" + \",\".join(map(str, overstock.numpy()))            + \"\\n\")\n",
    "            writer.write(\"sales:\"     + \",\".join(map(str, sales.numpy()))                + \"\\n\")\n",
    "            writer.write(\"stockout:\"  + \",\".join(map(str, stockout.numpy()))             + \"\\n\")\n",
    "            writer.write(\"capacity:\"  + \",\".join(map(str, (capacity / capacity).numpy())) + \"\\n\")\n",
    "\n",
    "            x = tf.maximum(0.0, x_u - sales)\n",
    "\n",
    "    print(f\"✓ Prediction complete — {FLAGS.output_file}\")\n",
    "\n",
    "\n",
    "print(\"predict_dqn() defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa000012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data files:\n",
      "==================================================\n",
      "  ✓  data220/train.tfrecords\n",
      "  ✓  data220/capacity.tfrecords\n",
      "  ✓  data220/stock.tfrecords\n",
      "  ✓  data220/test.tfrecords\n",
      "==================================================\n",
      "✓ All data files present — ready to train!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10. DATA FILE CHECK\n",
    "# ============================================================\n",
    "\n",
    "files = [\n",
    "    FLAGS.train_file, FLAGS.capacity_file,\n",
    "    FLAGS.stock_file, FLAGS.predict_file,\n",
    "]\n",
    "\n",
    "print(\"Checking data files:\")\n",
    "print(\"=\" * 50)\n",
    "all_ok = True\n",
    "for fp in files:\n",
    "    ok = os.path.exists(fp)\n",
    "    print(f\"  {'✓' if ok else '✗'}  {fp}\")\n",
    "    if not ok:\n",
    "        all_ok = False\n",
    "print(\"=\" * 50)\n",
    "if all_ok:\n",
    "    print(\"✓ All data files present — ready to train!\")\n",
    "else:\n",
    "    print(\n",
    "        \"✗ Missing files. Run prepare_data.py first:\\n\"\n",
    "        \"  python prepare_data.py --number_of_products 220 --middle_time_period 900 \\\\\\n\"\n",
    "        \"    --train_tfrecords_file data220/train.tfrecords \\\\\\n\"\n",
    "        \"    --test_tfrecords_file data220/test.tfrecords \\\\\\n\"\n",
    "        \"    --capacity_tfrecords_file data220/capacity.tfrecords \\\\\\n\"\n",
    "        \"    --stock_tfrecords_file data220/stock.tfrecords\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa000013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING DQN TRAINING (A2C fair-comparison mode)\n",
      "============================================================\n",
      "Loading data...\n",
      "Sales loaded: (900, 220)  (timesteps × products)\n",
      "Initialising DQN agent...\n",
      "✓ Restored checkpoint from episode 260\n",
      "============================================================\n",
      "DQN Training — A2C Fair Comparison\n",
      "    Episodes : 600\n",
      "   Timesteps : 900\n",
      "          LR : 0.001\n",
      "      Hidden : 128\n",
      "   GroupNorm : True\n",
      "============================================================\n",
      "Ep  261/600 | R=+0.4661  L=0.0000  SO=0.0053  W=0.0207  O=0.0372  Q=0.4708  ε=0.3540  buf=899\n",
      "Ep  262/600 | R=+0.4719  L=0.0302  SO=0.0054  W=0.0206  O=0.0376  Q=0.4645  ε=0.3516  buf=1798\n",
      "Ep  263/600 | R=+0.4751  L=0.0261  SO=0.0051  W=0.0207  O=0.0377  Q=0.4614  ε=0.3491  buf=2697\n",
      "Ep  264/600 | R=+0.4718  L=0.0263  SO=0.0054  W=0.0207  O=0.0374  Q=0.4647  ε=0.3466  buf=3596\n",
      "Ep  265/600 | R=+0.4746  L=0.0261  SO=0.0054  W=0.0207  O=0.0375  Q=0.4618  ε=0.3441  buf=4495\n",
      "Ep  266/600 | R=+0.4779  L=0.0269  SO=0.0052  W=0.0207  O=0.0380  Q=0.4583  ε=0.3417  buf=5394\n",
      "Ep  267/600 | R=+0.4772  L=0.0274  SO=0.0052  W=0.0207  O=0.0375  Q=0.4594  ε=0.3392  buf=6293\n",
      "Ep  268/600 | R=+0.4791  L=0.0254  SO=0.0052  W=0.0207  O=0.0366  Q=0.4583  ε=0.3367  buf=7192\n",
      "Ep  269/600 | R=+0.4807  L=0.0261  SO=0.0051  W=0.0207  O=0.0370  Q=0.4564  ε=0.3342  buf=8091\n",
      "Ep  270/600 | R=+0.4805  L=0.0246  SO=0.0051  W=0.0208  O=0.0364  Q=0.4572  ε=0.3317  buf=8990\n",
      "  ✓ Checkpoint saved at episode 270\n",
      "Ep  271/600 | R=+0.4821  L=0.0271  SO=0.0052  W=0.0207  O=0.0360  Q=0.4560  ε=0.3293  buf=9889\n",
      "Ep  272/600 | R=+0.4818  L=0.0254  SO=0.0051  W=0.0208  O=0.0367  Q=0.4557  ε=0.3268  buf=10788\n",
      "Ep  273/600 | R=+0.4852  L=0.0249  SO=0.0047  W=0.0208  O=0.0369  Q=0.4523  ε=0.3243  buf=11687\n",
      "Ep  274/600 | R=+0.4877  L=0.0239  SO=0.0049  W=0.0209  O=0.0364  Q=0.4501  ε=0.3218  buf=12586\n",
      "Ep  275/600 | R=+0.4863  L=0.0249  SO=0.0047  W=0.0209  O=0.0363  Q=0.4518  ε=0.3194  buf=13485\n",
      "Ep  276/600 | R=+0.4868  L=0.0275  SO=0.0049  W=0.0209  O=0.0363  Q=0.4511  ε=0.3169  buf=14384\n",
      "Ep  277/600 | R=+0.4894  L=0.0241  SO=0.0046  W=0.0209  O=0.0357  Q=0.4494  ε=0.3144  buf=15283\n",
      "Ep  278/600 | R=+0.4913  L=0.0273  SO=0.0048  W=0.0209  O=0.0351  Q=0.4479  ε=0.3119  buf=16182\n",
      "Ep  279/600 | R=+0.4922  L=0.0248  SO=0.0046  W=0.0209  O=0.0355  Q=0.4468  ε=0.3095  buf=17081\n",
      "Ep  280/600 | R=+0.4955  L=0.0265  SO=0.0046  W=0.0209  O=0.0342  Q=0.4448  ε=0.3070  buf=17980\n",
      "  ✓ Checkpoint saved at episode 280\n",
      "Ep  281/600 | R=+0.4970  L=0.0269  SO=0.0047  W=0.0208  O=0.0328  Q=0.4447  ε=0.3045  buf=18879\n",
      "Ep  282/600 | R=+0.4965  L=0.0282  SO=0.0044  W=0.0209  O=0.0337  Q=0.4446  ε=0.3020  buf=19778\n",
      "Ep  283/600 | R=+0.4984  L=0.0284  SO=0.0046  W=0.0209  O=0.0332  Q=0.4430  ε=0.2996  buf=20677\n",
      "Ep  284/600 | R=+0.4995  L=0.0279  SO=0.0046  W=0.0209  O=0.0338  Q=0.4412  ε=0.2971  buf=21576\n",
      "Ep  285/600 | R=+0.5020  L=0.0277  SO=0.0043  W=0.0209  O=0.0331  Q=0.4396  ε=0.2946  buf=22475\n",
      "Ep  286/600 | R=+0.5020  L=0.0274  SO=0.0044  W=0.0209  O=0.0332  Q=0.4395  ε=0.2921  buf=23374\n",
      "Ep  287/600 | R=+0.5027  L=0.0259  SO=0.0044  W=0.0209  O=0.0331  Q=0.4388  ε=0.2897  buf=24273\n",
      "Ep  288/600 | R=+0.5026  L=0.0276  SO=0.0042  W=0.0209  O=0.0329  Q=0.4393  ε=0.2872  buf=25172\n",
      "Ep  289/600 | R=+0.5037  L=0.0257  SO=0.0043  W=0.0209  O=0.0320  Q=0.4391  ε=0.2847  buf=26071\n",
      "Ep  290/600 | R=+0.5053  L=0.0278  SO=0.0042  W=0.0210  O=0.0328  Q=0.4368  ε=0.2822  buf=26970\n",
      "  ✓ Checkpoint saved at episode 290\n",
      "Ep  291/600 | R=+0.5095  L=0.0309  SO=0.0045  W=0.0209  O=0.0326  Q=0.4325  ε=0.2798  buf=27869\n",
      "Ep  292/600 | R=+0.5100  L=0.0285  SO=0.0043  W=0.0210  O=0.0324  Q=0.4323  ε=0.2773  buf=28768\n",
      "Ep  293/600 | R=+0.5111  L=0.0289  SO=0.0041  W=0.0210  O=0.0335  Q=0.4303  ε=0.2748  buf=29667\n",
      "Ep  294/600 | R=+0.5109  L=0.0276  SO=0.0041  W=0.0211  O=0.0325  Q=0.4314  ε=0.2723  buf=30566\n",
      "Ep  295/600 | R=+0.5129  L=0.0250  SO=0.0039  W=0.0211  O=0.0328  Q=0.4293  ε=0.2699  buf=31465\n",
      "Ep  296/600 | R=+0.5123  L=0.0272  SO=0.0039  W=0.0211  O=0.0324  Q=0.4303  ε=0.2674  buf=32364\n",
      "Ep  297/600 | R=+0.5141  L=0.0268  SO=0.0042  W=0.0211  O=0.0319  Q=0.4287  ε=0.2649  buf=33263\n",
      "Ep  298/600 | R=+0.5146  L=0.0261  SO=0.0042  W=0.0211  O=0.0319  Q=0.4282  ε=0.2624  buf=34162\n",
      "Ep  299/600 | R=+0.5159  L=0.0279  SO=0.0041  W=0.0211  O=0.0319  Q=0.4271  ε=0.2600  buf=35061\n",
      "Ep  300/600 | R=+0.5186  L=0.0264  SO=0.0040  W=0.0211  O=0.0313  Q=0.4251  ε=0.2575  buf=35960\n",
      "  ✓ Checkpoint saved at episode 300\n",
      "Ep  301/600 | R=+0.5193  L=0.0299  SO=0.0043  W=0.0210  O=0.0318  Q=0.4236  ε=0.2550  buf=36859\n",
      "Ep  302/600 | R=+0.5197  L=0.0283  SO=0.0037  W=0.0211  O=0.0318  Q=0.4237  ε=0.2525  buf=37758\n",
      "Ep  303/600 | R=+0.5261  L=0.0321  SO=0.0038  W=0.0212  O=0.0316  Q=0.4173  ε=0.2501  buf=38657\n",
      "Ep  304/600 | R=+0.5213  L=0.0280  SO=0.0038  W=0.0212  O=0.0318  Q=0.4219  ε=0.2476  buf=39556\n",
      "Ep  305/600 | R=+0.5224  L=0.0271  SO=0.0039  W=0.0212  O=0.0312  Q=0.4214  ε=0.2451  buf=40455\n",
      "Ep  306/600 | R=+0.5240  L=0.0297  SO=0.0038  W=0.0212  O=0.0310  Q=0.4199  ε=0.2426  buf=41354\n",
      "Ep  307/600 | R=+0.5234  L=0.0284  SO=0.0041  W=0.0212  O=0.0311  Q=0.4203  ε=0.2402  buf=42253\n",
      "Ep  308/600 | R=+0.5238  L=0.0272  SO=0.0037  W=0.0212  O=0.0315  Q=0.4197  ε=0.2377  buf=43152\n",
      "Ep  309/600 | R=+0.5257  L=0.0294  SO=0.0038  W=0.0212  O=0.0308  Q=0.4186  ε=0.2352  buf=44051\n",
      "Ep  310/600 | R=+0.5260  L=0.0298  SO=0.0036  W=0.0212  O=0.0300  Q=0.4192  ε=0.2327  buf=44950\n",
      "  ✓ Checkpoint saved at episode 310\n",
      "Ep  311/600 | R=+0.5271  L=0.0319  SO=0.0038  W=0.0211  O=0.0285  Q=0.4195  ε=0.2303  buf=45849\n",
      "Ep  312/600 | R=+0.5285  L=0.0268  SO=0.0039  W=0.0212  O=0.0292  Q=0.4172  ε=0.2278  buf=46748\n",
      "Ep  313/600 | R=+0.5287  L=0.0270  SO=0.0037  W=0.0212  O=0.0293  Q=0.4171  ε=0.2253  buf=47647\n",
      "Ep  314/600 | R=+0.5282  L=0.0272  SO=0.0038  W=0.0212  O=0.0291  Q=0.4177  ε=0.2228  buf=48546\n",
      "Ep  315/600 | R=+0.5299  L=0.0300  SO=0.0037  W=0.0212  O=0.0291  Q=0.4161  ε=0.2204  buf=49445\n",
      "Ep  316/600 | R=+0.5303  L=0.0286  SO=0.0037  W=0.0212  O=0.0288  Q=0.4161  ε=0.2179  buf=50344\n",
      "Ep  317/600 | R=+0.5301  L=0.0304  SO=0.0037  W=0.0212  O=0.0284  Q=0.4166  ε=0.2154  buf=51243\n",
      "Ep  318/600 | R=+0.5345  L=0.0305  SO=0.0035  W=0.0212  O=0.0284  Q=0.4124  ε=0.2129  buf=52142\n",
      "Ep  319/600 | R=+0.5345  L=0.0292  SO=0.0034  W=0.0212  O=0.0284  Q=0.4125  ε=0.2105  buf=53041\n",
      "Ep  320/600 | R=+0.5357  L=0.0263  SO=0.0035  W=0.0213  O=0.0279  Q=0.4117  ε=0.2080  buf=53940\n",
      "  ✓ Checkpoint saved at episode 320\n",
      "Ep  321/600 | R=+0.5393  L=0.0331  SO=0.0036  W=0.0212  O=0.0267  Q=0.4093  ε=0.2055  buf=54839\n",
      "Ep  322/600 | R=+0.5408  L=0.0304  SO=0.0036  W=0.0213  O=0.0277  Q=0.4067  ε=0.2030  buf=55738\n",
      "Ep  323/600 | R=+0.5423  L=0.0304  SO=0.0035  W=0.0213  O=0.0270  Q=0.4060  ε=0.2006  buf=56637\n",
      "Ep  324/600 | R=+0.5419  L=0.0338  SO=0.0034  W=0.0213  O=0.0269  Q=0.4066  ε=0.1981  buf=57536\n",
      "Ep  325/600 | R=+0.5411  L=0.0309  SO=0.0034  W=0.0213  O=0.0263  Q=0.4079  ε=0.1956  buf=58435\n",
      "Ep  326/600 | R=+0.5431  L=0.0304  SO=0.0034  W=0.0213  O=0.0259  Q=0.4063  ε=0.1931  buf=59334\n",
      "Ep  327/600 | R=+0.5449  L=0.0260  SO=0.0032  W=0.0213  O=0.0262  Q=0.4044  ε=0.1907  buf=60233\n",
      "Ep  328/600 | R=+0.5457  L=0.0292  SO=0.0032  W=0.0213  O=0.0253  Q=0.4046  ε=0.1882  buf=61132\n",
      "Ep  329/600 | R=+0.5461  L=0.0287  SO=0.0033  W=0.0213  O=0.0256  Q=0.4037  ε=0.1857  buf=62031\n",
      "Ep  330/600 | R=+0.5484  L=0.0302  SO=0.0033  W=0.0213  O=0.0250  Q=0.4020  ε=0.1832  buf=62930\n",
      "  ✓ Checkpoint saved at episode 330\n",
      "Ep  331/600 | R=+0.5486  L=0.0295  SO=0.0035  W=0.0213  O=0.0248  Q=0.4018  ε=0.1808  buf=63829\n",
      "Ep  332/600 | R=+0.5507  L=0.0308  SO=0.0032  W=0.0213  O=0.0253  Q=0.3994  ε=0.1783  buf=64728\n",
      "Ep  333/600 | R=+0.5490  L=0.0318  SO=0.0032  W=0.0213  O=0.0255  Q=0.4009  ε=0.1758  buf=65627\n",
      "Ep  334/600 | R=+0.5509  L=0.0322  SO=0.0032  W=0.0213  O=0.0246  Q=0.3999  ε=0.1733  buf=66526\n",
      "Ep  335/600 | R=+0.5535  L=0.0315  SO=0.0032  W=0.0213  O=0.0243  Q=0.3976  ε=0.1709  buf=67425\n",
      "Ep  336/600 | R=+0.5530  L=0.0307  SO=0.0031  W=0.0214  O=0.0247  Q=0.3978  ε=0.1684  buf=68324\n",
      "Ep  337/600 | R=+0.5518  L=0.0302  SO=0.0030  W=0.0213  O=0.0244  Q=0.3994  ε=0.1659  buf=69223\n",
      "Ep  338/600 | R=+0.5555  L=0.0290  SO=0.0031  W=0.0214  O=0.0237  Q=0.3963  ε=0.1634  buf=70122\n",
      "Ep  339/600 | R=+0.5551  L=0.0312  SO=0.0032  W=0.0214  O=0.0232  Q=0.3971  ε=0.1610  buf=71021\n",
      "Ep  340/600 | R=+0.5567  L=0.0284  SO=0.0030  W=0.0214  O=0.0231  Q=0.3958  ε=0.1585  buf=71920\n",
      "  ✓ Checkpoint saved at episode 340\n",
      "Ep  341/600 | R=+0.5564  L=0.0303  SO=0.0031  W=0.0213  O=0.0227  Q=0.3965  ε=0.1560  buf=72819\n",
      "Ep  342/600 | R=+0.5562  L=0.0322  SO=0.0030  W=0.0214  O=0.0228  Q=0.3967  ε=0.1535  buf=73718\n",
      "Ep  343/600 | R=+0.5571  L=0.0317  SO=0.0030  W=0.0213  O=0.0219  Q=0.3966  ε=0.1511  buf=74617\n",
      "Ep  344/600 | R=+0.5586  L=0.0323  SO=0.0030  W=0.0214  O=0.0217  Q=0.3954  ε=0.1486  buf=75516\n",
      "Ep  345/600 | R=+0.5587  L=0.0307  SO=0.0030  W=0.0214  O=0.0219  Q=0.3951  ε=0.1461  buf=76415\n",
      "Ep  346/600 | R=+0.5606  L=0.0296  SO=0.0030  W=0.0214  O=0.0211  Q=0.3939  ε=0.1436  buf=77314\n",
      "Ep  347/600 | R=+0.5593  L=0.0323  SO=0.0030  W=0.0214  O=0.0213  Q=0.3951  ε=0.1412  buf=78213\n",
      "Ep  348/600 | R=+0.5620  L=0.0294  SO=0.0029  W=0.0214  O=0.0214  Q=0.3923  ε=0.1387  buf=79112\n",
      "Ep  349/600 | R=+0.5634  L=0.0306  SO=0.0027  W=0.0214  O=0.0208  Q=0.3917  ε=0.1362  buf=80011\n",
      "Ep  350/600 | R=+0.5642  L=0.0307  SO=0.0027  W=0.0214  O=0.0205  Q=0.3912  ε=0.1337  buf=80910\n",
      "  ✓ Checkpoint saved at episode 350\n",
      "Ep  351/600 | R=+0.5715  L=0.0324  SO=0.0029  W=0.0214  O=0.0206  Q=0.3836  ε=0.1313  buf=81809\n",
      "Ep  352/600 | R=+0.5719  L=0.0301  SO=0.0028  W=0.0215  O=0.0206  Q=0.3832  ε=0.1288  buf=82708\n",
      "Ep  353/600 | R=+0.5714  L=0.0307  SO=0.0028  W=0.0215  O=0.0206  Q=0.3837  ε=0.1263  buf=83607\n",
      "Ep  354/600 | R=+0.5757  L=0.0301  SO=0.0026  W=0.0215  O=0.0203  Q=0.3799  ε=0.1238  buf=84506\n",
      "Ep  355/600 | R=+0.5761  L=0.0301  SO=0.0027  W=0.0215  O=0.0202  Q=0.3796  ε=0.1214  buf=85405\n",
      "Ep  356/600 | R=+0.5768  L=0.0274  SO=0.0026  W=0.0215  O=0.0195  Q=0.3796  ε=0.1189  buf=86304\n",
      "Ep  357/600 | R=+0.5772  L=0.0297  SO=0.0026  W=0.0215  O=0.0197  Q=0.3789  ε=0.1164  buf=87203\n",
      "Ep  358/600 | R=+0.5775  L=0.0304  SO=0.0027  W=0.0215  O=0.0194  Q=0.3790  ε=0.1139  buf=88102\n",
      "Ep  359/600 | R=+0.5789  L=0.0302  SO=0.0027  W=0.0215  O=0.0191  Q=0.3779  ε=0.1115  buf=89001\n",
      "Ep  360/600 | R=+0.5797  L=0.0290  SO=0.0027  W=0.0215  O=0.0189  Q=0.3772  ε=0.1090  buf=89900\n",
      "  ✓ Checkpoint saved at episode 360\n",
      "Ep  361/600 | R=+0.5791  L=0.0285  SO=0.0028  W=0.0215  O=0.0188  Q=0.3778  ε=0.1065  buf=90799\n",
      "Ep  362/600 | R=+0.5822  L=0.0305  SO=0.0026  W=0.0215  O=0.0183  Q=0.3754  ε=0.1040  buf=91698\n",
      "Ep  363/600 | R=+0.5812  L=0.0300  SO=0.0025  W=0.0215  O=0.0182  Q=0.3765  ε=0.1016  buf=92597\n",
      "Ep  364/600 | R=+0.5819  L=0.0269  SO=0.0027  W=0.0215  O=0.0181  Q=0.3758  ε=0.0991  buf=93496\n",
      "Ep  365/600 | R=+0.5845  L=0.0299  SO=0.0025  W=0.0215  O=0.0174  Q=0.3740  ε=0.0966  buf=94395\n",
      "Ep  366/600 | R=+0.5846  L=0.0289  SO=0.0024  W=0.0215  O=0.0177  Q=0.3738  ε=0.0941  buf=95294\n",
      "Ep  367/600 | R=+0.5848  L=0.0325  SO=0.0025  W=0.0215  O=0.0177  Q=0.3734  ε=0.0917  buf=96193\n",
      "Ep  368/600 | R=+0.5858  L=0.0257  SO=0.0025  W=0.0215  O=0.0173  Q=0.3729  ε=0.0892  buf=97092\n",
      "Ep  369/600 | R=+0.5890  L=0.0336  SO=0.0024  W=0.0215  O=0.0169  Q=0.3701  ε=0.0867  buf=97991\n",
      "Ep  370/600 | R=+0.5881  L=0.0282  SO=0.0024  W=0.0216  O=0.0166  Q=0.3713  ε=0.0842  buf=98890\n",
      "  ✓ Checkpoint saved at episode 370\n",
      "Ep  371/600 | R=+0.5866  L=0.0322  SO=0.0025  W=0.0215  O=0.0164  Q=0.3730  ε=0.0818  buf=99789\n",
      "Ep  372/600 | R=+0.5879  L=0.0325  SO=0.0024  W=0.0216  O=0.0169  Q=0.3713  ε=0.0793  buf=100000\n",
      "Ep  373/600 | R=+0.5885  L=0.0320  SO=0.0024  W=0.0216  O=0.0159  Q=0.3717  ε=0.0768  buf=100000\n",
      "Ep  374/600 | R=+0.5899  L=0.0297  SO=0.0024  W=0.0216  O=0.0164  Q=0.3699  ε=0.0743  buf=100000\n",
      "Ep  375/600 | R=+0.5908  L=0.0295  SO=0.0024  W=0.0216  O=0.0157  Q=0.3695  ε=0.0719  buf=100000\n",
      "Ep  376/600 | R=+0.5910  L=0.0330  SO=0.0023  W=0.0216  O=0.0160  Q=0.3691  ε=0.0694  buf=100000\n",
      "Ep  377/600 | R=+0.5916  L=0.0300  SO=0.0023  W=0.0216  O=0.0153  Q=0.3692  ε=0.0669  buf=100000\n",
      "Ep  378/600 | R=+0.5941  L=0.0277  SO=0.0023  W=0.0216  O=0.0153  Q=0.3667  ε=0.0644  buf=100000\n",
      "Ep  379/600 | R=+0.5925  L=0.0300  SO=0.0023  W=0.0216  O=0.0149  Q=0.3687  ε=0.0620  buf=100000\n",
      "Ep  380/600 | R=+0.5956  L=0.0293  SO=0.0023  W=0.0216  O=0.0147  Q=0.3658  ε=0.0595  buf=100000\n",
      "  ✓ Checkpoint saved at episode 380\n",
      "Ep  381/600 | R=+0.5959  L=0.0339  SO=0.0025  W=0.0215  O=0.0137  Q=0.3664  ε=0.0570  buf=100000\n",
      "Ep  382/600 | R=+0.5995  L=0.0322  SO=0.0021  W=0.0216  O=0.0138  Q=0.3630  ε=0.0545  buf=100000\n",
      "Ep  383/600 | R=+0.5976  L=0.0327  SO=0.0022  W=0.0216  O=0.0134  Q=0.3652  ε=0.0521  buf=100000\n",
      "Ep  384/600 | R=+0.5980  L=0.0323  SO=0.0022  W=0.0216  O=0.0134  Q=0.3648  ε=0.0496  buf=100000\n",
      "Ep  385/600 | R=+0.5990  L=0.0333  SO=0.0023  W=0.0216  O=0.0128  Q=0.3643  ε=0.0471  buf=100000\n",
      "Ep  386/600 | R=+0.6001  L=0.0314  SO=0.0021  W=0.0216  O=0.0130  Q=0.3632  ε=0.0446  buf=100000\n",
      "Ep  387/600 | R=+0.6017  L=0.0301  SO=0.0021  W=0.0216  O=0.0128  Q=0.3617  ε=0.0422  buf=100000\n",
      "Ep  388/600 | R=+0.6012  L=0.0316  SO=0.0021  W=0.0216  O=0.0120  Q=0.3631  ε=0.0397  buf=100000\n",
      "Ep  389/600 | R=+0.6022  L=0.0261  SO=0.0022  W=0.0216  O=0.0117  Q=0.3624  ε=0.0372  buf=100000\n",
      "Ep  390/600 | R=+0.6040  L=0.0348  SO=0.0021  W=0.0216  O=0.0114  Q=0.3609  ε=0.0347  buf=100000\n",
      "  ✓ Checkpoint saved at episode 390\n",
      "Ep  391/600 | R=+0.6090  L=0.0331  SO=0.0022  W=0.0216  O=0.0104  Q=0.3569  ε=0.0323  buf=100000\n",
      "Ep  392/600 | R=+0.6098  L=0.0344  SO=0.0021  W=0.0216  O=0.0104  Q=0.3561  ε=0.0298  buf=100000\n",
      "Ep  393/600 | R=+0.6114  L=0.0290  SO=0.0021  W=0.0216  O=0.0101  Q=0.3548  ε=0.0273  buf=100000\n",
      "Ep  394/600 | R=+0.6109  L=0.0293  SO=0.0021  W=0.0216  O=0.0096  Q=0.3558  ε=0.0248  buf=100000\n",
      "Ep  395/600 | R=+0.6127  L=0.0320  SO=0.0021  W=0.0216  O=0.0093  Q=0.3543  ε=0.0224  buf=100000\n",
      "Ep  396/600 | R=+0.6138  L=0.0303  SO=0.0020  W=0.0216  O=0.0089  Q=0.3536  ε=0.0199  buf=100000\n",
      "Ep  397/600 | R=+0.6156  L=0.0324  SO=0.0020  W=0.0216  O=0.0088  Q=0.3520  ε=0.0174  buf=100000\n",
      "Ep  398/600 | R=+0.6155  L=0.0315  SO=0.0020  W=0.0216  O=0.0083  Q=0.3526  ε=0.0149  buf=100000\n",
      "Ep  399/600 | R=+0.6155  L=0.0291  SO=0.0020  W=0.0216  O=0.0079  Q=0.3530  ε=0.0125  buf=100000\n",
      "Ep  400/600 | R=+0.6169  L=0.0319  SO=0.0020  W=0.0216  O=0.0075  Q=0.3519  ε=0.0100  buf=100000\n",
      "  ✓ Checkpoint saved at episode 400\n",
      "Ep  401/600 | R=+0.6156  L=0.0289  SO=0.0021  W=0.0215  O=0.0066  Q=0.3542  ε=0.0100  buf=100000\n",
      "Ep  402/600 | R=+0.6171  L=0.0341  SO=0.0020  W=0.0215  O=0.0064  Q=0.3531  ε=0.0100  buf=100000\n",
      "Ep  403/600 | R=+0.6186  L=0.0300  SO=0.0019  W=0.0216  O=0.0068  Q=0.3511  ε=0.0100  buf=100000\n",
      "Ep  404/600 | R=+0.6176  L=0.0341  SO=0.0020  W=0.0215  O=0.0063  Q=0.3526  ε=0.0100  buf=100000\n",
      "Ep  405/600 | R=+0.6162  L=0.0373  SO=0.0019  W=0.0216  O=0.0067  Q=0.3536  ε=0.0100  buf=100000\n",
      "Ep  406/600 | R=+0.6179  L=0.0315  SO=0.0019  W=0.0216  O=0.0064  Q=0.3522  ε=0.0100  buf=100000\n",
      "Ep  407/600 | R=+0.6155  L=0.0323  SO=0.0020  W=0.0215  O=0.0064  Q=0.3547  ε=0.0100  buf=100000\n",
      "Ep  408/600 | R=+0.6169  L=0.0293  SO=0.0019  W=0.0216  O=0.0063  Q=0.3533  ε=0.0100  buf=100000\n",
      "Ep  409/600 | R=+0.6183  L=0.0351  SO=0.0019  W=0.0216  O=0.0063  Q=0.3519  ε=0.0100  buf=100000\n",
      "Ep  410/600 | R=+0.6166  L=0.0304  SO=0.0020  W=0.0215  O=0.0060  Q=0.3539  ε=0.0100  buf=100000\n",
      "  ✓ Checkpoint saved at episode 410\n",
      "Ep  411/600 | R=+0.6140  L=0.0375  SO=0.0021  W=0.0214  O=0.0062  Q=0.3563  ε=0.0100  buf=100000\n",
      "Ep  412/600 | R=+0.6186  L=0.0307  SO=0.0019  W=0.0215  O=0.0065  Q=0.3514  ε=0.0100  buf=100000\n",
      "Ep  413/600 | R=+0.6203  L=0.0304  SO=0.0020  W=0.0216  O=0.0064  Q=0.3497  ε=0.0100  buf=100000\n",
      "Ep  414/600 | R=+0.6195  L=0.0289  SO=0.0020  W=0.0216  O=0.0063  Q=0.3507  ε=0.0100  buf=100000\n",
      "Ep  415/600 | R=+0.6214  L=0.0317  SO=0.0019  W=0.0216  O=0.0062  Q=0.3488  ε=0.0100  buf=100000\n",
      "Ep  416/600 | R=+0.6206  L=0.0341  SO=0.0020  W=0.0216  O=0.0062  Q=0.3497  ε=0.0100  buf=100000\n",
      "Ep  417/600 | R=+0.6205  L=0.0291  SO=0.0019  W=0.0216  O=0.0062  Q=0.3498  ε=0.0100  buf=100000\n",
      "Ep  418/600 | R=+0.6220  L=0.0309  SO=0.0021  W=0.0216  O=0.0065  Q=0.3478  ε=0.0100  buf=100000\n",
      "Ep  419/600 | R=+0.6226  L=0.0292  SO=0.0019  W=0.0216  O=0.0062  Q=0.3477  ε=0.0100  buf=100000\n",
      "Ep  420/600 | R=+0.6213  L=0.0307  SO=0.0019  W=0.0216  O=0.0064  Q=0.3488  ε=0.0100  buf=100000\n",
      "  ✓ Checkpoint saved at episode 420\n",
      "Ep  421/600 | R=+0.6165  L=0.0356  SO=0.0021  W=0.0214  O=0.0067  Q=0.3532  ε=0.0100  buf=100000\n",
      "Ep  422/600 | R=+0.6185  L=0.0309  SO=0.0020  W=0.0216  O=0.0067  Q=0.3512  ε=0.0100  buf=100000\n",
      "Ep  423/600 | R=+0.6180  L=0.0366  SO=0.0020  W=0.0216  O=0.0068  Q=0.3516  ε=0.0100  buf=100000\n",
      "Ep  424/600 | R=+0.6186  L=0.0361  SO=0.0020  W=0.0216  O=0.0067  Q=0.3512  ε=0.0100  buf=100000\n",
      "Ep  425/600 | R=+0.6182  L=0.0374  SO=0.0020  W=0.0216  O=0.0069  Q=0.3513  ε=0.0100  buf=100000\n",
      "Ep  426/600 | R=+0.6195  L=0.0353  SO=0.0019  W=0.0216  O=0.0071  Q=0.3498  ε=0.0100  buf=100000\n",
      "Ep  427/600 | R=+0.6196  L=0.0334  SO=0.0019  W=0.0216  O=0.0075  Q=0.3494  ε=0.0100  buf=100000\n",
      "Ep  428/600 | R=+0.6190  L=0.0334  SO=0.0019  W=0.0216  O=0.0071  Q=0.3503  ε=0.0100  buf=100000\n",
      "Ep  429/600 | R=+0.6199  L=0.0321  SO=0.0019  W=0.0216  O=0.0072  Q=0.3494  ε=0.0100  buf=100000\n",
      "Ep  430/600 | R=+0.6201  L=0.0343  SO=0.0020  W=0.0216  O=0.0075  Q=0.3488  ε=0.0100  buf=100000\n",
      "  ✓ Checkpoint saved at episode 430\n",
      "Ep  431/600 | R=+0.6136  L=0.0373  SO=0.0021  W=0.0215  O=0.0069  Q=0.3560  ε=0.0100  buf=100000\n",
      "Ep  432/600 | R=+0.6166  L=0.0400  SO=0.0020  W=0.0216  O=0.0070  Q=0.3528  ε=0.0100  buf=100000\n",
      "Ep  433/600 | R=+0.6165  L=0.0348  SO=0.0020  W=0.0216  O=0.0071  Q=0.3528  ε=0.0100  buf=100000\n",
      "Ep  434/600 | R=+0.6169  L=0.0315  SO=0.0020  W=0.0216  O=0.0075  Q=0.3520  ε=0.0100  buf=100000\n",
      "Ep  435/600 | R=+0.6157  L=0.0342  SO=0.0019  W=0.0216  O=0.0070  Q=0.3538  ε=0.0100  buf=100000\n",
      "Ep  436/600 | R=+0.6153  L=0.0404  SO=0.0020  W=0.0216  O=0.0077  Q=0.3534  ε=0.0100  buf=100000\n",
      "Ep  437/600 | R=+0.6146  L=0.0381  SO=0.0019  W=0.0216  O=0.0079  Q=0.3539  ε=0.0100  buf=100000\n",
      "Ep  438/600 | R=+0.6146  L=0.0313  SO=0.0019  W=0.0216  O=0.0081  Q=0.3538  ε=0.0100  buf=100000\n",
      "Ep  439/600 | R=+0.6158  L=0.0336  SO=0.0020  W=0.0216  O=0.0078  Q=0.3528  ε=0.0100  buf=100000\n",
      "Ep  440/600 | R=+0.6162  L=0.0343  SO=0.0019  W=0.0216  O=0.0074  Q=0.3528  ε=0.0100  buf=100000\n",
      "  ✓ Checkpoint saved at episode 440\n",
      "Ep  441/600 | R=+0.6119  L=0.0342  SO=0.0020  W=0.0215  O=0.0077  Q=0.3568  ε=0.0100  buf=100000\n",
      "Ep  442/600 | R=+0.6145  L=0.0359  SO=0.0020  W=0.0216  O=0.0078  Q=0.3541  ε=0.0100  buf=100000\n",
      "Ep  443/600 | R=+0.6128  L=0.0384  SO=0.0021  W=0.0216  O=0.0070  Q=0.3565  ε=0.0100  buf=100000\n",
      "Ep  444/600 | R=+0.6152  L=0.0344  SO=0.0019  W=0.0216  O=0.0080  Q=0.3532  ε=0.0100  buf=100000\n",
      "Ep  445/600 | R=+0.6138  L=0.0278  SO=0.0019  W=0.0216  O=0.0072  Q=0.3554  ε=0.0100  buf=100000\n",
      "Ep  446/600 | R=+0.6129  L=0.0347  SO=0.0019  W=0.0216  O=0.0075  Q=0.3562  ε=0.0100  buf=100000\n",
      "Ep  447/600 | R=+0.6143  L=0.0327  SO=0.0020  W=0.0216  O=0.0071  Q=0.3550  ε=0.0100  buf=100000\n",
      "Ep  448/600 | R=+0.6132  L=0.0311  SO=0.0020  W=0.0216  O=0.0069  Q=0.3563  ε=0.0100  buf=100000\n",
      "Ep  449/600 | R=+0.6135  L=0.0295  SO=0.0020  W=0.0216  O=0.0072  Q=0.3556  ε=0.0100  buf=100000\n",
      "Ep  450/600 | R=+0.6137  L=0.0300  SO=0.0020  W=0.0216  O=0.0079  Q=0.3548  ε=0.0100  buf=100000\n",
      "  ✓ Checkpoint saved at episode 450\n",
      "Ep  451/600 | R=+0.6118  L=0.0345  SO=0.0022  W=0.0216  O=0.0081  Q=0.3563  ε=0.0100  buf=100000\n",
      "Ep  452/600 | R=+0.6173  L=0.0309  SO=0.0020  W=0.0217  O=0.0080  Q=0.3509  ε=0.0100  buf=100000\n",
      "Ep  453/600 | R=+0.6168  L=0.0379  SO=0.0019  W=0.0217  O=0.0084  Q=0.3512  ε=0.0100  buf=100000\n",
      "Ep  454/600 | R=+0.6147  L=0.0313  SO=0.0019  W=0.0216  O=0.0083  Q=0.3534  ε=0.0100  buf=100000\n",
      "Ep  455/600 | R=+0.6159  L=0.0316  SO=0.0020  W=0.0216  O=0.0076  Q=0.3529  ε=0.0100  buf=100000\n",
      "Ep  456/600 | R=+0.6155  L=0.0316  SO=0.0019  W=0.0216  O=0.0086  Q=0.3523  ε=0.0100  buf=100000\n",
      "Ep  457/600 | R=+0.6162  L=0.0318  SO=0.0020  W=0.0216  O=0.0081  Q=0.3521  ε=0.0100  buf=100000\n",
      "Ep  458/600 | R=+0.6166  L=0.0344  SO=0.0019  W=0.0217  O=0.0087  Q=0.3511  ε=0.0100  buf=100000\n",
      "Ep  459/600 | R=+0.6151  L=0.0308  SO=0.0020  W=0.0217  O=0.0087  Q=0.3526  ε=0.0100  buf=100000\n",
      "Ep  460/600 | R=+0.6152  L=0.0335  SO=0.0020  W=0.0217  O=0.0083  Q=0.3529  ε=0.0100  buf=100000\n",
      "  ✓ Checkpoint saved at episode 460\n",
      "Ep  461/600 | R=+0.6169  L=0.0316  SO=0.0022  W=0.0216  O=0.0089  Q=0.3504  ε=0.0100  buf=100000\n",
      "Ep  462/600 | R=+0.6190  L=0.0346  SO=0.0020  W=0.0217  O=0.0092  Q=0.3481  ε=0.0100  buf=100000\n",
      "Ep  463/600 | R=+0.6197  L=0.0358  SO=0.0020  W=0.0217  O=0.0090  Q=0.3475  ε=0.0100  buf=100000\n",
      "Ep  464/600 | R=+0.6187  L=0.0347  SO=0.0020  W=0.0217  O=0.0094  Q=0.3482  ε=0.0100  buf=100000\n",
      "Ep  465/600 | R=+0.6184  L=0.0338  SO=0.0020  W=0.0217  O=0.0093  Q=0.3486  ε=0.0100  buf=100000\n",
      "Ep  466/600 | R=+0.6181  L=0.0343  SO=0.0020  W=0.0217  O=0.0094  Q=0.3488  ε=0.0100  buf=100000\n",
      "Ep  467/600 | R=+0.6188  L=0.0343  SO=0.0020  W=0.0217  O=0.0094  Q=0.3480  ε=0.0100  buf=100000\n",
      "Ep  468/600 | R=+0.6186  L=0.0390  SO=0.0019  W=0.0217  O=0.0095  Q=0.3482  ε=0.0100  buf=100000\n",
      "Ep  469/600 | R=+0.6190  L=0.0342  SO=0.0019  W=0.0217  O=0.0098  Q=0.3476  ε=0.0100  buf=100000\n",
      "Ep  470/600 | R=+0.6188  L=0.0332  SO=0.0019  W=0.0217  O=0.0089  Q=0.3487  ε=0.0100  buf=100000\n",
      "  ✓ Checkpoint saved at episode 470\n",
      "Ep  471/600 | R=+0.6137  L=0.0407  SO=0.0021  W=0.0215  O=0.0089  Q=0.3538  ε=0.0100  buf=100000\n",
      "Ep  472/600 | R=+0.6181  L=0.0343  SO=0.0020  W=0.0217  O=0.0079  Q=0.3503  ε=0.0100  buf=100000\n",
      "Ep  473/600 | R=+0.6169  L=0.0335  SO=0.0020  W=0.0217  O=0.0096  Q=0.3498  ε=0.0100  buf=100000\n",
      "Ep  474/600 | R=+0.6167  L=0.0341  SO=0.0019  W=0.0217  O=0.0087  Q=0.3511  ε=0.0100  buf=100000\n",
      "Ep  475/600 | R=+0.6170  L=0.0334  SO=0.0020  W=0.0217  O=0.0089  Q=0.3505  ε=0.0100  buf=100000\n",
      "Ep  476/600 | R=+0.6175  L=0.0330  SO=0.0020  W=0.0217  O=0.0084  Q=0.3504  ε=0.0100  buf=100000\n",
      "Ep  477/600 | R=+0.6172  L=0.0347  SO=0.0019  W=0.0217  O=0.0081  Q=0.3511  ε=0.0100  buf=100000\n",
      "Ep  478/600 | R=+0.6178  L=0.0334  SO=0.0020  W=0.0217  O=0.0084  Q=0.3501  ε=0.0100  buf=100000\n",
      "Ep  479/600 | R=+0.6165  L=0.0314  SO=0.0020  W=0.0217  O=0.0083  Q=0.3515  ε=0.0100  buf=100000\n",
      "Ep  480/600 | R=+0.6156  L=0.0365  SO=0.0020  W=0.0217  O=0.0084  Q=0.3524  ε=0.0100  buf=100000\n",
      "  ✓ Checkpoint saved at episode 480\n",
      "Ep  481/600 | R=+0.6093  L=0.0401  SO=0.0022  W=0.0215  O=0.0083  Q=0.3588  ε=0.0100  buf=100000\n",
      "Ep  482/600 | R=+0.6157  L=0.0402  SO=0.0019  W=0.0217  O=0.0082  Q=0.3525  ε=0.0100  buf=100000\n",
      "Ep  483/600 | R=+0.6153  L=0.0335  SO=0.0020  W=0.0217  O=0.0088  Q=0.3523  ε=0.0100  buf=100000\n",
      "Ep  484/600 | R=+0.6161  L=0.0336  SO=0.0019  W=0.0217  O=0.0081  Q=0.3523  ε=0.0100  buf=100000\n",
      "Ep  485/600 | R=+0.6167  L=0.0355  SO=0.0020  W=0.0217  O=0.0081  Q=0.3516  ε=0.0100  buf=100000\n",
      "Ep  486/600 | R=+0.6168  L=0.0310  SO=0.0020  W=0.0217  O=0.0082  Q=0.3513  ε=0.0100  buf=100000\n",
      "Ep  487/600 | R=+0.6163  L=0.0321  SO=0.0020  W=0.0217  O=0.0084  Q=0.3517  ε=0.0100  buf=100000\n",
      "Ep  488/600 | R=+0.6171  L=0.0373  SO=0.0020  W=0.0217  O=0.0080  Q=0.3512  ε=0.0100  buf=100000\n",
      "Ep  489/600 | R=+0.6165  L=0.0346  SO=0.0020  W=0.0217  O=0.0076  Q=0.3523  ε=0.0100  buf=100000\n",
      "Ep  490/600 | R=+0.6165  L=0.0413  SO=0.0020  W=0.0217  O=0.0087  Q=0.3512  ε=0.0100  buf=100000\n",
      "  ✓ Checkpoint saved at episode 490\n",
      "Ep  491/600 | R=+0.6118  L=0.0381  SO=0.0020  W=0.0215  O=0.0100  Q=0.3547  ε=0.0100  buf=100000\n",
      "Ep  492/600 | R=+0.6188  L=0.0363  SO=0.0020  W=0.0217  O=0.0099  Q=0.3476  ε=0.0100  buf=100000\n",
      "Ep  493/600 | R=+0.6181  L=0.0305  SO=0.0020  W=0.0217  O=0.0095  Q=0.3488  ε=0.0100  buf=100000\n",
      "Ep  494/600 | R=+0.6182  L=0.0331  SO=0.0020  W=0.0217  O=0.0101  Q=0.3480  ε=0.0100  buf=100000\n",
      "Ep  495/600 | R=+0.6186  L=0.0337  SO=0.0019  W=0.0217  O=0.0102  Q=0.3476  ε=0.0100  buf=100000\n",
      "Ep  496/600 | R=+0.6180  L=0.0387  SO=0.0020  W=0.0217  O=0.0092  Q=0.3491  ε=0.0100  buf=100000\n",
      "Ep  497/600 | R=+0.6183  L=0.0323  SO=0.0019  W=0.0217  O=0.0098  Q=0.3483  ε=0.0100  buf=100000\n",
      "Ep  498/600 | R=+0.6185  L=0.0372  SO=0.0020  W=0.0217  O=0.0095  Q=0.3483  ε=0.0100  buf=100000\n",
      "Ep  499/600 | R=+0.6158  L=0.0348  SO=0.0020  W=0.0217  O=0.0105  Q=0.3500  ε=0.0100  buf=100000\n",
      "Ep  500/600 | R=+0.6186  L=0.0315  SO=0.0019  W=0.0217  O=0.0095  Q=0.3482  ε=0.0100  buf=100000\n",
      "  ✓ Checkpoint saved at episode 500\n",
      "Ep  501/600 | R=+0.6095  L=0.0433  SO=0.0022  W=0.0215  O=0.0085  Q=0.3584  ε=0.0100  buf=100000\n",
      "Ep  502/600 | R=+0.6170  L=0.0381  SO=0.0020  W=0.0217  O=0.0088  Q=0.3505  ε=0.0100  buf=100000\n",
      "Ep  503/600 | R=+0.6159  L=0.0351  SO=0.0020  W=0.0217  O=0.0089  Q=0.3514  ε=0.0100  buf=100000\n",
      "\n",
      "============================================================\n",
      "Training interrupted — checkpoints saved.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 11. RUN TRAINING\n",
    "# ============================================================\n",
    "\n",
    "# Set FLAGS.use_wandb = True above if you want W&B tracking.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING DQN TRAINING (A2C fair-comparison mode)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    trained_agent = train_dqn()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training interrupted — checkpoints saved.\")\n",
    "    print(\"=\" * 60)\n",
    "except Exception as exc:\n",
    "    import traceback\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ERROR: {exc}\")\n",
    "    traceback.print_exc()\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa000014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising agent for prediction...\n",
      "✓ Loaded: checkpoints_dqn_comparison3\\ckpt-50\n",
      "Writing predictions to ./output_dqn_comparison.csv3...\n",
      "✓ Prediction complete — ./output_dqn_comparison.csv3\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 12. RUN PREDICTION\n",
    "#     (run this cell AFTER training, or after restoring a checkpoint)\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    predict_dqn()\n",
    "except Exception as exc:\n",
    "    import traceback\n",
    "    print(f\"ERROR: {exc}\")\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
