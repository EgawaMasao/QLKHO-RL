{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec53fb1",
   "metadata": {},
   "source": [
    "# A2C-mod (Modified A2C) Algorithm\n",
    "## Reinforcement Learning for Inventory Management\n",
    "\n",
    "This notebook implements the A2C-mod (Modified A2C) algorithm for optimizing grocery store inventory management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3310a69",
   "metadata": {},
   "source": [
    "## Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a3805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install required packages\n",
    "packages = ['tensorflow', 'numpy']\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"‚úì {package} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(f\"‚úì {package} installed successfully\")\n",
    "\n",
    "print(\"\\n‚úì All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f3b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(edgeitems=25, linewidth=10000, precision=12, suppress=True)\n",
    "\n",
    "# Set up algorithm\n",
    "ALGORITHM = 'A2C_mod'\n",
    "print(f\"Training with {ALGORITHM} Algorithm\")\n",
    "\n",
    "# Default FLAGS\n",
    "class FLAGS:\n",
    "    output_dir = 'C:\\\\NCKH\\\\XAI\\\\checkpoints_a2c_mod'\n",
    "    train_file = 'data/train.tfrecords'\n",
    "    capacity_file = 'data/capacity.tfrecords'\n",
    "    stock_file = 'data/stock.tfrecords'\n",
    "    predict_file = 'data/test.tfrecords'\n",
    "    output_file = './output_a2c_mod.csv'\n",
    "    dropout_prob = 0.1\n",
    "    train_episodes = 600\n",
    "    num_products = 220\n",
    "    num_timesteps = 900\n",
    "    num_features = 3\n",
    "    num_actions = 14\n",
    "    hidden_size = 32\n",
    "    entropy_coefficient = 0.001\n",
    "    gamma = 0.99\n",
    "    waste = 0.025\n",
    "    actor_learning_rate = 0.001\n",
    "    critic_learning_rate = 0.001\n",
    "    zero_inventory = 1e-5\n",
    "    batch_size = 32\n",
    "    action = 'TRAIN'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a913213",
   "metadata": {},
   "source": [
    "## Define Neural Network Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfbb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(tf.Module):\n",
    "  def __init__(self, input_dim, output_size, activation=None, stddev=1.0):\n",
    "    super(Dense, self).__init__()\n",
    "    self.w = tf.Variable(\n",
    "      tf.random.truncated_normal([input_dim, output_size], stddev=stddev), name='w')\n",
    "    self.b = tf.Variable(tf.zeros([output_size]), name='b')\n",
    "    self.activation = activation\n",
    "  def __call__(self, x):\n",
    "    y = tf.matmul(x, self.w) + self.b\n",
    "    if (self.activation):\n",
    "      y = self.activation(y)\n",
    "    return y\n",
    "\n",
    "class Actor(tf.Module):\n",
    "  def __init__(self, num_features, num_actions, hidden_size, activation=tf.nn.relu, dropout_prob=0.1):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer1 = Dense(num_features, hidden_size, activation=None)\n",
    "    self.layer2 = Dense(hidden_size, hidden_size, activation=None)\n",
    "    self.layer3 = Dense(hidden_size, hidden_size, activation=None)\n",
    "    self.layer4 = Dense(hidden_size, num_actions, activation=None)\n",
    "    self.activation = activation\n",
    "    self.dropout_prob = dropout_prob\n",
    "  def __call__(self, state):\n",
    "    layer_output = self.layer1(state)\n",
    "    layer_output = self.activation(layer_output)\n",
    "    layer_output = tf.nn.dropout(layer_output, self.dropout_prob)\n",
    "\n",
    "    layer_output = self.layer2(layer_output)\n",
    "    layer_output = self.activation(layer_output)\n",
    "    layer_output = tf.nn.dropout(layer_output, self.dropout_prob)\n",
    "\n",
    "    layer_output = self.layer3(layer_output)\n",
    "    layer_output = self.activation(layer_output)\n",
    "    layer_output = tf.nn.dropout(layer_output, self.dropout_prob)\n",
    "\n",
    "    layer_output = self.layer4(layer_output)\n",
    "    return tf.nn.softmax(layer_output)\n",
    "\n",
    "class Critic(tf.Module):\n",
    "  def __init__(self, num_features, hidden_size, activation=tf.nn.relu, dropout_prob=0.1):\n",
    "    super(Critic, self).__init__()\n",
    "    self.layer1 = Dense(num_features, hidden_size, activation=None)\n",
    "    self.layer2 = Dense(hidden_size, 1, activation=None)\n",
    "    self.activation = activation\n",
    "    self.dropout_prob = dropout_prob\n",
    "  def __call__(self, state):\n",
    "    layer_output = self.layer1(state)\n",
    "    # Layer normalization instead of GroupNormalization\n",
    "    layer_output = tf.keras.layers.LayerNormalization()(layer_output)\n",
    "    layer_output = self.activation(layer_output)\n",
    "    layer_output = tf.nn.dropout(layer_output, self.dropout_prob)\n",
    "\n",
    "    layer_output = self.layer2(layer_output)\n",
    "    return tf.squeeze(layer_output, axis=-1, name='factor_squeeze')\n",
    "\n",
    "print(\"Actor and Critic networks defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a25d06",
   "metadata": {},
   "source": [
    "## Implement Data Parsers and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd69f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sales_parser(serialized_example):\n",
    "  example = tf.io.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\"sales\": tf.io.FixedLenFeature([FLAGS.num_products], tf.float32)})\n",
    "  for name in list(example.keys()):\n",
    "    t = example[name]\n",
    "    if t.dtype == tf.int64:\n",
    "      t = tf.cast(t, tf.float32)\n",
    "      example[name] = t\n",
    "  return example\n",
    "\n",
    "def capacity_parser(serialized_example):\n",
    "  example = tf.io.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\"capacity\": tf.io.FixedLenFeature([FLAGS.num_products], tf.float32)})\n",
    "  for name in list(example.keys()):\n",
    "    t = example[name]\n",
    "    if t.dtype == tf.int64:\n",
    "      t = tf.cast(t, tf.float32)\n",
    "      example[name] = t\n",
    "  return example\n",
    "\n",
    "def stock_parser(serialized_example):\n",
    "  example = tf.io.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\"stock\": tf.io.FixedLenFeature([FLAGS.num_products], tf.float32)})\n",
    "  for name in list(example.keys()):\n",
    "    t = example[name]\n",
    "    if t.dtype == tf.int64:\n",
    "      t = tf.cast(t, tf.float32)\n",
    "      example[name] = t\n",
    "  return example\n",
    "\n",
    "def waste(x):\n",
    "   return FLAGS.waste * x\n",
    "\n",
    "def quantile(x, q):\n",
    "  return np.quantile(x, q)\n",
    "\n",
    "def cross_entropy(p, q):\n",
    "  return -tf.reduce_mean(tf.reduce_sum(p*tf.math.log(tf.math.maximum(1e-15, q)), axis=1))\n",
    "\n",
    "print(\"Data parsers and helper functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deebf3d",
   "metadata": {},
   "source": [
    "## Training Loop with A2C-mod Algorithm\n",
    "\n",
    "The A2C-mod algorithm modifies the policy update using advantage weighting:\n",
    "- p_new = softmax(log(œÄ(a|s)) + advantage / (|a_selected - a_all| + 1))\n",
    "- actor_loss = mean squared difference between p_old and p_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "class TrainingLogger:\n",
    "    def __init__(self, log_dir='./logA2Cmod'):\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.log_dir = log_dir\n",
    "        self.timestamp = timestamp\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.episode_logs = []\n",
    "        \n",
    "    def log_step(self, global_step, experience_step, rewards, stockouts, waste_val, delta, critic_loss, entropy_adj, actor_loss):\n",
    "        \"\"\"Log metrics for each training step\"\"\"\n",
    "        self.metrics['global_step'].append(int(global_step))\n",
    "        self.metrics['experience_step'].append(int(experience_step))\n",
    "        self.metrics['rewards'].append(float(rewards))\n",
    "        self.metrics['stockouts'].append(float(stockouts))\n",
    "        self.metrics['waste'].append(float(waste_val))\n",
    "        self.metrics['delta'].append(float(delta))\n",
    "        self.metrics['critic_loss'].append(float(critic_loss))\n",
    "        self.metrics['entropy_adjusted'].append(float(entropy_adj))\n",
    "        self.metrics['actor_loss'].append(float(actor_loss))\n",
    "        \n",
    "    def save_episode_logs(self, episode):\n",
    "        \"\"\"Save logs for each episode\"\"\"\n",
    "        if not self.metrics or len(self.metrics['global_step']) == 0:\n",
    "            return None, None\n",
    "        \n",
    "        # Create episode-specific filenames\n",
    "        json_file = os.path.join(self.log_dir, f'training_log_{self.timestamp}_episode_{episode:04d}.json')\n",
    "        csv_file = os.path.join(self.log_dir, f'training_log_{self.timestamp}_episode_{episode:04d}.csv')\n",
    "        \n",
    "        # Save as JSON\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(dict(self.metrics), f, indent=2)\n",
    "        \n",
    "        # Save as CSV\n",
    "        if self.metrics:\n",
    "            keys = list(self.metrics.keys())\n",
    "            with open(csv_file, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(keys)\n",
    "                for i in range(len(self.metrics[keys[0]])):\n",
    "                    writer.writerow([self.metrics[k][i] for k in keys])\n",
    "        \n",
    "        # Store episode summary\n",
    "        episode_summary = {\n",
    "            'episode': episode,\n",
    "            'steps': len(self.metrics['global_step']),\n",
    "            'rewards_mean': float(np.mean(self.metrics['rewards'])),\n",
    "            'rewards_std': float(np.std(self.metrics['rewards'])),\n",
    "            'stockouts_mean': float(np.mean(self.metrics['stockouts'])),\n",
    "            'waste_mean': float(np.mean(self.metrics['waste'])),\n",
    "            'delta_mean': float(np.mean(self.metrics['delta'])),\n",
    "            'critic_loss_mean': float(np.mean(self.metrics['critic_loss'])),\n",
    "            'entropy_adjusted_mean': float(np.mean(self.metrics['entropy_adjusted'])),\n",
    "            'actor_loss_mean': float(np.mean(self.metrics['actor_loss']))\n",
    "        }\n",
    "        self.episode_logs.append(episode_summary)\n",
    "        \n",
    "        # Reset metrics for next episode\n",
    "        self.metrics = defaultdict(list)\n",
    "        \n",
    "        return json_file, csv_file\n",
    "    \n",
    "    def save_summary(self):\n",
    "        \"\"\"Save summary of all episodes\"\"\"\n",
    "        summary_file = os.path.join(self.log_dir, f'training_summary_{self.timestamp}.json')\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(self.episode_logs, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n‚úì Training summary saved: {summary_file}\")\n",
    "        return summary_file\n",
    "\n",
    "print(\"TrainingLogger class defined successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3022a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  # Initialize logger\n",
    "  logger = TrainingLogger(log_dir='./logA2Cmod')\n",
    "  \n",
    "  sales_dataset = tf.data.TFRecordDataset(FLAGS.train_file).window(FLAGS.batch_size, shift=FLAGS.batch_size-1, drop_remainder=False)\n",
    "\n",
    "  capacity_dataset = tf.data.TFRecordDataset(FLAGS.capacity_file)\n",
    "  parsed_capacity_dataset = capacity_dataset.map(capacity_parser)\n",
    "  capacity = next(iter(parsed_capacity_dataset))['capacity']\n",
    "\n",
    "  actor_optimizer = tf.optimizers.Adam(FLAGS.actor_learning_rate)\n",
    "  critic_optimizer = tf.optimizers.Adam(FLAGS.critic_learning_rate)\n",
    "\n",
    "  actor = Actor(FLAGS.num_features, FLAGS.num_actions, FLAGS.hidden_size, activation=tf.nn.relu, dropout_prob=FLAGS.dropout_prob)\n",
    "  critic = Critic(FLAGS.num_features, FLAGS.hidden_size, activation=tf.nn.relu, dropout_prob=FLAGS.dropout_prob)\n",
    "\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  checkpoint_prefix = os.path.join(FLAGS.output_dir, \"ckpt\")\n",
    "  checkpoint = tf.train.Checkpoint(critic_optimizer=critic_optimizer, actor_optimizer=actor_optimizer, critic=critic, actor=actor, step=global_step)\n",
    "  status = checkpoint.restore(tf.train.latest_checkpoint(FLAGS.output_dir))\n",
    "\n",
    "  for episode in range(FLAGS.train_episodes):\n",
    "    x = tf.random.uniform(shape=[FLAGS.num_products], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
    "    q = waste(x)\n",
    "\n",
    "    for batch_dataset in sales_dataset:\n",
    "      with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "        experience_step = tf.constant(0)\n",
    "        experience_s = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products, FLAGS.num_features]), name=\"experience_s\")\n",
    "        experience_u = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_u\")\n",
    "        experience_p = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products, FLAGS.num_actions]), name=\"experience_p\")\n",
    "        experience_i = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.int64, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_i\")\n",
    "        experience_overstock = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_overstock\")\n",
    "        experience_s_prime = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products, FLAGS.num_features]), name=\"experience_s_prime\")\n",
    "        experience_r = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_r\")\n",
    "        experience_z = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_z\")\n",
    "        experience_q = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_q\")\n",
    "        experience_quan = tf.TensorArray(size=FLAGS.batch_size, dtype=tf.float32, element_shape=tf.TensorShape([FLAGS.num_products]), name=\"experience_quan\")\n",
    "\n",
    "        batch_iterator = batch_dataset.map(sales_parser)\n",
    "        sales = tf.divide(next(iter(batch_iterator))['sales'], capacity)\n",
    "\n",
    "        s = tf.transpose(tf.stack([x, sales, q], axis=0), perm=[1, 0])\n",
    "        policy_probs = actor(s)\n",
    "\n",
    "        for item in batch_iterator:\n",
    "          sales_prime = tf.divide(item['sales'], capacity)\n",
    "          policy_index = tf.squeeze(tf.random.categorical(tf.math.log(policy_probs), 1))\n",
    "          policy_mask = tf.one_hot(policy_index, FLAGS.num_actions)\n",
    "          policy_selected = tf.boolean_mask(policy_probs, policy_mask)\n",
    "     \n",
    "          action_space = tf.tile([[0, 0.005, 0.01, 0.0125, 0.015, 0.0175, 0.02, 0.03, 0.04, 0.08, 0.12, 0.2, 0.5, 1]], [FLAGS.num_products, 1])\n",
    "          u = tf.boolean_mask(action_space, policy_mask)\n",
    "\n",
    "          overstock = tf.math.maximum(0, (x + u) - 1)\n",
    "          x_u = tf.math.minimum(1, x + u)\n",
    "          x_prime = tf.math.maximum(0, x_u - sales)\n",
    "        \n",
    "          q_prime = waste(x_prime)\n",
    "          s_prime = tf.transpose(tf.stack([x_prime, sales_prime, q_prime], axis=0), perm=[1, 0])\n",
    "\n",
    "          z = tf.cast(x < FLAGS.zero_inventory, tf.float32)\n",
    "          quan = tf.repeat(tf.cast(quantile(x, 0.95) - quantile(x, 0.05), tf.float32), FLAGS.num_products)\n",
    "          r = tf.cast(1 - z - overstock - q - quan, tf.float32)\n",
    "\n",
    "          experience_s = experience_s.write(experience_step, s)\n",
    "          experience_u = experience_u.write(experience_step, u)\n",
    "          experience_p = experience_p.write(experience_step, policy_probs)\n",
    "          experience_i = experience_i.write(experience_step, policy_index)\n",
    "          experience_overstock = experience_overstock.write(experience_step, overstock)\n",
    "          experience_s_prime = experience_s_prime.write(experience_step, s_prime)\n",
    "          experience_r = experience_r.write(experience_step, r)\n",
    "          experience_z = experience_z.write(experience_step, z)\n",
    "          experience_q = experience_q.write(experience_step, q)\n",
    "          experience_quan = experience_quan.write(experience_step, quan)\n",
    "\n",
    "          policy_probs = actor(s_prime)\n",
    "          x = x_prime\n",
    "          q = q_prime\n",
    "          s = s_prime\n",
    "          sales = sales_prime\n",
    "          experience_step = experience_step + 1\n",
    "\n",
    "        s_batch = tf.reshape(experience_s.stack()[:experience_step, :, :], [-1, FLAGS.num_features])\n",
    "        p_batch = tf.reshape(experience_p.stack()[:experience_step, :], [-1, FLAGS.num_actions])\n",
    "        i_batch = tf.reshape(experience_i.stack()[:experience_step, :], [-1])\n",
    "        overstock_batch = tf.reshape(experience_overstock.stack()[:experience_step, :], [-1])\n",
    "        s_prime_batch = tf.reshape(experience_s_prime.stack()[:experience_step, :, :], [-1, FLAGS.num_features])\n",
    "        r_batch = tf.reshape(experience_r.stack()[:experience_step, :], [-1])\n",
    "        z_batch = tf.reshape(experience_z.stack()[:experience_step, :], [-1])\n",
    "        q_batch = tf.reshape(experience_q.stack()[:experience_step, :], [-1])\n",
    "        quan_batch = tf.reshape(experience_quan.stack()[:experience_step, :], [-1])\n",
    "\n",
    "        # Calculate metrics\n",
    "        rewards_mean = tf.reduce_mean(r_batch, keepdims=False)\n",
    "        stockouts_mean = tf.reduce_mean(z_batch, keepdims=False)\n",
    "        waste_mean = tf.reduce_mean(q_batch, keepdims=False)\n",
    "\n",
    "        tf.print(\"rewards:\", global_step, experience_step, rewards_mean, output_stream=sys.stderr, summarize=-1)\n",
    "        tf.print(\"stockouts:\", global_step, experience_step, stockouts_mean, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "        v = critic(s_batch)\n",
    "        v_prime = critic(s_prime_batch)\n",
    "        y = r_batch + FLAGS.gamma*v_prime\n",
    "\n",
    "        delta = y - v\n",
    "        delta_mean = tf.reduce_mean(delta, keepdims=False)\n",
    "        tf.print(\"delta:\", global_step, delta_mean, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "        critic_loss = 0.5*tf.reduce_mean(tf.math.square(delta), keepdims=False)\n",
    "        tf.print(\"critic loss:\", global_step, critic_loss, output_stream=sys.stderr, summarize=-1)\n",
    "\n",
    "        entropy_p = cross_entropy(p_batch, p_batch)\n",
    "        entropy_adj = FLAGS.entropy_coefficient*entropy_p\n",
    "\n",
    "        # A2C_mod Algorithm\n",
    "        ix_batch = tf.tile(tf.reshape(i_batch, [-1, 1]), [1, FLAGS.num_actions])\n",
    "        p_new = tf.nn.softmax(tf.math.log(tf.math.maximum(1e-15, p_batch)) + tf.reshape(delta, [-1, 1]) / tf.cast(tf.math.abs(ix_batch - tf.cast(tf.range(FLAGS.num_actions), tf.int64)) + 1, tf.float32))\n",
    "        per_timestep_actor_loss = tf.reduce_mean(tf.math.squared_difference(p_batch, p_new), axis=-1)\n",
    "        actor_loss = tf.reduce_mean(per_timestep_actor_loss, axis=-1)\n",
    "        \n",
    "        tf.print(\"actor loss:\", global_step, actor_loss, output_stream=sys.stderr, summarize=-1)\n",
    "        \n",
    "        # Log metrics to file\n",
    "        logger.log_step(\n",
    "            global_step=int(global_step),\n",
    "            experience_step=int(experience_step),\n",
    "            rewards=float(rewards_mean),\n",
    "            stockouts=float(stockouts_mean),\n",
    "            waste_val=float(waste_mean),\n",
    "            delta=float(delta_mean),\n",
    "            critic_loss=float(critic_loss),\n",
    "            entropy_adj=float(entropy_adj),\n",
    "            actor_loss=float(actor_loss)\n",
    "        )\n",
    "        \n",
    "        global_step.assign_add(1)\n",
    "\n",
    "      actor_gradients = actor_tape.gradient(actor_loss, actor.variables)\n",
    "      critic_gradients = critic_tape.gradient(critic_loss, critic.variables)\n",
    "\n",
    "      actor_optimizer.apply_gradients(zip(actor_gradients, actor.variables))\n",
    "      critic_optimizer.apply_gradients(zip(critic_gradients, critic.variables))\n",
    "\n",
    "    # Save logs after each episode\n",
    "    json_file, csv_file = logger.save_episode_logs(episode + 1)\n",
    "    if (episode + 1) % 10 == 0:\n",
    "      print(f\"Episode {episode + 1} - Logs saved:\")\n",
    "      print(f\"  JSON: {json_file}\")\n",
    "      print(f\"  CSV: {csv_file}\")\n",
    "    \n",
    "    if (episode + 1) % 10 == 0:\n",
    "      checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "      print(f\"Checkpoint saved at episode {episode + 1}\")\n",
    "\n",
    "  tf.print (\"episode:\", episode, global_step, output_stream=sys.stderr, summarize=-1)\n",
    "  \n",
    "  # Save summary of all episodes\n",
    "  logger.save_summary()\n",
    "\n",
    "print(\"Train function defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4f6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34e25cd2",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "  sales_dataset = tf.data.TFRecordDataset(FLAGS.predict_file)\n",
    "  capacity_dataset = tf.data.TFRecordDataset(FLAGS.capacity_file)\n",
    "  stock_dataset = tf.data.TFRecordDataset(FLAGS.stock_file)\n",
    "\n",
    "  parsed_capacity_dataset = capacity_dataset.map(capacity_parser)\n",
    "  capacity = next(iter(parsed_capacity_dataset))['capacity']\n",
    "\n",
    "  parsed_dataset = sales_dataset.map(sales_parser)\n",
    "\n",
    "  parsed_stock_dataset = stock_dataset.map(stock_parser)\n",
    "  x = next(iter(parsed_stock_dataset))['stock']\n",
    "\n",
    "  actor = Actor(FLAGS.num_features, FLAGS.num_actions, FLAGS.hidden_size, activation=tf.nn.relu, dropout_prob=FLAGS.dropout_prob)\n",
    "\n",
    "  checkpoint = tf.train.Checkpoint(actor=actor)\n",
    "  checkpoint.restore(tf.train.latest_checkpoint(FLAGS.output_dir)).expect_partial()\n",
    "\n",
    "  with tf.io.gfile.GFile(FLAGS.output_file, \"w\") as writer:\n",
    "    for sales_record in parsed_dataset:\n",
    "      sales = tf.divide(sales_record['sales'], capacity)\n",
    "      q = waste(x)\n",
    "      s = tf.transpose(tf.stack([x, sales, q], axis=0), perm=[1, 0])\n",
    "\n",
    "      policy_probs = actor(s)\n",
    "      policy_mask = tf.one_hot(tf.math.argmax(policy_probs, axis=-1), FLAGS.num_actions)\n",
    "      action_space = tf.tile([[0, 0.005, 0.01, 0.0125, 0.015, 0.0175, 0.02, 0.03, 0.04, 0.08, 0.12, 0.2, 0.5, 1]], [FLAGS.num_products, 1])\n",
    "      u = tf.boolean_mask(action_space, policy_mask)\n",
    "\n",
    "      overstock = tf.math.maximum(0, (x + u) - 1)\n",
    "      x_u = tf.math.minimum(1, x + u)\n",
    "      stockout = tf.math.minimum(0, x_u - sales)\n",
    "\n",
    "      writer.write(\"stock:\" + ','.join(  list(map(str,   x.numpy()    ))    ) + \"\\n\")\n",
    "      writer.write(\"action:\" + ','.join(  list(map(str,   u.numpy()    ))    ) + \"\\n\")\n",
    "      writer.write(\"overstock:\" + ','.join(  list(map(str,   overstock.numpy()    ))    ) + \"\\n\")\n",
    "      writer.write(\"sales:\" + ','.join(  list(map(str,   sales.numpy()    ))    ) + \"\\n\")\n",
    "      writer.write(\"stockout:\" + ','.join(  list(map(str,   stockout.numpy()    ))    ) + \"\\n\")\n",
    "      writer.write(\"capacity:\" + ','.join(  list(map(str,   (capacity/capacity).numpy()    ))    ) + \"\\n\")\n",
    "\n",
    "      x = tf.math.maximum(0, x_u - sales)\n",
    "\n",
    "print(\"Predict function defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5478d8",
   "metadata": {},
   "source": [
    "## Execute Training or Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf4a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(FLAGS.output_dir, exist_ok=True)\n",
    "\n",
    "if FLAGS.action == 'TRAIN':\n",
    "    print(f\"Starting {ALGORITHM} training...\")\n",
    "    train()\n",
    "    print(f\"{ALGORITHM} training completed!\")\n",
    "elif FLAGS.action == 'PREDICT':\n",
    "    print(f\"Starting {ALGORITHM} prediction...\")\n",
    "    predict()\n",
    "    print(f\"{ALGORITHM} prediction completed! Results saved to {FLAGS.output_file}\")\n",
    "else:\n",
    "    print(f\"Unknown action: {FLAGS.action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcd66f0",
   "metadata": {},
   "source": [
    "## Gi·∫£i Th√≠ch Chi Ti·∫øt C√°c Th√¥ng S·ªë FLAGS\n",
    "\n",
    "### **T·ªïng Quan V·ªÅ Hyperparameters**\n",
    "\n",
    "C√°c th√¥ng s·ªë FLAGS ƒë·ªãnh nghƒ©a:\n",
    "- **C·∫•u h√¨nh d·ªØ li·ªáu**: ƒê∆∞·ªùng d·∫´n file, k√≠ch th∆∞·ªõc d·ªØ li·ªáu\n",
    "- **Ki·∫øn tr√∫c m·∫°ng**: S·ªë l·ªõp ·∫©n, s·ªë h√†nh ƒë·ªông\n",
    "- **Tham s·ªë hu·∫•n luy·ªán**: Learning rate, gamma, entropy coefficient\n",
    "- **C·∫•u h√¨nh ch·∫°y**: S·ªë t·∫≠p phim, batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ee4cfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "GI·∫¢I TH√çCH CHI TI·∫æT C√ÅC TH√îNG S·ªê FLAGS (A2C-MOD)\n",
      "====================================================================================================\n",
      "\n",
      "            Th√¥ng S·ªë                         Gi√° Tr·ªã                                                     √ù Nghƒ©a\n",
      "          output_dir C:\\NCKH\\XAI\\checkpoints_a2c_mod             Th∆∞ m·ª•c l∆∞u tr·ªØ c√°c checkpoint (m√¥ h√¨nh ƒë√£ l∆∞u)\n",
      "          train_file            data/train.tfrecords          File TFRecords ch·ª©a d·ªØ li·ªáu b√°n h√†ng ƒë·ªÉ hu·∫•n luy·ªán\n",
      "       capacity_file         data/capacity.tfrecords      File TFRecords ch·ª©a d·ªØ li·ªáu s·ª©c ch·ª©a c·ªßa t·ª´ng s·∫£n ph·∫©m\n",
      "          stock_file            data/stock.tfrecords                 File TFRecords ch·ª©a d·ªØ li·ªáu t·ªìn kho ban ƒë·∫ßu\n",
      "        predict_file             data/test.tfrecords             File TFRecords ch·ª©a d·ªØ li·ªáu b√°n h√†ng ƒë·ªÉ d·ª± ƒëo√°n\n",
      "         output_file            ./output_a2c_mod.csv                               File CSV xu·∫•t k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
      "        dropout_prob                             0.1          T·ª∑ l·ªá dropout trong m·∫°ng n∆°-ron (gi·∫£m overfitting)\n",
      "      train_episodes                             600 S·ªë t·∫≠p phim (episodes) ƒë·ªÉ hu·∫•n luy·ªán (1 episode = 900 b∆∞·ªõc)\n",
      "        num_products                             220                     S·ªë lo·∫°i s·∫£n ph·∫©m qu·∫£n l√Ω (220 s·∫£n ph·∫©m)\n",
      "       num_timesteps                             900             S·ªë b∆∞·ªõc th·ªùi gian trong m·ªói t·∫≠p phim (900 b∆∞·ªõc)\n",
      "        num_features                               3       S·ªë ƒë·∫∑c tr∆∞ng c·ªßa tr·∫°ng th√°i (inventory, sales, waste)\n",
      "         num_actions                              14     S·ªë h√†nh ƒë·ªông m√† agent c√≥ th·ªÉ ch·ªçn (14 m·ª©c tƒÉng t·ªìn kho)\n",
      "         hidden_size                              32     S·ªë neuron trong m·ªói l·ªõp ·∫©n c·ªßa Actor/Critic (32 neuron)\n",
      " entropy_coefficient                           0.001           H·ªá s·ªë entropy ƒë·ªÉ khuy·∫øn kh√≠ch exploration (0.001)\n",
      "               gamma                            0.99      Discount factor - tr·ªçng s·ªë cho reward t∆∞∆°ng lai (0.99)\n",
      "               waste                           0.025                     T·ª∑ l·ªá l√£ng ph√≠ h√†ng h√≥a (2.5% m·ªói b∆∞·ªõc)\n",
      " actor_learning_rate                           0.001                     Learning rate cho Actor network (0.001)\n",
      "critic_learning_rate                           0.001                    Learning rate cho Critic network (0.001)\n",
      "      zero_inventory                            1e-5                           Ng∆∞·ª°ng xem l√† h·∫øt h√†ng (1e-5 ‚âà 0)\n",
      "          batch_size                              32   K√≠ch th∆∞·ªõc batch - s·ªë samples x·ª≠ l√Ω c√πng l√∫c (32 samples)\n",
      "              action                           TRAIN      Ch·∫ø ƒë·ªô ch·∫°y: TRAIN (hu·∫•n luy·ªán) ho·∫∑c PREDICT (d·ª± ƒëo√°n)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# T·∫°o b·∫£ng gi·∫£i th√≠ch chi ti·∫øt c√°c th√¥ng s·ªë FLAGS\n",
    "parameters_explanation = {\n",
    "    'Th√¥ng S·ªë': [\n",
    "        'output_dir',\n",
    "        'train_file',\n",
    "        'capacity_file',\n",
    "        'stock_file',\n",
    "        'predict_file',\n",
    "        'output_file',\n",
    "        'dropout_prob',\n",
    "        'train_episodes',\n",
    "        'num_products',\n",
    "        'num_timesteps',\n",
    "        'num_features',\n",
    "        'num_actions',\n",
    "        'hidden_size',\n",
    "        'entropy_coefficient',\n",
    "        'gamma',\n",
    "        'waste',\n",
    "        'actor_learning_rate',\n",
    "        'critic_learning_rate',\n",
    "        'zero_inventory',\n",
    "        'batch_size',\n",
    "        'action'\n",
    "    ],\n",
    "    'Gi√° Tr·ªã': [\n",
    "        'C:\\\\NCKH\\\\XAI\\\\checkpoints_a2c_mod',\n",
    "        'data/train.tfrecords',\n",
    "        'data/capacity.tfrecords',\n",
    "        'data/stock.tfrecords',\n",
    "        'data/test.tfrecords',\n",
    "        './output_a2c_mod.csv',\n",
    "        '0.1',\n",
    "        '600',\n",
    "        '220',\n",
    "        '900',\n",
    "        '3',\n",
    "        '14',\n",
    "        '32',\n",
    "        '0.001',\n",
    "        '0.99',\n",
    "        '0.025',\n",
    "        '0.001',\n",
    "        '0.001',\n",
    "        '1e-5',\n",
    "        '32',\n",
    "        'TRAIN'\n",
    "    ],\n",
    "    '√ù Nghƒ©a': [\n",
    "        'Th∆∞ m·ª•c l∆∞u tr·ªØ c√°c checkpoint (m√¥ h√¨nh ƒë√£ l∆∞u)',\n",
    "        'File TFRecords ch·ª©a d·ªØ li·ªáu b√°n h√†ng ƒë·ªÉ hu·∫•n luy·ªán',\n",
    "        'File TFRecords ch·ª©a d·ªØ li·ªáu s·ª©c ch·ª©a c·ªßa t·ª´ng s·∫£n ph·∫©m',\n",
    "        'File TFRecords ch·ª©a d·ªØ li·ªáu t·ªìn kho ban ƒë·∫ßu',\n",
    "        'File TFRecords ch·ª©a d·ªØ li·ªáu b√°n h√†ng ƒë·ªÉ d·ª± ƒëo√°n',\n",
    "        'File CSV xu·∫•t k·∫øt qu·∫£ d·ª± ƒëo√°n',\n",
    "        'T·ª∑ l·ªá dropout trong m·∫°ng n∆°-ron (gi·∫£m overfitting)',\n",
    "        'S·ªë t·∫≠p phim (episodes) ƒë·ªÉ hu·∫•n luy·ªán (1 episode = 900 b∆∞·ªõc)',\n",
    "        'S·ªë lo·∫°i s·∫£n ph·∫©m qu·∫£n l√Ω (220 s·∫£n ph·∫©m)',\n",
    "        'S·ªë b∆∞·ªõc th·ªùi gian trong m·ªói t·∫≠p phim (900 b∆∞·ªõc)',\n",
    "        'S·ªë ƒë·∫∑c tr∆∞ng c·ªßa tr·∫°ng th√°i (inventory, sales, waste)',\n",
    "        'S·ªë h√†nh ƒë·ªông m√† agent c√≥ th·ªÉ ch·ªçn (14 m·ª©c tƒÉng t·ªìn kho)',\n",
    "        'S·ªë neuron trong m·ªói l·ªõp ·∫©n c·ªßa Actor/Critic (32 neuron)',\n",
    "        'H·ªá s·ªë entropy ƒë·ªÉ khuy·∫øn kh√≠ch exploration (0.001)',\n",
    "        'Discount factor - tr·ªçng s·ªë cho reward t∆∞∆°ng lai (0.99)',\n",
    "        'T·ª∑ l·ªá l√£ng ph√≠ h√†ng h√≥a (2.5% m·ªói b∆∞·ªõc)',\n",
    "        'Learning rate cho Actor network (0.001)',\n",
    "        'Learning rate cho Critic network (0.001)',\n",
    "        'Ng∆∞·ª°ng xem l√† h·∫øt h√†ng (1e-5 ‚âà 0)',\n",
    "        'K√≠ch th∆∞·ªõc batch - s·ªë samples x·ª≠ l√Ω c√πng l√∫c (32 samples)',\n",
    "        'Ch·∫ø ƒë·ªô ch·∫°y: TRAIN (hu·∫•n luy·ªán) ho·∫∑c PREDICT (d·ª± ƒëo√°n)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_params = pd.DataFrame(parameters_explanation)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"GI·∫¢I TH√çCH CHI TI·∫æT C√ÅC TH√îNG S·ªê FLAGS (A2C-MOD)\")\n",
    "print(\"=\"*100)\n",
    "print()\n",
    "print(df_params.to_string(index=False))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e31f5c",
   "metadata": {},
   "source": [
    "### **Chi Ti·∫øt: 14 H√†nh ƒê·ªông (num_actions = 14)**\n",
    "\n",
    "C√°c h√†nh ƒë·ªông ƒë·∫°i di·ªán cho **14 m·ª©c tƒÉng t·ªìn kho** kh√°c nhau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c4e261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "CHI TI·∫æT 14 H√ÄNH ƒê·ªòNG (num_actions = 14)\n",
      "====================================================================================================\n",
      "\n",
      " Action Index TƒÉng T·ªìn Kho (%)                            √ù Nghƒ©a\n",
      "            0            0.00%            Kh√¥ng tƒÉng t·ªìn kho (0%)\n",
      "            1            0.50%           TƒÉng 0.5% - TƒÉng r·∫•t nh·ªè\n",
      "            2            1.00%                TƒÉng 1% - TƒÉng th·∫•p\n",
      "            3            1.25%             TƒÉng 1.25% - TƒÉng th·∫•p\n",
      "            4            1.50%              TƒÉng 1.5% - TƒÉng th·∫•p\n",
      "            5            1.75%              TƒÉng 1.75% - TƒÉng v·ª´a\n",
      "            6            2.00%                 TƒÉng 2% - TƒÉng v·ª´a\n",
      "            7            3.00%                 TƒÉng 3% - TƒÉng v·ª´a\n",
      "            8            4.00%                 TƒÉng 4% - TƒÉng v·ª´a\n",
      "            9            8.00%                 TƒÉng 8% - TƒÉng cao\n",
      "           10           12.00%                TƒÉng 12% - TƒÉng cao\n",
      "           11           20.00%            TƒÉng 20% - TƒÉng r·∫•t cao\n",
      "           12           50.00%            TƒÉng 50% - TƒÉng c·ª±c cao\n",
      "           13          100.00% TƒÉng 100% - TƒÉng c·ª±c ƒë·∫°i (g·∫•p ƒë√¥i)\n",
      "\n",
      "üí° Gi·∫£i Th√≠ch:\n",
      "   - Agent ch·ªçn m·ªôt trong 14 h√†nh ƒë·ªông n√†y ·ªü m·ªói b∆∞·ªõc th·ªùi gian\n",
      "   - M·ª•c ti√™u: T√¨m m·ª©c tƒÉng t·ªìn kho t·ªëi ∆∞u ƒë·ªÉ c√¢n b·∫±ng gi·ªØa:\n",
      "     * Tr√°nh h·∫øt h√†ng (stockout) - khi nhu c·∫ßu > t·ªìn kho\n",
      "     * Gi·∫£m l√£ng ph√≠ (waste) - h√†ng h√≥a kh√¥ng b√°n ƒë∆∞·ª£c\n",
      "   - N·∫øu ch·ªçn h√†nh ƒë·ªông qu√° nh·ªè ‚Üí h·∫øt h√†ng ‚Üí m·∫•t doanh thu\n",
      "   - N·∫øu ch·ªçn h√†nh ƒë·ªông qu√° l·ªõn ‚Üí l√£ng ph√≠ ‚Üí m·∫•t chi ph√≠\n"
     ]
    }
   ],
   "source": [
    "# Gi·∫£i th√≠ch 14 h√†nh ƒë·ªông\n",
    "action_space = [0, 0.005, 0.01, 0.0125, 0.015, 0.0175, 0.02, 0.03, 0.04, 0.08, 0.12, 0.2, 0.5, 1]\n",
    "\n",
    "actions_df = pd.DataFrame({\n",
    "    'Action Index': range(14),\n",
    "    'TƒÉng T·ªìn Kho (%)': [f\"{a*100:.2f}%\" for a in action_space],\n",
    "    '√ù Nghƒ©a': [\n",
    "        'Kh√¥ng tƒÉng t·ªìn kho (0%)',\n",
    "        'TƒÉng 0.5% - TƒÉng r·∫•t nh·ªè',\n",
    "        'TƒÉng 1% - TƒÉng th·∫•p',\n",
    "        'TƒÉng 1.25% - TƒÉng th·∫•p',\n",
    "        'TƒÉng 1.5% - TƒÉng th·∫•p',\n",
    "        'TƒÉng 1.75% - TƒÉng v·ª´a',\n",
    "        'TƒÉng 2% - TƒÉng v·ª´a',\n",
    "        'TƒÉng 3% - TƒÉng v·ª´a',\n",
    "        'TƒÉng 4% - TƒÉng v·ª´a',\n",
    "        'TƒÉng 8% - TƒÉng cao',\n",
    "        'TƒÉng 12% - TƒÉng cao',\n",
    "        'TƒÉng 20% - TƒÉng r·∫•t cao',\n",
    "        'TƒÉng 50% - TƒÉng c·ª±c cao',\n",
    "        'TƒÉng 100% - TƒÉng c·ª±c ƒë·∫°i (g·∫•p ƒë√¥i)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CHI TI·∫æT 14 H√ÄNH ƒê·ªòNG (num_actions = 14)\")\n",
    "print(\"=\"*100)\n",
    "print()\n",
    "print(actions_df.to_string(index=False))\n",
    "print()\n",
    "print(\"üí° Gi·∫£i Th√≠ch:\")\n",
    "print(\"   - Agent ch·ªçn m·ªôt trong 14 h√†nh ƒë·ªông n√†y ·ªü m·ªói b∆∞·ªõc th·ªùi gian\")\n",
    "print(\"   - M·ª•c ti√™u: T√¨m m·ª©c tƒÉng t·ªìn kho t·ªëi ∆∞u ƒë·ªÉ c√¢n b·∫±ng gi·ªØa:\")\n",
    "print(\"     * Tr√°nh h·∫øt h√†ng (stockout) - khi nhu c·∫ßu > t·ªìn kho\")\n",
    "print(\"     * Gi·∫£m l√£ng ph√≠ (waste) - h√†ng h√≥a kh√¥ng b√°n ƒë∆∞·ª£c\")\n",
    "print(\"   - N·∫øu ch·ªçn h√†nh ƒë·ªông qu√° nh·ªè ‚Üí h·∫øt h√†ng ‚Üí m·∫•t doanh thu\")\n",
    "print(\"   - N·∫øu ch·ªçn h√†nh ƒë·ªông qu√° l·ªõn ‚Üí l√£ng ph√≠ ‚Üí m·∫•t chi ph√≠\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8ca796",
   "metadata": {},
   "source": [
    "### **Chi Ti·∫øt: 3 ƒê·∫∑c Tr∆∞ng Tr·∫°ng Th√°i (num_features = 3)**\n",
    "\n",
    "Tr·∫°ng th√°i c·ªßa c·ª≠a h√†ng bao g·ªìm 3 th√¥ng tin ch√≠nh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c06d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "CHI TI·∫æT 3 ƒê·∫∂C TR∆ØNG TR·∫†NG TH√ÅI (num_features = 3)\n",
      "====================================================================================================\n",
      "\n",
      " Feature Index  T√™n ƒê·∫∑c Tr∆∞ng                                                                   Gi·∫£i Th√≠ch                                  C√¥ng Th·ª©c T√≠nh\n",
      "             0  x (inventory)  T·ªìn kho hi·ªán t·∫°i (t·ª∑ l·ªá t·ª´ 0 ƒë·∫øn 1) - Bao nhi√™u h√†ng h√≥a ƒëang c√≥ trong kho?                    x: t·ªìn kho / s·ª©c ch·ª©a t·ªëi ƒëa\n",
      "             1 sales (demand)         Nhu c·∫ßu b√°n h√†ng (t·ª∑ l·ªá t·ª´ 0 ƒë·∫øn 1) - Kh√°ch h√†ng mu·ªën mua bao nhi√™u?                sales: nhu c·∫ßu / s·ª©c ch·ª©a t·ªëi ƒëa\n",
      "             2      q (waste) T·ª∑ l·ªá l√£ng ph√≠ (t·ª∑ l·ªá t·ª´ 0 ƒë·∫øn 1) - Bao nhi√™u ph·∫ßn trƒÉm h√†ng s·∫Ω b·ªã l√£ng ph√≠? q = waste_rate √ó x (0.025 √ó x = 2.5% √ó t·ªìn kho)\n",
      "\n",
      "üìä M√¥ H√¨nh Tr·∫°ng Th√°i:\n",
      "   Cho m·ªói s·∫£n ph·∫©m trong 220 s·∫£n ph·∫©m, agent nh√¨n th·∫•y 3 gi√° tr·ªã n√†y:\n",
      "\n",
      "   V√≠ d·ª•: S·∫£n ph·∫©m A\n",
      "   ‚îú‚îÄ x = 0.7      ‚Üí T·ªìn kho = 70% c·ªßa s·ª©c ch·ª©a\n",
      "   ‚îú‚îÄ sales = 0.3  ‚Üí Nhu c·∫ßu = 30% c·ªßa s·ª©c ch·ª©a\n",
      "   ‚îî‚îÄ q = 0.0175   ‚Üí S·∫Ω l√£ng ph√≠ 1.75% c·ªßa t·ªìn kho\n",
      "\n",
      "   Tr·∫°ng th√°i l√† m·ªôt ma tr·∫≠n 220√ó3:\n",
      "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "   ‚îÇ [x‚ÇÅ, sales‚ÇÅ, q‚ÇÅ] (s·∫£n ph·∫©m 1)\n",
      "   ‚îÇ [x‚ÇÇ, sales‚ÇÇ, q‚ÇÇ] (s·∫£n ph·∫©m 2)\n",
      "   ‚îÇ ...                         ‚îÇ\n",
      "   ‚îÇ [x‚ÇÇ‚ÇÇ‚ÇÄ, sales‚ÇÇ‚ÇÇ‚ÇÄ, q‚ÇÇ‚ÇÇ‚ÇÄ]      ‚îÇ\n",
      "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "# Gi·∫£i th√≠ch 3 ƒë·∫∑c tr∆∞ng tr·∫°ng th√°i\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature Index': [0, 1, 2],\n",
    "    'T√™n ƒê·∫∑c Tr∆∞ng': ['x (inventory)', 'sales (demand)', 'q (waste)'],\n",
    "    'Gi·∫£i Th√≠ch': [\n",
    "        'T·ªìn kho hi·ªán t·∫°i (t·ª∑ l·ªá t·ª´ 0 ƒë·∫øn 1) - Bao nhi√™u h√†ng h√≥a ƒëang c√≥ trong kho?',\n",
    "        'Nhu c·∫ßu b√°n h√†ng (t·ª∑ l·ªá t·ª´ 0 ƒë·∫øn 1) - Kh√°ch h√†ng mu·ªën mua bao nhi√™u?',\n",
    "        'T·ª∑ l·ªá l√£ng ph√≠ (t·ª∑ l·ªá t·ª´ 0 ƒë·∫øn 1) - Bao nhi√™u ph·∫ßn trƒÉm h√†ng s·∫Ω b·ªã l√£ng ph√≠?'\n",
    "    ],\n",
    "    'C√¥ng Th·ª©c T√≠nh': [\n",
    "        'x: t·ªìn kho / s·ª©c ch·ª©a t·ªëi ƒëa',\n",
    "        'sales: nhu c·∫ßu / s·ª©c ch·ª©a t·ªëi ƒëa',\n",
    "        'q = waste_rate √ó x (0.025 √ó x = 2.5% √ó t·ªìn kho)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CHI TI·∫æT 3 ƒê·∫∂C TR∆ØNG TR·∫†NG TH√ÅI (num_features = 3)\")\n",
    "print(\"=\"*100)\n",
    "print()\n",
    "print(features_df.to_string(index=False))\n",
    "print()\n",
    "print(\"üìä M√¥ H√¨nh Tr·∫°ng Th√°i:\")\n",
    "print(\"   Cho m·ªói s·∫£n ph·∫©m trong 220 s·∫£n ph·∫©m, agent nh√¨n th·∫•y 3 gi√° tr·ªã n√†y:\")\n",
    "print()\n",
    "print(\"   V√≠ d·ª•: S·∫£n ph·∫©m A\")\n",
    "print(\"   ‚îú‚îÄ x = 0.7      ‚Üí T·ªìn kho = 70% c·ªßa s·ª©c ch·ª©a\")\n",
    "print(\"   ‚îú‚îÄ sales = 0.3  ‚Üí Nhu c·∫ßu = 30% c·ªßa s·ª©c ch·ª©a\")\n",
    "print(\"   ‚îî‚îÄ q = 0.0175   ‚Üí S·∫Ω l√£ng ph√≠ 1.75% c·ªßa t·ªìn kho\")\n",
    "print()\n",
    "print(\"   Tr·∫°ng th√°i l√† m·ªôt ma tr·∫≠n 220√ó3:\")\n",
    "print(\"   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "print(\"   ‚îÇ [x‚ÇÅ, sales‚ÇÅ, q‚ÇÅ] (s·∫£n ph·∫©m 1)\")\n",
    "print(\"   ‚îÇ [x‚ÇÇ, sales‚ÇÇ, q‚ÇÇ] (s·∫£n ph·∫©m 2)\")\n",
    "print(\"   ‚îÇ ...                         ‚îÇ\")\n",
    "print(\"   ‚îÇ [x‚ÇÇ‚ÇÇ‚ÇÄ, sales‚ÇÇ‚ÇÇ‚ÇÄ, q‚ÇÇ‚ÇÇ‚ÇÄ]      ‚îÇ\")\n",
    "print(\"   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5b6107",
   "metadata": {},
   "source": [
    "### **T√≥m T·∫Øt: Lu·ªìng X·ª≠ L√Ω Trong M·ªói B∆∞·ªõc Th·ªùi Gian**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e2fe270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "LU·ªíNG X·ª¨ L√ù TRONG M·ªñI B∆Ø·ªöC TH·ªúI GIAN\n",
      "====================================================================================================\n",
      "\n",
      "1Ô∏è‚É£  ƒê·∫¶U V√ÄO (INPUT)\n",
      "   Tr·∫°ng th√°i hi·ªán t·∫°i:\n",
      "   ‚îú‚îÄ T·ªìn kho c·ªßa 220 s·∫£n ph·∫©m (x)\n",
      "   ‚îú‚îÄ Nhu c·∫ßu b√°n h√†ng (sales)\n",
      "   ‚îî‚îÄ T·ª∑ l·ªá l√£ng ph√≠ (q = 2.5% √ó x)\n",
      "   ‚Üí K√≠ch th∆∞·ªõc: 220 √ó 3 = 660 gi√° tr·ªã\n",
      "\n",
      "2Ô∏è‚É£  X·ª¨ L√ù B·∫∞NG ACTOR NETWORK\n",
      "   ‚îú‚îÄ Input: 3 ƒë·∫∑c tr∆∞ng c·ªßa 220 s·∫£n ph·∫©m\n",
      "   ‚îú‚îÄ Hidden layers: 32 ‚Üí 32 ‚Üí 32 neurons (activation: ReLU + Dropout)\n",
      "   ‚îú‚îÄ Output: 14 x√°c su·∫•t h√†nh ƒë·ªông (softmax)\n",
      "   ‚îî‚îÄ K·∫øt qu·∫£: X√°c su·∫•t ch·ªçn m·ªói h√†nh ƒë·ªông tƒÉng t·ªìn kho\n",
      "\n",
      "3Ô∏è‚É£  CH·ªåN H√ÄNH ƒê·ªòNG\n",
      "   ‚îú‚îÄ Agent l·∫•y m·∫´u t·ª´ 14 h√†nh ƒë·ªông d·ª±a tr√™n x√°c su·∫•t\n",
      "   ‚îú‚îÄ K√≠ch th∆∞·ªõc tƒÉng t·ªìn kho ƒë∆∞·ª£c x√°c ƒë·ªãnh\n",
      "   ‚îî‚îÄ M·ªói s·∫£n ph·∫©m ƒë∆∞·ª£c tƒÉng c√πng m·ªôt m·ª©c (action space)\n",
      "\n",
      "4Ô∏è‚É£  ƒê·∫¶U RA (OUTPUT REWARD)\n",
      "   Agent nh·∫≠n ƒë∆∞·ª£c ph·∫ßn th∆∞·ªüng:\n",
      "   ‚îú‚îÄ +1 ƒëi·ªÉm: Kh√¥ng h·∫øt h√†ng\n",
      "   ‚îú‚îÄ -1 ƒëi·ªÉm: H·∫øt h√†ng (m·∫•t b√°n h√†ng)\n",
      "   ‚îî‚îÄ -waste: L√£ng ph√≠ h√†ng h√≥a\n",
      "   ‚Üí M·ª•c ti√™u: Maximize reward\n",
      "\n",
      "5Ô∏è‚É£  TR·∫† TH√ÅI TI·∫æP THEO\n",
      "   ‚îú‚îÄ T·ªìn kho m·ªõi: x' = max(0, x + action - sales)\n",
      "   ‚îú‚îÄ Nhu c·∫ßu ti·∫øp theo: sales_next\n",
      "   ‚îî‚îÄ L√£ng ph√≠ ti·∫øp theo: q' = 2.5% √ó x'\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "V√ç D·ª§ C·ª§ TH·ªÇ: S·∫¢N PH·∫®M T√ÅO\n",
      "====================================================================================================\n",
      "\n",
      "B∆∞·ªõc        T·ªìn Kho (x) Nhu C·∫ßu (sales)   L√£ng Ph√≠ (q)            H√†nh ƒê·ªông                        Reward\n",
      "   T 0.8 (80% s·ª©c ch·ª©a)       0.3 (30%)      0.02 (2%)   TƒÉng 2% (action=6) 0.68 (kh√¥ng h·∫øt, √≠t l√£ng ph√≠)\n",
      " T+1         0.55 (55%)      0.25 (25%) 0.0138 (1.38%) TƒÉng 0.5% (action=1)                   0.745 (t·ªët)\n",
      " T+2         0.42 (42%)      0.35 (35%) 0.0105 (1.05%)   TƒÉng 4% (action=8)              0.605 (h·∫øt h√†ng)\n",
      "\n",
      "Gi·∫£i th√≠ch:\n",
      "  T:   x=0.8, sales=0.3, q=0.02 ‚Üí T·ªìn kho ƒë·ªß nh∆∞ng c√≥ l√£ng ph√≠\n",
      "       Agent ch·ªçn tƒÉng 2% ‚Üí x' = min(1, 0.8+0.02) - 0.3 = 0.52\n",
      "       Reward = 1 - 0 - 0.02 = 0.98 (t·ªët)\n",
      "\n",
      "  T+1: x=0.55, sales=0.25, q=0.0138 ‚Üí T·ªìn kho v·ª´a ph·∫£i\n",
      "       Agent ch·ªçn tƒÉng 0.5% ‚Üí x' = min(1, 0.55+0.005) - 0.25 = 0.305\n",
      "       Reward = 1 - 0 - 0.0138 = 0.986 (r·∫•t t·ªët)\n",
      "\n",
      "  T+2: x=0.42, sales=0.35, q=0.0105 ‚Üí T·ªìn kho th·∫•p, nhu c·∫ßu cao\n",
      "       Agent ch·ªçn tƒÉng 4% ‚Üí x' = max(0, 0.42+0.04-0.35) = 0.11\n",
      "       Reward = 0 - 1 - 0.0105 = -1.0105 (x·∫•u - h·∫øt h√†ng!)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"LU·ªíNG X·ª¨ L√ù TRONG M·ªñI B∆Ø·ªöC TH·ªúI GIAN\")\n",
    "print(\"=\"*100)\n",
    "print()\n",
    "print(\"1Ô∏è‚É£  ƒê·∫¶U V√ÄO (INPUT)\")\n",
    "print(\"   Tr·∫°ng th√°i hi·ªán t·∫°i:\")\n",
    "print(\"   ‚îú‚îÄ T·ªìn kho c·ªßa 220 s·∫£n ph·∫©m (x)\")\n",
    "print(\"   ‚îú‚îÄ Nhu c·∫ßu b√°n h√†ng (sales)\")\n",
    "print(\"   ‚îî‚îÄ T·ª∑ l·ªá l√£ng ph√≠ (q = 2.5% √ó x)\")\n",
    "print(\"   ‚Üí K√≠ch th∆∞·ªõc: 220 √ó 3 = 660 gi√° tr·ªã\")\n",
    "print()\n",
    "print(\"2Ô∏è‚É£  X·ª¨ L√ù B·∫∞NG ACTOR NETWORK\")\n",
    "print(\"   ‚îú‚îÄ Input: 3 ƒë·∫∑c tr∆∞ng c·ªßa 220 s·∫£n ph·∫©m\")\n",
    "print(\"   ‚îú‚îÄ Hidden layers: 32 ‚Üí 32 ‚Üí 32 neurons (activation: ReLU + Dropout)\")\n",
    "print(\"   ‚îú‚îÄ Output: 14 x√°c su·∫•t h√†nh ƒë·ªông (softmax)\")\n",
    "print(\"   ‚îî‚îÄ K·∫øt qu·∫£: X√°c su·∫•t ch·ªçn m·ªói h√†nh ƒë·ªông tƒÉng t·ªìn kho\")\n",
    "print()\n",
    "print(\"3Ô∏è‚É£  CH·ªåN H√ÄNH ƒê·ªòNG\")\n",
    "print(\"   ‚îú‚îÄ Agent l·∫•y m·∫´u t·ª´ 14 h√†nh ƒë·ªông d·ª±a tr√™n x√°c su·∫•t\")\n",
    "print(\"   ‚îú‚îÄ K√≠ch th∆∞·ªõc tƒÉng t·ªìn kho ƒë∆∞·ª£c x√°c ƒë·ªãnh\")\n",
    "print(\"   ‚îî‚îÄ M·ªói s·∫£n ph·∫©m ƒë∆∞·ª£c tƒÉng c√πng m·ªôt m·ª©c (action space)\")\n",
    "print()\n",
    "print(\"4Ô∏è‚É£  ƒê·∫¶U RA (OUTPUT REWARD)\")\n",
    "print(\"   Agent nh·∫≠n ƒë∆∞·ª£c ph·∫ßn th∆∞·ªüng:\")\n",
    "print(\"   ‚îú‚îÄ +1 ƒëi·ªÉm: Kh√¥ng h·∫øt h√†ng\")\n",
    "print(\"   ‚îú‚îÄ -1 ƒëi·ªÉm: H·∫øt h√†ng (m·∫•t b√°n h√†ng)\")\n",
    "print(\"   ‚îî‚îÄ -waste: L√£ng ph√≠ h√†ng h√≥a\")\n",
    "print(\"   ‚Üí M·ª•c ti√™u: Maximize reward\")\n",
    "print()\n",
    "print(\"5Ô∏è‚É£  TR·∫† TH√ÅI TI·∫æP THEO\")\n",
    "print(\"   ‚îú‚îÄ T·ªìn kho m·ªõi: x' = max(0, x + action - sales)\")\n",
    "print(\"   ‚îú‚îÄ Nhu c·∫ßu ti·∫øp theo: sales_next\")\n",
    "print(\"   ‚îî‚îÄ L√£ng ph√≠ ti·∫øp theo: q' = 2.5% √ó x'\")\n",
    "print()\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"V√ç D·ª§ C·ª§ TH·ªÇ: S·∫¢N PH·∫®M T√ÅO\")\n",
    "print(\"=\"*100)\n",
    "print()\n",
    "\n",
    "example_data = {\n",
    "    'B∆∞·ªõc': ['T', 'T+1', 'T+2'],\n",
    "    'T·ªìn Kho (x)': ['0.8 (80% s·ª©c ch·ª©a)', '0.55 (55%)', '0.42 (42%)'],\n",
    "    'Nhu C·∫ßu (sales)': ['0.3 (30%)', '0.25 (25%)', '0.35 (35%)'],\n",
    "    'L√£ng Ph√≠ (q)': ['0.02 (2%)', '0.0138 (1.38%)', '0.0105 (1.05%)'],\n",
    "    'H√†nh ƒê·ªông': ['TƒÉng 2% (action=6)', 'TƒÉng 0.5% (action=1)', 'TƒÉng 4% (action=8)'],\n",
    "    'Reward': ['0.68 (kh√¥ng h·∫øt, √≠t l√£ng ph√≠)', '0.745 (t·ªët)', '0.605 (h·∫øt h√†ng)']\n",
    "}\n",
    "\n",
    "example_df = pd.DataFrame(example_data)\n",
    "print(example_df.to_string(index=False))\n",
    "print()\n",
    "print(\"Gi·∫£i th√≠ch:\")\n",
    "print(\"  T:   x=0.8, sales=0.3, q=0.02 ‚Üí T·ªìn kho ƒë·ªß nh∆∞ng c√≥ l√£ng ph√≠\")\n",
    "print(\"       Agent ch·ªçn tƒÉng 2% ‚Üí x' = min(1, 0.8+0.02) - 0.3 = 0.52\")\n",
    "print(\"       Reward = 1 - 0 - 0.02 = 0.98 (t·ªët)\")\n",
    "print()\n",
    "print(\"  T+1: x=0.55, sales=0.25, q=0.0138 ‚Üí T·ªìn kho v·ª´a ph·∫£i\")\n",
    "print(\"       Agent ch·ªçn tƒÉng 0.5% ‚Üí x' = min(1, 0.55+0.005) - 0.25 = 0.305\")\n",
    "print(\"       Reward = 1 - 0 - 0.0138 = 0.986 (r·∫•t t·ªët)\")\n",
    "print()\n",
    "print(\"  T+2: x=0.42, sales=0.35, q=0.0105 ‚Üí T·ªìn kho th·∫•p, nhu c·∫ßu cao\")\n",
    "print(\"       Agent ch·ªçn tƒÉng 4% ‚Üí x' = max(0, 0.42+0.04-0.35) = 0.11\")\n",
    "print(\"       Reward = 0 - 1 - 0.0105 = -1.0105 (x·∫•u - h·∫øt h√†ng!)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997de36",
   "metadata": {},
   "source": [
    "### Visualize Training Logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b917b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "def plot_episode_logs(timestamp=None, episode=None):\n",
    "    \"\"\"\n",
    "    Plot detailed metrics for a specific episode from the training logs\n",
    "    \"\"\"\n",
    "    log_dir = Path('./logA2Cmod')\n",
    "    if not log_dir.exists():\n",
    "        return\n",
    "    \n",
    "    # If no timestamp specified, find the latest one\n",
    "    if timestamp is None:\n",
    "        log_files = sorted(log_dir.glob('training_log_*_episode_*.json'))\n",
    "        if not log_files:\n",
    "            print(\"No episode log files found.\")\n",
    "            return\n",
    "        log_file_path = str(log_files[-1])\n",
    "    else:\n",
    "        if episode is None:\n",
    "            print(\"Please specify episode number when providing timestamp\")\n",
    "            return\n",
    "        log_file_path = str(log_dir / f'training_log_{timestamp}_episode_{episode:04d}.json')\n",
    "    \n",
    "    # Load the log file\n",
    "    try:\n",
    "        with open(log_file_path, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading log file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(metrics)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "    fig.suptitle(f'A2C-mod Training Metrics\\n{Path(log_file_path).name}', fontsize=16)\n",
    "    \n",
    "    # Plot each metric\n",
    "    metrics_to_plot = [\n",
    "        ('rewards', 'Rewards', axes[0, 0]),\n",
    "        ('stockouts', 'Stockouts', axes[0, 1]),\n",
    "        ('waste', 'Waste', axes[0, 2]),\n",
    "        ('delta', 'Delta (Advantage)', axes[1, 0]),\n",
    "        ('critic_loss', 'Critic Loss', axes[1, 1]),\n",
    "        ('entropy_adjusted', 'Entropy (Adjusted)', axes[1, 2]),\n",
    "        ('actor_loss', 'Actor Loss', axes[2, 0]),\n",
    "    ]\n",
    "    \n",
    "    for col, title, ax in metrics_to_plot:\n",
    "        if col in df.columns:\n",
    "            ax.plot(df[col], linewidth=1.5, alpha=0.8)\n",
    "            ax.set_title(title, fontweight='bold')\n",
    "            ax.set_xlabel('Step')\n",
    "            ax.set_ylabel('Value')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove extra subplots\n",
    "    axes[2, 1].remove()\n",
    "    axes[2, 2].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(log_file_path.replace('.json', '_plot.png'), dpi=150, bbox_inches='tight')\n",
    "    print(f\"‚úì Plot saved: {log_file_path.replace('.json', '_plot.png')}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EPISODE STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    for col in df.columns:\n",
    "        if col not in ['global_step', 'experience_step']:\n",
    "            print(f\"\\n{col.upper()}:\")\n",
    "            print(f\"  Mean:   {df[col].mean():.6f}\")\n",
    "            print(f\"  Std:    {df[col].std():.6f}\")\n",
    "            print(f\"  Min:    {df[col].min():.6f}\")\n",
    "            print(f\"  Max:    {df[col].max():.6f}\")\n",
    "\n",
    "def plot_training_summary(timestamp=None):\n",
    "    \"\"\"\n",
    "    Plot summary statistics across all episodes\n",
    "    \"\"\"\n",
    "    log_dir = Path('./logA2Cmod')\n",
    "    if not log_dir.exists():\n",
    "        print(\"No training logs found. Please run training first.\")\n",
    "        return\n",
    "    \n",
    "    # If no timestamp specified, find the latest one\n",
    "    if timestamp is None:\n",
    "        summary_files = sorted(log_dir.glob('training_summary_*.json'))\n",
    "        if not summary_files:\n",
    "            print(\"No training summary found.\")\n",
    "            return\n",
    "        summary_file = str(summary_files[-1])\n",
    "    else:\n",
    "        summary_file = str(log_dir / f'training_summary_{timestamp}.json')\n",
    "    \n",
    "    # Load summary\n",
    "    try:\n",
    "        with open(summary_file, 'r') as f:\n",
    "            episode_logs = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading summary file: {e}\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(episode_logs)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle(f'A2C-mod Training Summary\\n{Path(summary_file).name}', fontsize=16)\n",
    "    \n",
    "    metrics_to_plot = [\n",
    "        ('rewards_mean', 'Avg Rewards per Episode', axes[0, 0]),\n",
    "        ('stockouts_mean', 'Avg Stockouts per Episode', axes[0, 1]),\n",
    "        ('waste_mean', 'Avg Waste per Episode', axes[0, 2]),\n",
    "        ('critic_loss_mean', 'Avg Critic Loss per Episode', axes[1, 0]),\n",
    "        ('entropy_adjusted_mean', 'Avg Entropy per Episode', axes[1, 1]),\n",
    "        ('actor_loss_mean', 'Avg Actor Loss per Episode', axes[1, 2]),\n",
    "    ]\n",
    "    \n",
    "    for col, title, ax in metrics_to_plot:\n",
    "        if col in df.columns:\n",
    "            ax.plot(df['episode'], df[col], linewidth=2, marker='o', markersize=4, alpha=0.8)\n",
    "            ax.set_title(title, fontweight='bold')\n",
    "            ax.set_xlabel('Episode')\n",
    "            ax.set_ylabel('Value')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(summary_file.replace('.json', '_plot.png'), dpi=150, bbox_inches='tight')\n",
    "    print(f\"‚úì Summary plot saved: {summary_file.replace('.json', '_plot.png')}\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTotal episodes trained: {len(df)}\")\n",
    "\n",
    "print(\"Plot functions defined successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
