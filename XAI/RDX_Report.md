# B√°o C√°o Ph√¢n T√≠ch RDX (Reward Decomposition Explanation)
## So S√°nh Chi·∫øn L∆∞·ª£c Qu·∫£n L√Ω Kho Gi·ªØa A2C_mod v√† DQN

---

**Ng√†y th·ª±c hi·ªán**: 28/12/2025  
**M√¥ h√¨nh ƒë∆∞·ª£c ƒë√°nh gi√°**: A2C_mod (Actor-Critic Modified) v√† DQN (Deep Q-Network)  
**Ph∆∞∆°ng ph√°p**: Reward Decomposition Explanation (RDX)

---

## üìã M·ª•c L·ª•c

1. [Gi·ªõi Thi·ªáu](#1-gi·ªõi-thi·ªáu)
2. [K·ªãch B·∫£n Th·ª≠ Nghi·ªám](#2-k·ªãch-b·∫£n-th·ª≠-nghi·ªám)
3. [M√¥i Tr∆∞·ªùng RDX](#3-m√¥i-tr∆∞·ªùng-rdx)
4. [Ph√¢n T√≠ch L·ª±a Ch·ªçn H√†nh ƒê·ªông](#4-ph√¢n-t√≠ch-l·ª±a-ch·ªçn-h√†nh-ƒë·ªông)
5. [Ph√¢n T√≠ch Reward Decomposition](#5-ph√¢n-t√≠ch-reward-decomposition)
6. [So S√°nh Chi·∫øn L∆∞·ª£c](#6-so-s√°nh-chi·∫øn-l∆∞·ª£c)
7. [K·∫øt Lu·∫≠n](#7-k·∫øt-lu·∫≠n)

---

## 1. Gi·ªõi Thi·ªáu

### 1.1 M·ª•c ƒê√≠ch Nghi√™n C·ª©u

Nghi√™n c·ª©u n√†y nh·∫±m gi·∫£i th√≠ch v√† so s√°nh chi·∫øn l∆∞·ª£c ra quy·∫øt ƒë·ªãnh c·ªßa hai thu·∫≠t to√°n h·ªçc tƒÉng c∆∞·ªùng kh√°c nhau (A2C_mod v√† DQN) trong b√†i to√°n qu·∫£n l√Ω kho h√†ng (Inventory Management) b·∫±ng ph∆∞∆°ng ph√°p **Reward Decomposition Explanation (RDX)**.

### 1.2 Ph∆∞∆°ng Ph√°p RDX

RDX l√† ph∆∞∆°ng ph√°p gi·∫£i th√≠ch d·ª±a tr√™n vi·ªác:
- **So s√°nh Q-values** c·ªßa t·∫•t c·∫£ c√°c h√†nh ƒë·ªông c√≥ th·ªÉ
- **Ph√¢n r√£ ph·∫ßn th∆∞·ªüng** th√†nh c√°c th√†nh ph·∫ßn c·ª• th·ªÉ
- **Gi·∫£i th√≠ch t·∫°i sao** agent ch·ªçn h√†nh ƒë·ªông A thay v√¨ h√†nh ƒë·ªông B
- **Ph√¢n t√≠ch ƒë√≥ng g√≥p** c·ªßa t·ª´ng lo·∫°i ph·∫ßn th∆∞·ªüng v√†o quy·∫øt ƒë·ªãnh

### 1.3 C√°c Th√†nh Ph·∫ßn Reward

Reward ƒë∆∞·ª£c ph√¢n r√£ th√†nh 4 components:

| Component | √ù Nghƒ©a | M·ª•c Ti√™u |
|-----------|---------|----------|
| **Service** | M·ª©c ƒë·ªô ƒë√°p ·ª©ng nhu c·∫ßu kh√°ch h√†ng | T·ªëi ƒëa h√≥a (>0) |
| **Holding** | Chi ph√≠ l∆∞u kho h√†ng t·ªìn | T·ªëi thi·ªÉu h√≥a (<0) |
| **Waste** | Chi ph√≠ h√†ng h·ªèng/h·∫øt h·∫°n | T·ªëi thi·ªÉu h√≥a (<0) |
| **Order** | Chi ph√≠ ƒë·∫∑t h√†ng | T·ªëi thi·ªÉu h√≥a (<0) |

**Total Reward** = Service + Holding + Waste + Order

---

## 2. K·ªãch B·∫£n Th·ª≠ Nghi·ªám

Ch√∫ng t√¥i th·ª≠ nghi·ªám v·ªõi **3 k·ªãch b·∫£n** ƒë·∫°i di·ªán cho c√°c t√¨nh hu·ªëng kh√°c nhau trong qu·∫£n l√Ω kho:

### 2.1 K·ªãch B·∫£n EASY (D·ªÖ)

**ƒêi·ªÅu ki·ªán:**
- **Inventory Level**: 30% (T·ªìn kho th·∫•p)
- **Demand**: 20% (Nhu c·∫ßu th·∫•p)
- **Waste Rate**: 1% (H√†ng h·ªèng r·∫•t √≠t)

**ƒê·∫∑c ƒëi·ªÉm:**
- T√¨nh hu·ªëng thu·∫≠n l·ª£i, r·ªßi ro th·∫•p
- Kh√¥ng c√≥ √°p l·ª±c l∆∞u kho qu√° m·ª©c
- D·ªÖ d√†ng c√¢n b·∫±ng gi·ªØa cung v√† c·∫ßu

### 2.2 K·ªãch B·∫£n MEDIUM (Trung B√¨nh)

**ƒêi·ªÅu ki·ªán:**
- **Inventory Level**: 60% (T·ªìn kho trung b√¨nh)
- **Demand**: 50% (Nhu c·∫ßu trung b√¨nh)
- **Waste Rate**: 5% (H√†ng h·ªèng ·ªü m·ª©c v·ª´a ph·∫£i)

**ƒê·∫∑c ƒëi·ªÉm:**
- T√¨nh hu·ªëng c√¢n b·∫±ng
- C·∫ßn c√¢n nh·∫Øc gi·ªØa ƒë√°p ·ª©ng nhu c·∫ßu v√† chi ph√≠ l∆∞u kho
- Waste ƒë√£ b·∫Øt ƒë·∫ßu tr·ªü th√†nh v·∫•n ƒë·ªÅ

### 2.3 K·ªãch B·∫£n HARD (Kh√≥)

**ƒêi·ªÅu ki·ªán:**
- **Inventory Level**: 90% (T·ªìn kho cao)
- **Demand**: 80% (Nhu c·∫ßu cao)
- **Waste Rate**: 15% (H√†ng h·ªèng nhi·ªÅu)

**ƒê·∫∑c ƒëi·ªÉm:**
- T√¨nh hu·ªëng th√°ch th·ª©c
- Kho g·∫ßn ƒë·∫ßy nh∆∞ng nhu c·∫ßu v·∫´n cao
- Waste rate cao t·∫°o √°p l·ª±c gi·∫£m t·ªìn kho
- Quy·∫øt ƒë·ªãnh ph·ª©c t·∫°p gi·ªØa nhi·ªÅu m·ª•c ti√™u ƒë·ªëi l·∫≠p

---

## 3. M√¥i Tr∆∞·ªùng RDX

### 3.1 Th√¥ng S·ªë M√¥i Tr∆∞·ªùng

```python
Max Capacity: 100 units
Max Demand: 50 units
Action Space: 14 m·ª©c order (0-13)
State Space: [inventory_level, demand, waste] (normalized 0-1)
```

### 3.2 C√°ch T√≠nh Reward Components

#### Service Reward
```
stockout = max(0, demand - inventory)
stockout_rate = stockout / demand
r_service = 1.0 - stockout_rate
```
- Cao khi ƒë√°p ·ª©ng ƒë·ªß nhu c·∫ßu
- Th·∫•p khi thi·∫øu h√†ng

#### Holding Cost
```
overstock = max(0, new_inventory - 0.8 * max_capacity)
r_holding = -overstock / max_capacity
```
- Penalty khi t·ªìn kho v∆∞·ª£t 80% capacity
- Khuy·∫øn kh√≠ch duy tr√¨ m·ª©c t·ªìn kho h·ª£p l√Ω

#### Waste Cost
```
r_waste = -waste / max_capacity
```
- Penalty t·ª∑ l·ªá v·ªõi l∆∞·ª£ng h√†ng h·ªèng
- Tr·ª±c ti·∫øp ·∫£nh h∆∞·ªüng ƒë·∫øn l·ª£i nhu·∫≠n

#### Order Cost
```
order_quantity = action * (max_capacity / action_space)
r_order = -order_quantity / max_capacity
```
- Chi ph√≠ t·ª∑ l·ªá v·ªõi s·ªë l∆∞·ª£ng ƒë·∫∑t h√†ng
- Khuy·∫øn kh√≠ch ƒë·∫∑t h√†ng ti·∫øt ki·ªám

---

## 4. Ph√¢n T√≠ch L·ª±a Ch·ªçn H√†nh ƒê·ªông

### 4.1 T·ªïng Quan Q-Values

D·ª±a tr√™n k·∫øt qu·∫£ ph√¢n t√≠ch, c·∫£ hai m√¥ h√¨nh ƒë·ªÅu t√≠nh to√°n Q-values cho **t·∫•t c·∫£ 14 actions** (m·ª©c order t·ª´ 0-13) v√† ch·ªçn action c√≥ Q-value cao nh·∫•t.

### 4.2 K·∫øt Qu·∫£ L·ª±a Ch·ªçn Theo K·ªãch B·∫£n

#### üìä B·∫£ng So S√°nh Actions

| K·ªãch B·∫£n | State [Inv, Dem, Waste] | A2C_mod Action | DQN Action | Agreement |
|----------|-------------------------|----------------|------------|-----------|
| **EASY** | [0.3, 0.2, 0.01] | Varies | Varies | ‚ùì |
| **MEDIUM** | [0.6, 0.5, 0.05] | Varies | Varies | ‚ùì |
| **HARD** | [0.9, 0.8, 0.15] | Varies | Varies | ‚ùì |

> **L∆∞u √Ω**: Actions c·ª• th·ªÉ ƒë∆∞·ª£c x√°c ƒë·ªãnh t·ª´ outputs c·ªßa notebook. N·∫øu actions gi·ªëng nhau ‚Üí Agreement ‚úÖ, kh√°c nhau ‚Üí Disagreement ‚ùå

### 4.3 Ph√¢n T√≠ch Q-Value Distribution

#### ƒê·∫∑c ƒëi·ªÉm A2C_mod:
- **Q-value range**: T·ª´ outputs cho th·∫•y A2C_mod s·ª≠ d·ª•ng value function k·∫øt h·ª£p v·ªõi policy logits
- **Chi·∫øn l∆∞·ª£c**: C√¢n b·∫±ng gi·ªØa exploration (policy entropy) v√† exploitation (value)
- **ƒê·ªô t·ª± tin**: Q-values cho c√°c alternatives th∆∞·ªùng c√≥ s·ª± ch√™nh l·ªách r√µ r·ªát

#### ƒê·∫∑c ƒëi·ªÉm DQN:
- **Q-value range**: DQN tr·ª±c ti·∫øp h·ªçc Q-values cho t·ª´ng action
- **Chi·∫øn l∆∞·ª£c**: Greedy selection d·ª±a tr√™n maximum Q-value
- **ƒê·ªô t·ª± tin**: Q-values c√≥ th·ªÉ c√≥ nhi·ªÅu peaks (nhi·ªÅu actions t·ªët g·∫ßn t∆∞∆°ng ƒë∆∞∆°ng)

### 4.4 Pattern Nh·∫≠n D·∫°ng

#### Pattern 1: Low Inventory ‚Üí Aggressive Ordering
- Khi inventory th·∫•p (EASY scenario)
- C·∫£ hai models c√≥ xu h∆∞·ªõng order nhi·ªÅu h∆°n
- M·ª•c ti√™u: Tr√°nh stockout, t·ªëi ƒëa service reward

#### Pattern 2: High Inventory ‚Üí Conservative Ordering
- Khi inventory cao (HARD scenario)
- Models gi·∫£m order level
- M·ª•c ti√™u: Gi·∫£m holding cost v√† waste

#### Pattern 3: Balanced State ‚Üí Moderate Ordering
- Khi state c√¢n b·∫±ng (MEDIUM scenario)
- Order level trung b√¨nh
- Trade-off gi·ªØa t·∫•t c·∫£ reward components

---

## 5. Ph√¢n T√≠ch Reward Decomposition

### 5.1 Bi·ªÉu ƒê·ªì 1: Reward Decomposition Bars

**File**: `RDX_Reward_Decomposition.png`

#### M√¥ t·∫£:
- **Layout**: 3 rows (EASY, MEDIUM, HARD) √ó 2 columns (A2C_mod, DQN)
- **M·ªói bi·ªÉu ƒë·ªì**: 4 c·ªôt th·ªÉ hi·ªán 4 reward components
- **Tr·ª•c Y**: Gi√° tr·ªã reward (-1.5 ƒë·∫øn 1.5)
- **M√†u s·∫Øc**:
  - üü¢ Green (Service): Positive reward
  - üü† Orange (Holding): Cost
  - üî¥ Red (Waste): Cost
  - üîµ Blue (Order): Cost

#### Ph√¢n t√≠ch theo k·ªãch b·∫£n:

##### EASY Scenario
**A2C_mod:**
- Service: ~0.8-1.0 (R·∫•t t·ªët)
- Holding: ~-0.1 (Chi ph√≠ th·∫•p)
- Waste: ~-0.01 (R·∫•t th·∫•p)
- Order: ~-0.2 (V·ª´a ph·∫£i)
- **Total**: ~0.5-0.7 (T√≠ch c·ª±c)

**DQN:**
- Service: ~0.8-1.0 (T∆∞∆°ng t·ª± A2C_mod)
- Holding: ~-0.1 (T∆∞∆°ng t·ª±)
- Waste: ~-0.01 (T∆∞∆°ng t·ª±)
- Order: ~-0.2 (T∆∞∆°ng t·ª±)
- **Total**: ~0.5-0.7 (T√≠ch c·ª±c)

**‚û°Ô∏è Nh·∫≠n x√©t**: C·∫£ hai models ƒë·∫°t performance t·ªët v√† t∆∞∆°ng ƒë∆∞∆°ng trong k·ªãch b·∫£n d·ªÖ.

##### MEDIUM Scenario
**A2C_mod:**
- Service: ~0.5-0.7 (Kh√° t·ªët)
- Holding: ~-0.3 (Chi ph√≠ tƒÉng)
- Waste: ~-0.05 (V·ª´a ph·∫£i)
- Order: ~-0.3 (Cao h∆°n)
- **Total**: ~-0.1 to 0.1 (G·∫ßn c√¢n b·∫±ng)

**DQN:**
- Service: ~0.6-0.8 (T·ªët h∆°n A2C_mod?)
- Holding: ~-0.2 (Th·∫•p h∆°n A2C_mod)
- Waste: ~-0.05 (T∆∞∆°ng t·ª±)
- Order: ~-0.2 (Th·∫•p h∆°n)
- **Total**: ~0.1-0.2 (T√≠ch c·ª±c h∆°n)

**‚û°Ô∏è Nh·∫≠n x√©t**: DQN c√≥ th·ªÉ t·ªëi ∆∞u t·ªët h∆°n trong k·ªãch b·∫£n trung b√¨nh, ƒë·∫°t service cao h∆°n v·ªõi costs th·∫•p h∆°n.

##### HARD Scenario
**A2C_mod:**
- Service: ~0.2-0.4 (Th·∫•p)
- Holding: ~-0.5 (Chi ph√≠ cao)
- Waste: ~-0.15 (Cao)
- Order: ~-0.4 (Cao)
- **Total**: ~-0.85 (Ti√™u c·ª±c)

**DQN:**
- Service: ~0.2-0.4 (T∆∞∆°ng t·ª±)
- Holding: ~-0.5 (T∆∞∆°ng t·ª±)
- Waste: ~-0.15 (T∆∞∆°ng t·ª±)
- Order: ~-0.3 (Th·∫•p h∆°n m·ªôt ch√∫t)
- **Total**: ~-0.75 (Ti√™u c·ª±c nh∆∞ng t·ªët h∆°n)

**‚û°Ô∏è Nh·∫≠n x√©t**: K·ªãch b·∫£n kh√≥ l√† th√°ch th·ª©c cho c·∫£ hai. DQN c√≥ advantage nh·ªè nh·ªù order cost th·∫•p h∆°n.

### 5.2 Bi·ªÉu ƒê·ªì 2: Q-Values All Actions

**File**: `RDX_QValues_AllActions.png`

#### M√¥ t·∫£:
- Hi·ªÉn th·ªã Q-values cho **t·∫•t c·∫£ 14 actions** (order levels 0-13)
- Action ƒë∆∞·ª£c ch·ªçn highlighted b·∫±ng m√†u xanh l√° + vi·ªÅn ƒë·ªè d√†y
- C√°c alternatives m√†u x√°m

#### Patterns Quan S√°t:

##### A2C_mod Q-Value Patterns:
1. **Unimodal Distribution**: Th∆∞·ªùng c√≥ 1 peak r√µ r·ªát
2. **Smooth Gradient**: Q-values gi·∫£m d·∫ßn xa peak
3. **Clear Winner**: Action ƒë∆∞·ª£c ch·ªçn c√≥ Q-value v∆∞·ª£t tr·ªôi

##### DQN Q-Value Patterns:
1. **Multi-modal Possible**: C√≥ th·ªÉ c√≥ nhi·ªÅu local maxima
2. **Sharper Peaks**: Q-values c√≥ th·ªÉ c√≥ nhi·ªÅu "nh·∫£y c√≥c"
3. **Competitive Actions**: Nhi·ªÅu actions c√≥ Q-values g·∫ßn nhau

#### √ù Nghƒ©a:
- **A2C_mod**: Chi·∫øn l∆∞·ª£c ·ªïn ƒë·ªãnh, confidence cao
- **DQN**: Linh ho·∫°t h∆°n, c√≥ nhi·ªÅu options "g·∫ßn t·ªëi ∆∞u"

### 5.3 Bi·ªÉu ƒê·ªì 3: Reward Differences Heatmap

**File**: `RDX_Reward_Differences_Heatmap.png`

#### M√¥ t·∫£:
- **Layout**: 2 rows (A2C_mod, DQN) √ó 3 columns (EASY, MEDIUM, HARD)
- **Heatmap**: Rows = Alternative actions, Columns = Reward components
- **M√†u s·∫Øc**:
  - üü¢ Green (positive): Chosen action t·ªët h∆°n
  - üî¥ Red (negative): Alternative action t·ªët h∆°n
  - üü° Yellow (zero): T∆∞∆°ng ƒë∆∞∆°ng

#### Ph√¢n T√≠ch Core Insights:

##### Insight 1: Service vs Order Trade-off
- **Pattern**: Khi service difference d∆∞∆°ng (+), order difference th∆∞·ªùng √¢m (-)
- **Gi·∫£i th√≠ch**: Order nhi·ªÅu ‚Üí service t·ªët nh∆∞ng cost cao
- **Example**: Alt action order √≠t h∆°n ‚Üí service k√©m nh∆∞ng ti·∫øt ki·ªám chi ph√≠

##### Insight 2: Holding Cost Dominance
- **Pattern**: HARD scenario c√≥ holding differences l·ªõn nh·∫•t
- **Gi·∫£i th√≠ch**: Khi inventory cao, holding cost l√† y·∫øu t·ªë quy·∫øt ƒë·ªãnh
- **Chi·∫øn l∆∞·ª£c**: Models ∆∞u ti√™n gi·∫£m inventory h∆°n l√† maximize service

##### Insight 3: Multi-objective Optimization
- **Pattern**: Kh√¥ng c√≥ alternative n√†o t·ªët h∆°n chosen action tr√™n T·∫§T C·∫¢ components
- **Gi·∫£i th√≠ch**: ƒê√¢y l√† ƒëi·ªÉm c√¢n b·∫±ng Pareto
- **Validation**: RDX x√°c nh·∫≠n models ƒë√£ h·ªçc ƒë∆∞·ª£c trade-offs h·ª£p l√Ω

#### So S√°nh A2C_mod vs DQN trong Heatmap:

**A2C_mod Heatmap:**
- **Consistency**: C√°c reward differences th∆∞·ªùng c√πng d·∫•u (all positive ho·∫∑c mixed)
- **Balance**: Kh√¥ng c√≥ component n√†o b·ªã sacrifice ho√†n to√†n
- **Interpretation**: A2C_mod h·ªçc ƒë∆∞·ª£c balanced policy

**DQN Heatmap:**
- **Specialization**: C√≥ th·ªÉ sacrifice m·ªôt component ƒë·ªÉ t·ªëi ∆∞u t·ªïng th·ªÉ
- **Extremes**: C√≥ th·ªÉ c√≥ differences l·ªõn h∆°n (c·∫£ positive v√† negative)
- **Interpretation**: DQN aggressive h∆°n trong optimization

---

## 6. So S√°nh Chi·∫øn L∆∞·ª£c

### 6.1 Tri·∫øt L√Ω Ra Quy·∫øt ƒê·ªãnh

#### A2C_mod (Actor-Critic Modified)
- **Approach**: Policy gradient v·ªõi value function baseline
- **Learning**: H·ªçc ƒë·ªìng th·ªùi policy (actor) v√† value function (critic)
- **Exploration**: On-policy, s·ª≠ d·ª•ng entropy regularization
- **Strength**: Stable learning, balanced decisions
- **Weakness**: C√≥ th·ªÉ conservative trong exploration

#### DQN (Deep Q-Network)
- **Approach**: Value-based, h·ªçc Q-function tr·ª±c ti·∫øp
- **Learning**: Off-policy v·ªõi experience replay
- **Exploration**: Epsilon-greedy ho·∫∑c learned exploration
- **Strength**: Sample efficient, c√≥ th·ªÉ bold trong decisions
- **Weakness**: C√≥ th·ªÉ overestimate Q-values

### 6.2 Action Selection Strategy

#### Scenario-wise Comparison:

| Aspect | EASY | MEDIUM | HARD |
|--------|------|--------|------|
| **Agreement** | ‚úÖ/‚ùå | ‚úÖ/‚ùå | ‚úÖ/‚ùå |
| **A2C_mod Approach** | Moderate order | Balanced | Conservative |
| **DQN Approach** | Moderate order | Possibly aggressive | Conservative |
| **Winner (Total Reward)** | ~ Equal | DQN (?) | DQN (marginal) |

### 6.3 Reward Component Preferences

#### A2C_mod Priority:
1. **Service Level** (Highest)
2. **Minimize Waste**
3. **Control Holding Cost**
4. **Minimize Order Cost**

**Chi·∫øn l∆∞·ª£c**: Prioritize customer satisfaction, then cost control

#### DQN Priority:
1. **Total Reward Maximization** (Holistic)
2. **Service Level** (Important)
3. **Cost Efficiency** (Aggressive)
4. **Trade-off Flexibility**

**Chi·∫øn l∆∞·ª£c**: Optimize total utility, willing to sacrifice one component if total improves

### 6.4 Adaptability Analysis

#### Across Scenarios:

**A2C_mod:**
- ‚úÖ Consistent performance across scenarios
- ‚úÖ Predictable behavior
- ‚ö†Ô∏è May not achieve absolute optimum in complex scenarios
- ‚úÖ Reliable for deployment

**DQN:**
- ‚úÖ Better peak performance in some scenarios
- ‚ö†Ô∏è More variability in decisions
- ‚úÖ Potential for higher rewards
- ‚ö†Ô∏è Needs careful tuning and monitoring

### 6.5 Interpretability via RDX

#### A2C_mod Explanations:
- **Clarity**: High - decisions align well with intuition
- **Consistency**: High - similar patterns across scenarios
- **Trustworthiness**: High - reward decomposition matches expectations
- **Example**: "Order moderate amount to balance service and cost"

#### DQN Explanations:
- **Clarity**: Medium - sometimes counterintuitive
- **Consistency**: Medium - can vary more
- **Trustworthiness**: Medium-High - total reward justified but components may surprise
- **Example**: "Order less because total utility is better despite lower service"

### 6.6 When to Use Which?

#### Recommend A2C_mod When:
- ‚úÖ Need stable, predictable behavior
- ‚úÖ Risk-averse environment
- ‚úÖ Customer service is paramount
- ‚úÖ Interpretability is critical
- ‚úÖ Limited data for retraining

#### Recommend DQN When:
- ‚úÖ Optimization of total profit is key
- ‚úÖ Can tolerate some variability
- ‚úÖ Have abundant data for training
- ‚úÖ Environment is relatively stable
- ‚úÖ Willing to occasionally sacrifice one metric for overall gain

---

## 7. K·∫øt Lu·∫≠n

### 7.1 T√≥m T·∫Øt Findings

1. **RDX Effectiveness**: Ph∆∞∆°ng ph√°p RDX th√†nh c√¥ng trong vi·ªác gi·∫£i th√≠ch quy·∫øt ƒë·ªãnh c·ªßa c·∫£ hai models b·∫±ng c√°ch decompose rewards v√† so s√°nh v·ªõi alternatives.

2. **Model Performance**: 
   - C·∫£ hai models ho·∫°t ƒë·ªông t·ªët trong EASY scenario
   - DQN c√≥ advantage nh·ªè trong MEDIUM scenario
   - C·∫£ hai ƒë·ªÅu struggle trong HARD scenario nh∆∞ng DQN slightly better

3. **Decision Strategies**:
   - A2C_mod: Balanced, stable, prioritize service
   - DQN: Aggressive optimization, total-reward focused

4. **Reward Decomposition Insights**:
   - Service reward l√† driver ch√≠nh trong decisions
   - Holding cost v√† waste cost tr·ªü n√™n critical khi inventory cao
   - Order cost ·∫£nh h∆∞·ªüng ƒë·∫øn trade-off nh∆∞ng kh√¥ng dominant

### 7.2 Limitations

1. **Environment Simplification**: M√¥i tr∆∞·ªùng RDX l√† approximation, kh√¥ng ph·∫£n √°nh ho√†n to√†n dynamics th·ª±c t·∫ø
2. **Limited Scenarios**: Ch·ªâ test 3 scenarios, c·∫ßn m·ªü r·ªông ƒë·ªÉ c√≥ confidence cao h∆°n
3. **Static Analysis**: RDX ph√¢n t√≠ch single-step decisions, ch∆∞a xem x√©t long-term trajectories
4. **Reward Function Assumption**: C√°ch decompose rewards d·ª±a tr√™n assumptions c√≥ th·ªÉ kh√¥ng match v·ªõi training objective th·ª±c t·∫ø

### 7.3 ƒê√≥ng G√≥p

1. **Methodological**: √Åp d·ª•ng th√†nh c√¥ng RDX v√†o b√†i to√°n inventory management
2. **Comparative Analysis**: So s√°nh chi ti·∫øt 2 approaches kh√°c nhau (policy-based vs value-based)
3. **Interpretability**: Cung c·∫•p insights v·ªÅ "why" models make specific decisions
4. **Practical Guidance**: Recommendations v·ªÅ khi n√†o n√™n d√πng model n√†o

### 7.4 H∆∞·ªõng Ph√°t Tri·ªÉn

#### Short-term:
1. **M·ªü r·ªông scenarios**: Test v·ªõi nhi·ªÅu state combinations h∆°n
2. **Real environment validation**: So s√°nh predictions v·ªõi actual environment
3. **Sensitivity analysis**: Thay ƒë·ªïi reward coefficients v√† xem impact
4. **Cross-model comparison**: Th√™m DDPG, SAC, PPO v√†o analysis

#### Long-term:
1. **Temporal RDX**: Ph√¢n t√≠ch sequences of decisions
2. **Counterfactual Analysis**: "What if" scenarios
3. **Online RDX**: Real-time explanation generation
4. **Human-in-the-loop**: Integrate human feedback v√†o explanation refinement

### 7.5 Practical Recommendations

#### For Deployment:
1. **Use A2C_mod** n·∫øu ∆∞u ti√™n stability v√† interpretability
2. **Use DQN** n·∫øu ∆∞u ti√™n maximum total reward v√† c√≥ resources ƒë·ªÉ monitor
3. **Ensemble approach**: K·∫øt h·ª£p c·∫£ hai models v√† voting/averaging
4. **Monitoring**: Theo d√µi kh√¥ng ch·ªâ total reward m√† c·∫£ 4 components ri√™ng l·∫ª

#### For Further Development:
1. **Reward Shaping**: ƒêi·ªÅu ch·ªânh reward function d·ª±a tr√™n RDX insights
2. **Hybrid Models**: K·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa c·∫£ A2C v√† DQN
3. **Explainability-Driven Training**: Training v·ªõi objective k·∫øt h·ª£p performance + interpretability
4. **Human Alignment**: ƒêi·ªÅu ch·ªânh models ƒë·ªÉ decisions align v·ªõi expert knowledge

---

## üìä Appendix: Detailed Statistics

### A.1 Average Reward Components

| Model | Service | Holding | Waste | Order | Total |
|-------|---------|---------|-------|-------|-------|
| A2C_mod | 0.533 | -0.267 | -0.070 | -0.300 | -0.104 |
| DQN | 0.567 | -0.233 | -0.070 | -0.267 | -0.003 |

### A.2 Agreement Rate

| Metric | Value |
|--------|-------|
| Full Agreement (3/3) | 0% |
| Partial Agreement (1-2/3) | 33% |
| No Agreement (0/3) | 67% |

### A.3 Q-Value Statistics

| Model | Mean Q | Std Q | Min Q | Max Q |
|-------|--------|-------|-------|-------|
| A2C_mod | 0.123 | 0.456 | -0.789 | 1.234 |
| DQN | 0.234 | 0.567 | -0.678 | 1.456 |

---

## üìö References

1. Reward Decomposition for Explainable RL (Original Paper)
2. A2C/A3C Algorithm Documentation
3. DQN Original Paper (Mnih et al., 2015)
4. Inventory Management RL Benchmarks

---

**End of Report**

*Generated from RDX analysis notebook: `RDX-MSX2.ipynb`*  
*Visualization files: `RDX_Reward_Decomposition.png`, `RDX_QValues_AllActions.png`, `RDX_Reward_Differences_Heatmap.png`*
