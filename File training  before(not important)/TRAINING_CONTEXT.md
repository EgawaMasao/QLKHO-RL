# ğŸ¯ Ngá»¯ Cáº£nh Mong Muá»‘n Ä‘á»ƒ Huáº¥n Luyá»‡n Agent Q-Learning

## ğŸ“‹ Tá»•ng quan bÃ i toÃ¡n

### Váº¥n Ä‘á» kinh doanh
Má»™t nhÃ  bÃ¡n láº» cáº§n quáº£n lÃ½ tá»“n kho sáº£n pháº©m Ä‘á»ƒ:
- âœ… **ÄÃ¡p á»©ng nhu cáº§u khÃ¡ch hÃ ng** (trÃ¡nh háº¿t hÃ ng)
- âœ… **Giáº£m thiá»ƒu chi phÃ­** (lÆ°u trá»¯ + Ä‘áº·t hÃ ng + thiáº¿u hÃ ng)
- âœ… **Ra quyáº¿t Ä‘á»‹nh tá»± Ä‘á»™ng** má»—i ngÃ y: CÃ³ nÃªn Ä‘áº·t hÃ ng khÃ´ng?

### Má»¥c tiÃªu há»c mÃ¡y
Train má»™t **agent thÃ´ng minh** sá»­ dá»¥ng **Q-Learning** Ä‘á»ƒ:
- Há»c tá»« kinh nghiá»‡m (trial & error)
- Tá»± Ä‘á»™ng quyáº¿t Ä‘á»‹nh **khi nÃ o Ä‘áº·t hÃ ng** vÃ  **Ä‘áº·t bao nhiÃªu**
- Tá»‘i Æ°u hÃ³a chi phÃ­ dÃ i háº¡n (long-term cost minimization)

---

## ğŸ­ MÃ´i trÆ°á»ng kinh doanh (Environment)

### 1. ThÃ´ng sá»‘ sáº£n pháº©m
```python
mu = 3.0           # Nhu cáº§u trung bÃ¬nh: 3 sáº£n pháº©m/ngÃ y
sigma = 1.0        # Äá»™ biáº¿n Ä‘á»™ng nhu cáº§u: Â±1 sáº£n pháº©m
LT = 1             # Lead time: HÃ ng Ä‘áº¿n sau 1 ngÃ y ká»ƒ tá»« khi Ä‘áº·t
```

**Ã nghÄ©a thá»±c táº¿:**
- KhÃ¡ch hÃ ng mua trung bÃ¬nh 3 sáº£n pháº©m/ngÃ y
- Nhu cáº§u dao Ä‘á»™ng: CÃ³ ngÃ y 1-2 sáº£n pháº©m, cÃ³ ngÃ y 4-5 sáº£n pháº©m
- NhÃ  cung cáº¥p giao hÃ ng sau 1 ngÃ y â†’ Pháº£i dá»± Ä‘oÃ¡n trÆ°á»›c!

### 2. Cáº¥u trÃºc chi phÃ­
```python
O = 50.0 â‚¬         # Chi phÃ­ Ä‘áº·t hÃ ng (ordering cost)
                   # - PhÃ­ váº­n chuyá»ƒn
                   # - Chi phÃ­ xá»­ lÃ½ Ä‘Æ¡n hÃ ng
                   # - Chi phÃ­ hÃ nh chÃ­nh

h = 10/365 â‚¬/ngÃ y  # Chi phÃ­ lÆ°u trá»¯ (holding cost)
                   # - Tiá»n thuÃª kho
                   # - Báº£o hiá»ƒm
                   # - Hao há»¥t, máº¥t mÃ¡t

b = 20.0 â‚¬         # Chi phÃ­ thiáº¿u hÃ ng (backorder cost)
                   # - Máº¥t lÃ²ng tin khÃ¡ch hÃ ng
                   # - Pháº£i giao hÃ ng kháº©n cáº¥p
                   # - Máº¥t doanh thu

q = 6              # Sá»‘ lÆ°á»£ng Ä‘áº·t má»—i láº§n (order quantity)
```

**Trade-offs kinh doanh:**
- **Äáº·t hÃ ng thÆ°á»ng xuyÃªn:** Chi phÃ­ ordering cao (O = 50â‚¬)
- **Tá»“n kho nhiá»u:** Chi phÃ­ lÆ°u trá»¯ cao (h Ã— inventory)
- **Tá»“n kho Ã­t:** Nguy cÆ¡ háº¿t hÃ ng â†’ chi phÃ­ thiáº¿u hÃ ng (b Ã— shortage)

### 3. Biáº¿n tráº¡ng thÃ¡i (State)
```python
State = Inventory Position (IP)
IP = On-hand inventory + Orders in transit - Backorders
Range: [-20, 20] â†’ 41 states
```

**Ã nghÄ©a cÃ¡c tráº¡ng thÃ¡i:**
- **IP < 0:** Thiáº¿u hÃ ng (backorders) - KhÃ¡ch Ä‘Ã£ Ä‘áº·t nhÆ°ng chÆ°a cÃ³ hÃ ng
- **IP = 0:** Vá»«a Ä‘á»§ (sáº¯p háº¿t)
- **IP = 3-6:** Tá»“n kho an toÃ n (safety stock)
- **IP > 10:** Tá»“n kho dÆ° thá»«a (overstock)

### 4. KhÃ´ng gian hÃ nh Ä‘á»™ng (Action)
```python
Action 0: KhÃ´ng Ä‘áº·t hÃ ng (Do nothing)
Action 1: Äáº·t hÃ ng q=6 sáº£n pháº©m
```

**Quyáº¿t Ä‘á»‹nh hÃ ng ngÃ y:**
- Má»—i sÃ¡ng, agent xem IP hiá»‡n táº¡i
- Chá»n: Äáº·t hÃ ng 6 sáº£n pháº©m HOáº¶C chá» thÃªm?

---

## ğŸ“ Quy trÃ¬nh há»c (Learning Process)

### 1. Exploration vs Exploitation
```python
epsilon_start = 0.3    # 30% explore á»Ÿ Ä‘áº§u
epsilon_end = 0.01     # 1% explore á»Ÿ cuá»‘i
epsilon_decay = 0.998  # Giáº£m dáº§n qua episodes
```

**Giai Ä‘oáº¡n há»c:**

#### Phase 1: Early Learning (Episodes 1-500, Îµ â‰ˆ 0.3 â†’ 0.15)
- **Behavior:** Agent thá»­ nhiá»u hÃ nh Ä‘á»™ng khÃ¡c nhau
- **Goal:** KhÃ¡m phÃ¡ táº¥t cáº£ states vÃ  consequences
- **Example:** Thá»­ Ä‘áº·t hÃ ng khi IP=10 (tá»“n cao) â†’ PhÃ¡t hiá»‡n chi phÃ­ cao

#### Phase 2: Intermediate (Episodes 500-1500, Îµ â‰ˆ 0.15 â†’ 0.03)
- **Behavior:** Agent báº¯t Ä‘áº§u khai thÃ¡c kiáº¿n thá»©c Ä‘Ã£ há»c
- **Goal:** Fine-tune policy, kiá»ƒm tra cÃ¡c states Ã­t gáº·p
- **Example:** Há»c Ä‘Æ°á»£c nÃªn Ä‘áº·t hÃ ng khi IPâ‰¤3

#### Phase 3: Exploitation (Episodes 1500-2000, Îµ â‰ˆ 0.03 â†’ 0.01)
- **Behavior:** Chá»§ yáº¿u follow learned policy (99%)
- **Goal:** Consolidate knowledge, final refinements
- **Example:** Policy á»•n Ä‘á»‹nh, chá»‰ explore 1% Ä‘á»ƒ avoid local optima

### 2. Reward Signal (Feedback)
```python
reward = -total_cost / REWARD_SCALE

total_cost = ordering_cost + holding_cost + backorder_cost
           = OÂ·I[order] + hÂ·max(inventory,0) + bÂ·max(-inventory,0)
```

**CÃ¡ch agent há»c tá»« reward:**
- **Reward cao (gáº§n 0):** Chi phÃ­ tháº¥p â†’ HÃ nh Ä‘á»™ng tá»‘t
- **Reward tháº¥p (ráº¥t Ã¢m):** Chi phÃ­ cao â†’ HÃ nh Ä‘á»™ng xáº¥u

**VÃ­ dá»¥:**
```
Scenario A: IP=2 â†’ Äáº·t hÃ ng â†’ Cost=50+1=51â‚¬ â†’ Reward=-0.51
Scenario B: IP=2 â†’ KhÃ´ng Ä‘áº·t â†’ Háº¿t hÃ ng â†’ Cost=0+60=60â‚¬ â†’ Reward=-0.60
â†’ Agent há»c: Äáº·t hÃ ng tá»‘t hÆ¡n khi IP tháº¥p!
```

### 3. Q-Learning Update Rule
```python
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]
```

**CÃ¡c tham sá»‘:**
- **Î± = 0.2** (learning rate): Há»c cháº­m, á»•n Ä‘á»‹nh
- **Î³ = 0.95** (discount factor): Quan tÃ¢m 95% future rewards
- **Episode length = 300 days**: Äá»§ dÃ i Ä‘á»ƒ tháº¥y consequences

**CÃ¡ch update hoáº¡t Ä‘á»™ng:**
1. Agent á»Ÿ state s (e.g., IP=3), chá»n action a (e.g., Ä‘áº·t hÃ ng)
2. Nháº­n reward r = -0.51 (chi phÃ­ 51â‚¬)
3. Chuyá»ƒn sang state s' (e.g., IP=6 sau khi hÃ ng vá»)
4. Update Q(IP=3, Ä‘áº·t hÃ ng) dá»±a trÃªn:
   - Reward ngay láº­p tá»©c: -0.51
   - Future value: Î³ Ã— max Q(IP=6, ...)
   - Mix vá»›i old Q-value: Î± = 20% new, 80% old

### 4. Training Episodes
```python
num_episodes = 2000
episode_length = 300 days
```

**Má»—i episode mÃ´ phá»ng:**
1. **Day 1-300:** Agent Ä‘iá»u hÃ nh kho hÃ ng
2. **Má»—i ngÃ y:**
   - Xem IP hiá»‡n táº¡i
   - Quyáº¿t Ä‘á»‹nh Ä‘áº·t hÃ ng hay khÃ´ng (Îµ-greedy)
   - Nhu cáº§u random xuáº¥t hiá»‡n
   - Nháº­n hÃ ng (náº¿u Ä‘Ã£ Ä‘áº·t 1 ngÃ y trÆ°á»›c)
   - TÃ­nh chi phÃ­
   - Update Q-table
3. **End of episode:** Reset mÃ´i trÆ°á»ng, báº¯t Ä‘áº§u láº¡i

**Tá»•ng training:**
- **2000 episodes Ã— 300 days = 600,000 steps**
- Má»—i state (41 states) Ä‘Æ°á»£c visit trung bÃ¬nh ~14,600 láº§n
- Äá»§ Ä‘á»ƒ há»c cáº£ states hiáº¿m (IP=-20, IP=20)

---

## ğŸ¯ Ngá»¯ cáº£nh mong muá»‘n Agent há»c Ä‘Æ°á»£c

### 1. Basic Inventory Logic
**Mong muá»‘n:**
- âœ… IP tháº¥p â†’ Äáº·t hÃ ng (trÃ¡nh háº¿t hÃ ng)
- âœ… IP cao â†’ KhÃ´ng Ä‘áº·t hÃ ng (trÃ¡nh tá»“n dÆ°)
- âœ… CÃ¢n báº±ng ordering frequency vs inventory level

**Policy lÃ½ tÆ°á»Ÿng (tÆ°Æ¡ng tá»± r,q policy):**
```
Náº¿u IP â‰¤ r (reorder point) â†’ Äáº·t hÃ ng q sáº£n pháº©m
NgÆ°á»£c láº¡i â†’ KhÃ´ng Ä‘áº·t hÃ ng

Vá»›i r â‰ˆ 2-4 (phá»¥ thuá»™c vÃ o mu, LT, safety stock)
```

### 2. Lead Time Awareness
**Mong muá»‘n:**
Agent hiá»ƒu ráº±ng:
- HÃ ng Ä‘áº¿n sau **1 ngÃ y** â†’ Pháº£i dá»± Ä‘oÃ¡n trÆ°á»›c
- Náº¿u IP=4 hÃ´m nay, ngÃ y mai demand=3 â†’ IP=1
- Pháº£i Ä‘áº·t hÃ ng **HÃ”M NAY** Ä‘á»ƒ hÃ ng vá» **NGÃ€Y MAI**

**Behavior mong muá»‘n:**
```
State: IP=3, demand dá»± kiáº¿n 3/ngÃ y
â†’ NgÃ y mai IP â‰ˆ 0 (nguy hiá»ƒm!)
â†’ Action: Äáº·t hÃ ng ngay Ä‘á»ƒ hÃ ng vá» ngÃ y mai (IP=6)
```

### 3. Cost Trade-off Understanding
**Mong muá»‘n:**
Agent tá»± há»c Ä‘Æ°á»£c balance:

#### Trade-off 1: Ordering cost vs Frequency
```
Strategy A: Äáº·t hÃ ng thÆ°á»ng xuyÃªn (IP threshold cao)
  - Pros: Ãt thiáº¿u hÃ ng
  - Cons: Nhiá»u láº§n Ä‘áº·t â†’ Ordering cost cao

Strategy B: Äáº·t hÃ ng hiáº¿m (IP threshold tháº¥p)
  - Pros: Ãt ordering cost
  - Cons: Nguy cÆ¡ thiáº¿u hÃ ng â†’ Backorder cost cao
```

#### Trade-off 2: Holding cost vs Backorder cost
```
h = 10/365 â‰ˆ 0.027 â‚¬/day/unit (tháº¥p)
b = 20 â‚¬ (cao gáº¥p 730 láº§n!)

â†’ Agent nÃªn há»c: Thiáº¿u hÃ ng Tá»† HÆ N NHIá»€U so vá»›i tá»“n dÆ°
â†’ Prefer slightly higher inventory than stockout
```

### 4. Stochastic Demand Handling
**Mong muá»‘n:**
Agent há»c Ä‘Æ°á»£c xá»­ lÃ½ uncertainty:

```python
Demand ~ N(Î¼=3, Ïƒ=1)
â†’ 68% ngÃ y: demand âˆˆ [2, 4]
â†’ 95% ngÃ y: demand âˆˆ [1, 5]
â†’ Hiáº¿m khi: demand = 0 hoáº·c 6+
```

**Safety stock logic mong muá»‘n:**
```
Reorder point r khÃ´ng nÃªn = Î¼ Ã— LT = 3 Ã— 1 = 3
VÃ¬ 50% trÆ°á»ng há»£p demand > 3 â†’ Háº¿t hÃ ng!

NÃªn: r = Î¼ Ã— LT + z Ã— Ïƒ Ã— âˆšLT
     = 3 Ã— 1 + 1.65 Ã— 1 Ã— 1
     = 4.65 â‰ˆ 5 (cho 95% service level)
```

### 5. Long-term Planning
**Mong muá»‘n:**
Vá»›i Î³=0.95, agent nÃªn quan tÃ¢m ~20 bÆ°á»›c tÆ°Æ¡ng lai:

```
Î³^1 = 0.95    â†’ NgÃ y mai quan trá»ng 95%
Î³^2 = 0.90    â†’ 2 ngÃ y sau quan trá»ng 90%
Î³^5 = 0.77    â†’ 5 ngÃ y sau quan trá»ng 77%
Î³^10 = 0.60   â†’ 10 ngÃ y sau quan trá»ng 60%
Î³^20 = 0.36   â†’ 20 ngÃ y sau quan trá»ng 36%
```

**Behavior mong muá»‘n:**
- KhÃ´ng chá»‰ optimize hÃ´m nay
- Hiá»ƒu ráº±ng quyáº¿t Ä‘á»‹nh hÃ´m nay áº£nh hÆ°á»Ÿng 2-3 tuáº§n sau
- VÃ­ dá»¥: Äáº·t hÃ ng hÃ´m nay â†’ Inventory cao 5 ngÃ y sau â†’ Holding cost kÃ©o dÃ i

---

## ğŸš€ Ká»‹ch báº£n Training LÃ½ tÆ°á»Ÿng

### Episode 1-100: Chaotic Exploration
**HÃ nh vi:**
- Agent thá»­ random: Äáº·t hÃ ng khi IP=15, khÃ´ng Ä‘áº·t khi IP=-10, v.v.
- Chi phÃ­ cao: 100,000-300,000 â‚¬ per episode
- Q-values chÆ°a á»•n Ä‘á»‹nh, update liÃªn tá»¥c

**Há»c Ä‘Æ°á»£c:**
- States nÃ o Ä‘Æ°á»£c visit thÆ°á»ng xuyÃªn (IP âˆˆ [-5, 10])
- States nÃ o hiáº¿m (IP < -10 hoáº·c IP > 15)
- Backorder cost Ráº¤T Äáº®T (b=20â‚¬)

### Episode 100-500: Pattern Recognition
**HÃ nh vi:**
- Báº¯t Ä‘áº§u tháº¥y pattern: IP tháº¥p â†’ NÃªn Ä‘áº·t hÃ ng
- Chi phÃ­ giáº£m: 50,000-100,000 â‚¬ per episode
- Epsilon giáº£m: 0.30 â†’ 0.15

**Há»c Ä‘Æ°á»£c:**
- Reorder point nÃªn á»Ÿ Ä‘Ã¢u (~IP=3-5)
- KhÃ´ng nÃªn Ä‘áº·t hÃ ng khi IP>8
- Lead time = 1 ngÃ y â†’ Cáº§n anticipate

### Episode 500-1500: Policy Refinement
**HÃ nh vi:**
- Policy gáº§n á»•n Ä‘á»‹nh: Äáº·t hÃ ng khi IPâ‰¤4
- Chi phÃ­ á»•n Ä‘á»‹nh: 20,000-40,000 â‚¬ per episode
- Epsilon giáº£m: 0.15 â†’ 0.03

**Há»c Ä‘Æ°á»£c:**
- Fine-tune reorder point: IP=3 hay IP=4?
- Handle edge cases: IP=-15, IP=18
- Balance holding vs backorder

### Episode 1500-2000: Convergence
**HÃ nh vi:**
- Policy há»™i tá»¥, Ã­t thay Ä‘á»•i
- Chi phÃ­ tháº¥p nháº¥t: ~20,000 â‚¬ per episode
- Epsilon minimal: 0.03 â†’ 0.01

**Há»c Ä‘Æ°á»£c:**
- Stable policy: IPâ‰¤3 â†’ Order, IPâ‰¥4 â†’ Don't order
- Q-values há»™i tá»¥
- Ready for deployment!

---

## ğŸ“Š Chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ thÃ nh cÃ´ng

### 1. Q-table Health
```python
âœ… Mong muá»‘n:
- Táº¥t cáº£ 82 Q-values (41Ã—2) Ä‘á»u â‰  0 (táº¥t cáº£ states visited)
- Q-values phÃ¢n bá»‘ há»£p lÃ½: min â‰ˆ -20,000, max â‰ˆ -500
- Q(low IP, order) > Q(low IP, no order)
- Q(high IP, no order) > Q(high IP, order)
```

### 2. Learning Curve
```python
âœ… Mong muá»‘n:
- Episode costs giáº£m dáº§n: 300,000 â†’ 20,000 â‚¬
- Convergence rÃµ rÃ ng sau episode 1500
- Variance giáº£m (policy á»•n Ä‘á»‹nh)
```

### 3. Policy Quality
```python
âœ… Mong muá»‘n:
- Learned cost â‰ˆ (r,q) cost Â± 20%
- Policy logic há»£p lÃ½: Threshold rÃµ rÃ ng
- Robust vá»›i random seeds khÃ¡c nhau
```

### 4. Operational Metrics
```python
âœ… Mong muá»‘n:
- Service level: >95% (Ã­t thiáº¿u hÃ ng)
- Average inventory: 3-6 units (há»£p lÃ½)
- Order frequency: ~30-50% days (khÃ´ng quÃ¡ thÆ°á»ng xuyÃªn)
```

---

## ğŸ¬ Káº¿t luáº­n: Ngá»¯ cáº£nh mong muá»‘n tÃ³m gá»n

**Agent lÃ½ tÆ°á»Ÿng sau khi training:**

1. **Hiá»ƒu business logic:** 
   - Low inventory â†’ Order to avoid stockout
   - High inventory â†’ Don't order to save costs

2. **Handle uncertainty:**
   - Demand stochastic â†’ Maintain safety stock
   - Lead time â†’ Plan 1 day ahead

3. **Optimize long-term:**
   - Not just today's cost, but future 20 days
   - Balance ordering frequency vs inventory level

4. **Converged policy:**
   - Clear reorder point (r â‰ˆ 3-5)
   - Consistent behavior across episodes
   - Cost â‰ˆ 30-60 â‚¬/day (comparable to optimal)

5. **Robust vÃ  stable:**
   - Works vá»›i different random seeds
   - Q-values há»™i tá»¥, khÃ´ng oscillate
   - Ready for real-world deployment

---

## ğŸ“š So sÃ¡nh vá»›i (r,q) Policy

| Aspect | (r,q) Policy | Q-Learning Agent (Mong muá»‘n) |
|--------|-------------|-------------------------------|
| **Logic** | If IPâ‰¤r â†’ Order | Learn optimal threshold tá»« data |
| **Parameters** | r, q chosen manually | Learned automatically |
| **Flexibility** | Fixed rule | Adapt to different costs/demands |
| **Optimality** | Good if r,q tuned well | Potentially better (learn non-linear) |
| **Training** | No training needed | 2000 episodes â‰ˆ 30 minutes |
| **Interpretability** | Very clear | Q-table harder to interpret |

**Ká»³ vá»ng cuá»‘i cÃ¹ng:**
- Q-Learning **match hoáº·c beat** (r,q) by 0-30%
- Náº¿u tá»‡ hÆ¡n >50% â†’ CÃ³ váº¥n Ä‘á» (nhÆ° Ä‘Ã£ fix)
- Náº¿u tá»‘t hÆ¡n >30% â†’ Excellent! Q-learning found better pattern

---

**ğŸ¯ TÃ“M Láº I:** Agent cáº§n há»c trong mÃ´i trÆ°á»ng stochastic inventory vá»›i demand dao Ä‘á»™ng, trade-offs giá»¯a 3 loáº¡i chi phÃ­, vÃ  lead time 1 ngÃ y. Sau 2000 episodes vá»›i epsilon decay vÃ  gamma=0.95, agent nÃªn há»™i tá»¥ vá» policy tÆ°Æ¡ng tá»± (r,q) vá»›i reorder point ~3-5, cost ~30-60â‚¬/day, vÃ  hÃ nh vi stable & logical! ğŸš€
